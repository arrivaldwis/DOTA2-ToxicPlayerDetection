{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<title>Toxic Player Detection Dota 2</title>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isjhar-pc\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Isjhar-\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Bidirectional, Flatten\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 10\n",
    "PAD_WORD = \"__PAD__\"\n",
    "WORD_EMBEDDING_FILE = 'word_embedding.csv'\n",
    "TOKENIZER_FILE = 'tokenizer.pickle'\n",
    "NER_FILE = 'toxicner.pkl'\n",
    "BEST_CLASSIFICATION_MODEL_JSON = 'best_classification_model.json'\n",
    "BEST_CLASSIFICATION_MODEL_WEIGHT = 'best_classification_model.h5'\n",
    "\n",
    "def word_embedding_and_tokenizer_file_name(embedding_size, entity_masking):\n",
    "    masking_text = ''\n",
    "    if(entity_masking):\n",
    "        masking_text = '_with_masking'\n",
    "    tokenizer_file = 'results/tokenizer_' + str(embedding_size) + masking_text + '.pickle'\n",
    "    word_embedding_file = 'results/word_embedding_' + str(embedding_size) + masking_text + '.csv'\n",
    "    return word_embedding_file, tokenizer_file\n",
    "\n",
    "def create_word_embedding(data_word_embedding_numpy, embedding_size, entity_masking=False):\n",
    "    # document to sentences\n",
    "    word_embedding_sentences = []\n",
    "    for index, document in enumerate(data_word_embedding_numpy):\n",
    "        sentences = sent_tokenize(document);\n",
    "        for index, sentence in enumerate(sentences):\n",
    "            word_embedding_sentences.append(sentence)    \n",
    "\n",
    "    \n",
    "    \n",
    "    if(entity_masking):\n",
    "        ner_model = load_ner_model()\n",
    "        word_embedding_sentences = [ entity_mask(ner_model, sentence) for sentence in word_embedding_sentences ]  \n",
    "\n",
    "#     print('sentences: ')\n",
    "#     print(word_embedding_sentences)\n",
    "    # melakukan tokenisasi (menghilangkan tanda baca, dll)\n",
    "    # membuat dictionary setiap token\n",
    "    word_embedding_tokenizer = Tokenizer(oov_token='OOV')\n",
    "    word_embedding_tokenizer.fit_on_texts(word_embedding_sentences)\n",
    "#     print('\\nword index:')\n",
    "#     print(word_embedding_tokenizer.word_index)\n",
    "    \n",
    "    \n",
    "    # encoding setiap token sesuai dengan nomor indexnya\n",
    "    word_embedding_sequences = word_embedding_tokenizer.texts_to_sequences(word_embedding_sentences)\n",
    "#     print('\\nencoding:')\n",
    "#     print(word_embedding_sequences)\n",
    "    \n",
    "    # mengembalikan sequence index kembali katanya yang sudah hilang tanda bacanya\n",
    "    # karena input gensim adalah kata\n",
    "    data_word_embedding_numpy_temp = [[word_embedding_tokenizer.index_word[word_index] for word_index in sentence] for sentence in word_embedding_sequences]\n",
    "#     print('\\ntokenisasi:')\n",
    "#     print(data_word_embedding_numpy_temp)\n",
    "    \n",
    "    # training word embedding\n",
    "    word_embedding_model = gensim.models.Word2Vec(data_word_embedding_numpy_temp, min_count = 1, size = embedding_size, window = 5, sg = 1) \n",
    "    word_index = word_embedding_tokenizer.word_index\n",
    "    # membuat array yang berisi value word_embedding berdasar index katanya pada dictionary\n",
    "    word_embedding_temp = np.zeros((len(word_index)+1, embedding_size))\n",
    "    for word, word_object in word_embedding_model.wv.vocab.items():\n",
    "        index = word_index[word]\n",
    "        word_embedding_temp[index] = word_embedding_model.wv[word]   \n",
    "    \n",
    "    word_embedding_file, tokenizer_file = word_embedding_and_tokenizer_file_name(embedding_size, entity_masking)\n",
    "    # store tokenizer dictionary \n",
    "    with open(tokenizer_file, 'wb') as handle:\n",
    "        pickle.dump(word_embedding_tokenizer,  handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print('tokenizer stored on: ', tokenizer_file)\n",
    "    \n",
    "    # word embedding to fil\n",
    "    np.savetxt(word_embedding_file, word_embedding_temp, delimiter=',')\n",
    "    print('word embedding stored on: ', word_embedding_file)\n",
    "    \n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def is_match(X1, X2):\n",
    "    for i in range(len(X1)):\n",
    "        arr1 = X1[i]\n",
    "        arr2 = X2[i]\n",
    "        if(len(arr1) != len(arr2)):\n",
    "            print(i)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def word2nerfeatures(sent, i):\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def sent2nerfeatures(sent):\n",
    "    return [word2nerfeatures(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def entity_mask(ner_model, sentence):\n",
    "    sent_token = word_tokenize(sentence)\n",
    "    features = [sent2nerfeatures(sent_token)]   \n",
    "    entities = ner_model.predict(features)[0]\n",
    "    result = []\n",
    "    for index in range(len(entities)):\n",
    "        if(entities[index] != 'O'):\n",
    "            masking = map_entity_and_mask(entities[index])\n",
    "            result.append(masking)\n",
    "        else:\n",
    "            result.append(sent_token[index])\n",
    "    return \" \".join(result)\n",
    "\n",
    "def map_entity_and_mask(entity):\n",
    "    if entity == 'B-bad' or entity == 'I-bad':\n",
    "        return 'XXBADXX'\n",
    "    elif entity == 'B-pra' or entity == 'I-pra':\n",
    "        return 'XXPRAISEXX'\n",
    "    elif entity == 'B-her' or entity == 'I-her':\n",
    "        return 'XXHEROXX'\n",
    "    return None\n",
    "\n",
    "def load_ner_model():\n",
    "    # Load model ner dari pkl file\n",
    "    return joblib.load(NER_FILE) \n",
    "\n",
    "def load_word_embedding_and_tokenizer(embedding_size, entity_masking):\n",
    "    word_embedding_file, tokenizer_file = word_embedding_and_tokenizer_file_name(embedding_size, entity_masking)\n",
    "    \n",
    "    #load word embedding\n",
    "    word_embedding = np.loadtxt(word_embedding_file, delimiter=',')\n",
    "    #load tokenizer \n",
    "    with open(tokenizer_file, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "        \n",
    "    return word_embedding, tokenizer\n",
    "\n",
    "def save_best_model(best_model):\n",
    "    best_model_json = best_model.to_json()\n",
    "    with open(BEST_CLASSIFICATION_MODEL_JSON, \"w\") as json_file:\n",
    "        json_file.write(best_model_json)\n",
    "    # serialize weights to HDF5\n",
    "    best_model.save_weights(BEST_CLASSIFICATION_MODEL_WEIGHT)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "def load_best_model():\n",
    "    json_file = open(BEST_CLASSIFICATION_MODEL_JSON, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(BEST_CLASSIFICATION_MODEL_WEIGHT)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    "\n",
    "def preprocess(X_raw, embedding_size, entity_masking):\n",
    "    word_embedding, tokenizer = load_word_embedding_and_tokenizer(embedding_size, entity_masking)\n",
    "    \n",
    "    if(entity_masking):\n",
    "        print('sent_tokenize')\n",
    "        X_raw = [ sent_tokenize(sentence) for sentence in X_raw ]\n",
    "    \n",
    "    X = tokenizer.texts_to_sequences(X_raw)\n",
    "    X = pad_sequences(X, maxlen=SEQUENCE_LENGTH, padding='post', value=0)\n",
    "    return X\n",
    "\n",
    "def do_experiment(X_raw, y_raw, technique, embedding_size, entity_masking=False, dropout_layer=False):\n",
    "    \n",
    "    word_embedding, tokenizer = load_word_embedding_and_tokenizer(embedding_size, entity_masking)\n",
    "\n",
    "    X_preprocess = X_raw\n",
    "    if(entity_masking):\n",
    "        X_preprocess = entity_mask_raw_data(X_raw)\n",
    "        \n",
    "    \n",
    "        \n",
    "    #text to sequence\n",
    "    X = tokenizer.texts_to_sequences(X_preprocess)\n",
    "    y = to_categorical(y_raw)\n",
    "    \n",
    "    #padding\n",
    "    X = pad_sequences(X, maxlen=SEQUENCE_LENGTH, padding='post', value=0)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "    cvscores = []\n",
    "    for train, test in kfold.split(X_preprocess, y_raw):\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        \n",
    "        #create model\n",
    "        model_val = Sequential()\n",
    "        model_val.add(Embedding(\n",
    "            input_dim=len(tokenizer.word_index)+1, \n",
    "            output_dim=embedding_size, \n",
    "            input_length=SEQUENCE_LENGTH, \n",
    "            weights=[word_embedding],\n",
    "            trainable=False))\n",
    "        \n",
    "        if(technique == 'lstm'):\n",
    "            model_val.add(LSTM(128, recurrent_dropout=0.2))\n",
    "        else:\n",
    "            model_val.add(Bidirectional(LSTM(128, recurrent_dropout=0.2)))\n",
    "\n",
    "        if(dropout_layer):\n",
    "            model_val.add(Dropout(0.3))    \n",
    "        model_val.add(Dense(2, activation=\"softmax\"))\n",
    "        # compile as rmsprop optimizer\n",
    "        # aswell as with recall metric\n",
    "        model_val.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\", f1_m])\n",
    "        \n",
    "        model_val.summary()\n",
    "        # Fit the model\n",
    "        model_val.fit(X_train, y_train, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "        # evaluate the model\n",
    "        scores = model_val.evaluate(X_test, y_test)\n",
    "        print(\"%s: %.2f%%\" % (model_val.metrics_names[2], scores[2]*100))\n",
    "        cvscores.append(scores[2] * 100)\n",
    "        \n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    \n",
    "def entity_mask_raw_data(X_raw):\n",
    "    ner_model = load_ner_model()\n",
    "    X_preprocess = []\n",
    "    for doc_index in range(len(X_raw)):\n",
    "        sentences = sent_tokenize(X_raw[doc_index])\n",
    "        for sent_index in range(len(sentences)):\n",
    "            sentences[sent_index] = entity_mask(ner_model, sentences[sent_index])            \n",
    "        X_preprocess.append(\". \".join(sentences))\n",
    "    return X_preprocess\n",
    "    \n",
    "def train_best_model(X_raw, y_raw, technique, embedding_size, entity_masking=False, dropout_layer=False):\n",
    "    \n",
    "    word_embedding, tokenizer = load_word_embedding_and_tokenizer(embedding_size, entity_masking)\n",
    "\n",
    "    X_preprocess = X_raw\n",
    "    if(entity_masking):\n",
    "        X_preprocess = entity_mask_raw_data(X_raw)\n",
    "    \n",
    "    print(type(tokenizer))\n",
    "    #text to sequence\n",
    "    X = tokenizer.texts_to_sequences(X_preprocess)\n",
    "    y = to_categorical(y_raw)\n",
    "    \n",
    "    #padding\n",
    "    X = pad_sequences(X, maxlen=SEQUENCE_LENGTH, padding='post', value=0)\n",
    "        \n",
    "    #create model\n",
    "    model_val = Sequential()\n",
    "    model_val.add(Embedding(\n",
    "        input_dim=len(tokenizer.word_index)+1, \n",
    "        output_dim=embedding_size, \n",
    "        input_length=SEQUENCE_LENGTH, \n",
    "        weights=[word_embedding],\n",
    "        trainable=False))\n",
    "\n",
    "    if(technique == 'lstm'):\n",
    "        model_val.add(LSTM(128, recurrent_dropout=0.2))\n",
    "    else:\n",
    "        model_val.add(Bidirectional(LSTM(128, recurrent_dropout=0.2)))\n",
    "\n",
    "    if(dropout_layer):\n",
    "        model_val.add(Dropout(0.3))    \n",
    "    model_val.add(Dense(2, activation=\"softmax\"))\n",
    "    # compile as rmsprop optimizer\n",
    "    # aswell as with recall metric\n",
    "    model_val.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", \n",
    "          metrics=[\"accuracy\", f1_m])\n",
    "\n",
    "    model_val.summary()\n",
    "    # Fit the model\n",
    "    model_val.fit(X, y, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return model_val\n",
    "\n",
    "def evaluate_best_model(X_raw, y_raw, technique, embedding_size, entity_masking=False, dropout_layer=False):\n",
    "    \n",
    "    word_embedding, tokenizer = load_word_embedding_and_tokenizer(embedding_size, entity_masking)\n",
    "\n",
    "    X_preprocess = X_raw\n",
    "    if(entity_masking):\n",
    "        X_preprocess = entity_mask_raw_data(X_raw)\n",
    "    \n",
    "    print(type(tokenizer))\n",
    "    #text to sequence\n",
    "    X = tokenizer.texts_to_sequences(X_preprocess)\n",
    "    y = to_categorical(y_raw)\n",
    "    \n",
    "    #padding\n",
    "    X = pad_sequences(X, maxlen=SEQUENCE_LENGTH, padding='post', value=0)\n",
    "        \n",
    "    #create model\n",
    "    best_model_predict = load_best_model()\n",
    "    scores = best_model_predict.evaluate(X, y)\n",
    "    print(\"%s: %.2f%%\" % (model_val.metrics_names[2], scores[2]*100))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes dog'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model = load_ner_model()\n",
    "entity_mask(ner_model, 'yes dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocess Raw Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>ладно гг</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>изи</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>од</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>ебаный</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>мусор на войде</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>мусор</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>на войде</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>репорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match  slot                    text\n",
       "0      0     9               ладно гг \n",
       "1      0     9                     изи\n",
       "2      0     9                      од\n",
       "3      0     9                  ебаный\n",
       "4      0     9          мусор на войде\n",
       "5      0     9                  мусор \n",
       "6      0     9                на войде\n",
       "7      0     9                  репорт\n",
       "8      0     9                    100%\n",
       "9      1     0  twitch.tv/rage_channel"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/dota2_chat_messages.csv\", encoding=\"utf_8\", usecols=[\"match\", \"slot\", \"text\"])\n",
    "data = data.astype({\"match\": int, \"slot\": int, \"text\": str})\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "match  slot\n",
       "0      9       ладно гг . изи. од. ебаный. мусор на войде. му...\n",
       "1      0       twitch.tv/rage_channel. https://www.twitch.tv/...\n",
       "       4       где даша?. даша домой. долбоеб сука на дизрапторе\n",
       "       6       даун с 1 тычки забашил . шок . стример харду с...\n",
       "       7                                   2 даша подряд . баша \n",
       "       8                                                     )))\n",
       "2      0       yes dog. yeah . fast and furious. too fas. hah...\n",
       "       2                               no idiot. we too pro. lol\n",
       "       4                                HAHAH. COMMEND ME TY. EZ\n",
       "       6                                              carry. lul\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.groupby(['match', 'slot'])['text'].apply('. '.join)\n",
    "data.reset_index()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>ладно гг . изи. од. ебаный. мусор на войде. му...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>twitch.tv/rage_channel. https://www.twitch.tv/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>где даша?. даша домой. долбоеб сука на дизрапторе</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>даун с 1 тычки забашил . шок . стример харду с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2 даша подряд . баша</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>)))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes dog. yeah . fast and furious. too fas. hah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>no idiot. we too pro. lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>HAHAH. COMMEND ME TY. EZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>carry. lul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match  slot                                               text\n",
       "0      0     9  ладно гг . изи. од. ебаный. мусор на войде. му...\n",
       "1      1     0  twitch.tv/rage_channel. https://www.twitch.tv/...\n",
       "2      1     4  где даша?. даша домой. долбоеб сука на дизрапторе\n",
       "3      1     6  даун с 1 тычки забашил . шок . стример харду с...\n",
       "4      1     7                              2 даша подряд . баша \n",
       "5      1     8                                                )))\n",
       "6      2     0  yes dog. yeah . fast and furious. too fas. hah...\n",
       "7      2     2                          no idiot. we too pro. lol\n",
       "8      2     4                           HAHAH. COMMEND ME TY. EZ\n",
       "9      2     6                                         carry. lul"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/dota2_chat_joined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4.747547e+06</td>\n",
       "      <td>4.747547e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4.999057e+05</td>\n",
       "      <td>4.493503e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.887862e+05</td>\n",
       "      <td>2.873926e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2.497860e+05</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>4.999710e+05</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7.500870e+05</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9.999990e+05</td>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              match          slot\n",
       "count  4.747547e+06  4.747547e+06\n",
       "mean   4.999057e+05  4.493503e+00\n",
       "std    2.887862e+05  2.873926e+00\n",
       "min    0.000000e+00  0.000000e+00\n",
       "25%    2.497860e+05  2.000000e+00\n",
       "50%    4.999710e+05  5.000000e+00\n",
       "75%    7.500870e+05  7.000000e+00\n",
       "max    9.999990e+05  9.000000e+00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes dog. yeah . fast and furious. too fas. hah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>no idiot. we too pro. lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>HAHAH. COMMEND ME TY. EZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>so ya mama likes dick ehh?. figures. ur not ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>reprot. SAD. fucking reported axe. WORST HOOK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>gg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>axe is axe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>sorry nex. i killed u . almost . gg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>PUSH. not defending. dodger lc. swap commend t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>what. jeje fAM. free farming ls. not coming in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match  slot                                               text\n",
       "0      2     0  yes dog. yeah . fast and furious. too fas. hah...\n",
       "1      2     2                          no idiot. we too pro. lol\n",
       "2      2     4                           HAHAH. COMMEND ME TY. EZ\n",
       "3      6     0  so ya mama likes dick ehh?. figures. ur not ev...\n",
       "4      6     1  reprot. SAD. fucking reported axe. WORST HOOK ...\n",
       "5      6     3                                                 gg\n",
       "6      6     4                                         axe is axe\n",
       "7      6     8                sorry nex. i killed u . almost . gg\n",
       "8      8     6  PUSH. not defending. dodger lc. swap commend t...\n",
       "9      9     3  what. jeje fAM. free farming ls. not coming in..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word_embedding = pd.read_csv(\"data/dota2_chat_final.csv\", encoding=\"latin_1\", usecols=[\"match\", \"slot\", \"text\"])\n",
    "data_word_embedding.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word_embedding_numpy = data_word_embedding.drop(columns=['match', 'slot']).to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer stored on:  tokenizer_50.pickle\n",
      "word embedding stored on:  word_embedding_50.csv\n"
     ]
    }
   ],
   "source": [
    "create_word_embedding(data_word_embedding_numpy, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer stored on:  tokenizer_100.pickle\n",
      "word embedding stored on:  word_embedding_100.csv\n"
     ]
    }
   ],
   "source": [
    "create_word_embedding(data_word_embedding_numpy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer stored on:  tokenizer_50_with_masking.pickle\n",
      "word embedding stored on:  word_embedding_50_with_masking.csv\n"
     ]
    }
   ],
   "source": [
    "create_word_embedding(data_word_embedding_numpy, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer stored on:  tokenizer_100_with_masking.pickle\n",
      "word embedding stored on:  word_embedding_100_with_masking.csv\n"
     ]
    }
   ],
   "source": [
    "create_word_embedding(data_word_embedding_numpy, 100, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding_50_with_masking.csv\n",
      "tokenizer_50_with_masking.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'OOV': 1,\n",
       " 'i': 2,\n",
       " 'u': 3,\n",
       " 'xxbadxx': 4,\n",
       " 'you': 5,\n",
       " 'gg': 6,\n",
       " 'ez': 7,\n",
       " 'lol': 8,\n",
       " 'report': 9,\n",
       " 'this': 10,\n",
       " 'to': 11,\n",
       " 'mid': 12,\n",
       " 'me': 13,\n",
       " 'xxpraisexx': 14,\n",
       " 'and': 15,\n",
       " 'a': 16,\n",
       " 'game': 17,\n",
       " 'is': 18,\n",
       " 'so': 19,\n",
       " 'end': 20,\n",
       " 'my': 21,\n",
       " 'no': 22,\n",
       " 'go': 23,\n",
       " 'team': 24,\n",
       " 'the': 25,\n",
       " 'for': 26,\n",
       " 'just': 27,\n",
       " 'noob': 28,\n",
       " 'haha': 29,\n",
       " 'he': 30,\n",
       " 'im': 31,\n",
       " 'that': 32,\n",
       " 'we': 33,\n",
       " 'not': 34,\n",
       " 'can': 35,\n",
       " 'what': 36,\n",
       " 'dont': 37,\n",
       " 'ur': 38,\n",
       " 'wp': 39,\n",
       " 'in': 40,\n",
       " 'it': 41,\n",
       " 'how': 42,\n",
       " 'pls': 43,\n",
       " 'have': 44,\n",
       " 'nice': 45,\n",
       " 'are': 46,\n",
       " 'xd': 47,\n",
       " 'of': 48,\n",
       " 'your': 49,\n",
       " 'win': 50,\n",
       " 'shit': 51,\n",
       " 'guys': 52,\n",
       " 'all': 53,\n",
       " 'why': 54,\n",
       " 'lc': 55,\n",
       " 'do': 56,\n",
       " 'wtf': 57,\n",
       " 'pa': 58,\n",
       " 'd': 59,\n",
       " 'like': 60,\n",
       " 'pudge': 61,\n",
       " 'dog': 62,\n",
       " 'ok': 63,\n",
       " 'cant': 64,\n",
       " 'sad': 65,\n",
       " 'commend': 66,\n",
       " 'good': 67,\n",
       " 'sf': 68,\n",
       " 'now': 69,\n",
       " 'with': 70,\n",
       " 'see': 71,\n",
       " 'am': 72,\n",
       " 'kill': 73,\n",
       " 'at': 74,\n",
       " '2': 75,\n",
       " 'get': 76,\n",
       " 'feed': 77,\n",
       " 'on': 78,\n",
       " 'got': 79,\n",
       " '3': 80,\n",
       " 'was': 81,\n",
       " 'hahaha': 82,\n",
       " 'man': 83,\n",
       " 'know': 84,\n",
       " 'afk': 85,\n",
       " 'more': 86,\n",
       " 'wait': 87,\n",
       " 'come': 88,\n",
       " 'pro': 89,\n",
       " '1': 90,\n",
       " 'be': 91,\n",
       " 'carry': 92,\n",
       " 'if': 93,\n",
       " 'pick': 94,\n",
       " 'ggwp': 95,\n",
       " \"'s\": 96,\n",
       " 'trash': 97,\n",
       " 'farm': 98,\n",
       " 'last': 99,\n",
       " 'please': 100,\n",
       " 'will': 101,\n",
       " 'its': 102,\n",
       " 'they': 103,\n",
       " '0': 104,\n",
       " 'one': 105,\n",
       " \"n't\": 106,\n",
       " 'lmao': 107,\n",
       " 'when': 108,\n",
       " 'play': 109,\n",
       " 'retard': 110,\n",
       " 'die': 111,\n",
       " 'plz': 112,\n",
       " 'him': 113,\n",
       " 'but': 114,\n",
       " 'stop': 115,\n",
       " 'ursa': 116,\n",
       " 'fast': 117,\n",
       " 'even': 118,\n",
       " 'useless': 119,\n",
       " 'support': 120,\n",
       " 'wow': 121,\n",
       " 'idiot': 122,\n",
       " 'let': 123,\n",
       " '4': 124,\n",
       " 'up': 125,\n",
       " 'only': 126,\n",
       " 'top': 127,\n",
       " 'mirana': 128,\n",
       " 'really': 129,\n",
       " 'well': 130,\n",
       " 'time': 131,\n",
       " 'dude': 132,\n",
       " 'sniper': 133,\n",
       " 'fuck': 134,\n",
       " 'first': 135,\n",
       " 'suck': 136,\n",
       " 'too': 137,\n",
       " 'dick': 138,\n",
       " 'na': 139,\n",
       " 'medusa': 140,\n",
       " 'hero': 141,\n",
       " 'sure': 142,\n",
       " 'jugg': 143,\n",
       " 'hard': 144,\n",
       " 'g': 145,\n",
       " 'did': 146,\n",
       " 'russian': 147,\n",
       " '5': 148,\n",
       " 'axe': 149,\n",
       " 'need': 150,\n",
       " 'invoker': 151,\n",
       " 'next': 152,\n",
       " 'there': 153,\n",
       " 'then': 154,\n",
       " 'out': 155,\n",
       " 'dota': 156,\n",
       " 'es': 157,\n",
       " 'said': 158,\n",
       " 'yes': 159,\n",
       " 'rofl': 160,\n",
       " 'oh': 161,\n",
       " 'stupid': 162,\n",
       " 'were': 163,\n",
       " 'never': 164,\n",
       " 'as': 165,\n",
       " 'player': 166,\n",
       " 'who': 167,\n",
       " '10': 168,\n",
       " 'done': 169,\n",
       " 'bad': 170,\n",
       " 'our': 171,\n",
       " 'yeah': 172,\n",
       " 'ty': 173,\n",
       " 'kid': 174,\n",
       " 'sorry': 175,\n",
       " 'storm': 176,\n",
       " 'still': 177,\n",
       " 'fucking': 178,\n",
       " 'had': 179,\n",
       " 'back': 180,\n",
       " 'tinker': 181,\n",
       " 'lose': 182,\n",
       " 'right': 183,\n",
       " 'cunt': 184,\n",
       " 'stfu': 185,\n",
       " 'want': 186,\n",
       " 'bb': 187,\n",
       " 'try': 188,\n",
       " 'hahah': 189,\n",
       " 'didnt': 190,\n",
       " 'an': 191,\n",
       " 'help': 192,\n",
       " 'def': 193,\n",
       " 'bro': 194,\n",
       " 'real': 195,\n",
       " 'off': 196,\n",
       " 'make': 197,\n",
       " 'monkey': 198,\n",
       " 'ff': 199,\n",
       " 'think': 200,\n",
       " 'cancer': 201,\n",
       " 'base': 202,\n",
       " 'than': 203,\n",
       " 'call': 204,\n",
       " 'y': 205,\n",
       " 'vs': 206,\n",
       " 'asshole': 207,\n",
       " 'us': 208,\n",
       " 'lucky': 209,\n",
       " 'take': 210,\n",
       " 'by': 211,\n",
       " 'run': 212,\n",
       " 'poor': 213,\n",
       " 'dead': 214,\n",
       " 'skill': 215,\n",
       " 'bitch': 216,\n",
       " 'invo': 217,\n",
       " 'mmr': 218,\n",
       " 'gank': 219,\n",
       " 'won': 220,\n",
       " 'ward': 221,\n",
       " 'cm': 222,\n",
       " 'xxheroxx': 223,\n",
       " 'his': 224,\n",
       " 'here': 225,\n",
       " 'again': 226,\n",
       " 'from': 227,\n",
       " 'finish': 228,\n",
       " 'give': 229,\n",
       " 'fucker': 230,\n",
       " 'look': 231,\n",
       " 'wk': 232,\n",
       " 'playing': 233,\n",
       " 'void': 234,\n",
       " 'gay': 235,\n",
       " 'has': 236,\n",
       " 'use': 237,\n",
       " 'these': 238,\n",
       " 'blood': 239,\n",
       " 'de': 240,\n",
       " '100': 241,\n",
       " 'shut': 242,\n",
       " 'already': 243,\n",
       " 'buy': 244,\n",
       " 'luna': 245,\n",
       " \"'\": 246,\n",
       " 'ends': 247,\n",
       " 'slark': 248,\n",
       " 'hook': 249,\n",
       " 'hit': 250,\n",
       " 'better': 251,\n",
       " 'od': 252,\n",
       " 'gege': 253,\n",
       " 'lost': 254,\n",
       " 'care': 255,\n",
       " 'new': 256,\n",
       " 'k': 257,\n",
       " 'lvl': 258,\n",
       " 'death': 259,\n",
       " 'being': 260,\n",
       " 'kids': 261,\n",
       " 'bark': 262,\n",
       " 'or': 263,\n",
       " 'thx': 264,\n",
       " 'min': 265,\n",
       " 'retards': 266,\n",
       " 'thats': 267,\n",
       " 'about': 268,\n",
       " 'solo': 269,\n",
       " \"'re\": 270,\n",
       " 'wew': 271,\n",
       " 'omni': 272,\n",
       " 'every': 273,\n",
       " 'ck': 274,\n",
       " 'toxic': 275,\n",
       " 'ya': 276,\n",
       " 'killed': 277,\n",
       " 'does': 278,\n",
       " 'god': 279,\n",
       " 'mom': 280,\n",
       " 'bot': 281,\n",
       " 'happy': 282,\n",
       " 'wind': 283,\n",
       " 'slow': 284,\n",
       " 'huskar': 285,\n",
       " 'leave': 286,\n",
       " 'la': 287,\n",
       " 'lets': 288,\n",
       " '1v1': 289,\n",
       " 'pussy': 290,\n",
       " 'omg': 291,\n",
       " 'hey': 292,\n",
       " 'mother': 293,\n",
       " 'same': 294,\n",
       " 'aw': 295,\n",
       " 'she': 296,\n",
       " 'joke': 297,\n",
       " 'wont': 298,\n",
       " 'reported': 299,\n",
       " 'other': 300,\n",
       " 'ever': 301,\n",
       " 'lul': 302,\n",
       " 'some': 303,\n",
       " 'feeding': 304,\n",
       " 'ulti': 305,\n",
       " 'hehe': 306,\n",
       " 'sry': 307,\n",
       " 'thanks': 308,\n",
       " 'gon': 309,\n",
       " 'cuz': 310,\n",
       " 'ill': 311,\n",
       " 'mean': 312,\n",
       " 'much': 313,\n",
       " 'nothing': 314,\n",
       " 'item': 315,\n",
       " 'hits': 316,\n",
       " 'meepo': 317,\n",
       " 'aa': 318,\n",
       " 'worst': 319,\n",
       " 'push': 320,\n",
       " 'ls': 321,\n",
       " 'fat': 322,\n",
       " 'whats': 323,\n",
       " 'day': 324,\n",
       " 'year': 325,\n",
       " 'because': 326,\n",
       " 'blink': 327,\n",
       " 'offlane': 328,\n",
       " 'talking': 329,\n",
       " 'ahaha': 330,\n",
       " 'lane': 331,\n",
       " 'life': 332,\n",
       " 'rage': 333,\n",
       " 'fuckin': 334,\n",
       " 'bara': 335,\n",
       " 'lel': 336,\n",
       " 'ass': 337,\n",
       " 'pause': 338,\n",
       " 'say': 339,\n",
       " 'fun': 340,\n",
       " 'jungle': 341,\n",
       " \"'m\": 342,\n",
       " 'gl': 343,\n",
       " 'anyway': 344,\n",
       " 'hell': 345,\n",
       " 'doesnt': 346,\n",
       " 'moron': 347,\n",
       " 'hi': 348,\n",
       " 'talk': 349,\n",
       " 'anything': 350,\n",
       " '9': 351,\n",
       " 'counter': 352,\n",
       " 'teammates': 353,\n",
       " 'hate': 354,\n",
       " 'btw': 355,\n",
       " 'nah': 356,\n",
       " 'dust': 357,\n",
       " 'them': 358,\n",
       " 'items': 359,\n",
       " 'hahahah': 360,\n",
       " 'best': 361,\n",
       " 'bh': 362,\n",
       " '6': 363,\n",
       " 'herald': 364,\n",
       " 'morons': 365,\n",
       " 'people': 366,\n",
       " 'point': 367,\n",
       " 'luck': 368,\n",
       " 'always': 369,\n",
       " 'kunka': 370,\n",
       " 'thought': 371,\n",
       " 'almost': 372,\n",
       " 'farming': 373,\n",
       " 'played': 374,\n",
       " 'r': 375,\n",
       " 'roll': 376,\n",
       " 'mana': 377,\n",
       " 'would': 378,\n",
       " 'archon': 379,\n",
       " 'quit': 380,\n",
       " 'lag': 381,\n",
       " 'worth': 382,\n",
       " 'down': 383,\n",
       " 'miss': 384,\n",
       " 'love': 385,\n",
       " 'server': 386,\n",
       " 'must': 387,\n",
       " 'bye': 388,\n",
       " 'long': 389,\n",
       " 'okay': 390,\n",
       " 'rly': 391,\n",
       " 'wards': 392,\n",
       " 'minute': 393,\n",
       " 'minutes': 394,\n",
       " 'everyone': 395,\n",
       " 'sven': 396,\n",
       " 'hf': 397,\n",
       " 'boys': 398,\n",
       " 'fault': 399,\n",
       " 'p': 400,\n",
       " 'que': 401,\n",
       " 'ta': 402,\n",
       " 'weaver': 403,\n",
       " '16': 404,\n",
       " 'told': 405,\n",
       " 'tell': 406,\n",
       " 'alche': 407,\n",
       " 'reporten': 408,\n",
       " 'noobs': 409,\n",
       " 'mrda': 410,\n",
       " 'very': 411,\n",
       " 'techies': 412,\n",
       " 'cheat': 413,\n",
       " 'ss': 414,\n",
       " 'fight': 415,\n",
       " 'blame': 416,\n",
       " 'speak': 417,\n",
       " 'rich': 418,\n",
       " 'over': 419,\n",
       " 'hes': 420,\n",
       " 'ember': 421,\n",
       " 'rubick': 422,\n",
       " 'fucked': 423,\n",
       " 'sd': 424,\n",
       " 'hahahaha': 425,\n",
       " 'fail': 426,\n",
       " 'lion': 427,\n",
       " 'wan': 428,\n",
       " 'deserve': 429,\n",
       " 'lina': 430,\n",
       " 'deff': 431,\n",
       " 'pinoy': 432,\n",
       " 'brainless': 433,\n",
       " 'cock': 434,\n",
       " 'passive': 435,\n",
       " 'shadow': 436,\n",
       " 'blade': 437,\n",
       " 'ult': 438,\n",
       " 'yet': 439,\n",
       " 'q': 440,\n",
       " 'wants': 441,\n",
       " 'hahahahaha': 442,\n",
       " 'alien': 443,\n",
       " 'stick': 444,\n",
       " 'happened': 445,\n",
       " 'mins': 446,\n",
       " 'boi': 447,\n",
       " 'gold': 448,\n",
       " 'lycan': 449,\n",
       " 'forget': 450,\n",
       " 'been': 451,\n",
       " 'games': 452,\n",
       " 'rude': 453,\n",
       " 'lich': 454,\n",
       " 'called': 455,\n",
       " 'country': 456,\n",
       " 'russians': 457,\n",
       " 'without': 458,\n",
       " 'cute': 459,\n",
       " 'funny': 460,\n",
       " 'where': 461,\n",
       " '25': 462,\n",
       " 'mate': 463,\n",
       " 'mad': 464,\n",
       " 'true': 465,\n",
       " 'rata': 466,\n",
       " 'spanish': 467,\n",
       " 'also': 468,\n",
       " 'injoker': 469,\n",
       " 'thank': 470,\n",
       " 'troll': 471,\n",
       " 'russia': 472,\n",
       " 'eu': 473,\n",
       " 'voker': 474,\n",
       " 'waw': 475,\n",
       " 'cry': 476,\n",
       " 'holy': 477,\n",
       " 'chat': 478,\n",
       " 'bkb': 479,\n",
       " 'kills': 480,\n",
       " 'puck': 481,\n",
       " 'viper': 482,\n",
       " '18': 483,\n",
       " 'faggot': 484,\n",
       " 'actually': 485,\n",
       " 'xdd': 486,\n",
       " 'whatever': 487,\n",
       " 'isnt': 488,\n",
       " 'nigga': 489,\n",
       " 'girl': 490,\n",
       " 'guy': 491,\n",
       " 'gpm': 492,\n",
       " 'al': 493,\n",
       " 'tu': 494,\n",
       " 'ahah': 495,\n",
       " 'nope': 496,\n",
       " 'myself': 497,\n",
       " 'gj': 498,\n",
       " '7': 499,\n",
       " 'supp': 500,\n",
       " 'click': 501,\n",
       " 'np': 502,\n",
       " 'literally': 503,\n",
       " 'dumb': 504,\n",
       " 'legion': 505,\n",
       " 'single': 506,\n",
       " 'picks': 507,\n",
       " 'picker': 508,\n",
       " 'qop': 509,\n",
       " '8': 510,\n",
       " 'easy': 511,\n",
       " 'enjoy': 512,\n",
       " 'retarded': 513,\n",
       " 'heroes': 514,\n",
       " 'kunkka': 515,\n",
       " 'throw': 516,\n",
       " 'sit': 517,\n",
       " 'cool': 518,\n",
       " 'rampage': 519,\n",
       " 'mo': 520,\n",
       " 'press': 521,\n",
       " 'repor': 522,\n",
       " 'lp': 523,\n",
       " 'moph': 524,\n",
       " 'boy': 525,\n",
       " 'coz': 526,\n",
       " 'needs': 527,\n",
       " 'wd': 528,\n",
       " 'jacky': 529,\n",
       " 'reasons': 530,\n",
       " 'clingz': 531,\n",
       " 'l0l': 532,\n",
       " 'mk': 533,\n",
       " 'mama': 534,\n",
       " 'swap': 535,\n",
       " 'free': 536,\n",
       " 'coming': 537,\n",
       " 'into': 538,\n",
       " 'start': 539,\n",
       " 'yea': 540,\n",
       " 'her': 541,\n",
       " 'ti': 542,\n",
       " '2017': 543,\n",
       " 'ca': 544,\n",
       " 'clock': 545,\n",
       " 'divine': 546,\n",
       " 'tk': 547,\n",
       " 'rep': 548,\n",
       " 'waiting': 549,\n",
       " 'many': 550,\n",
       " 'second': 551,\n",
       " 'da': 552,\n",
       " 'chill': 553,\n",
       " 'bashes': 554,\n",
       " 'dogs': 555,\n",
       " '21': 556,\n",
       " 'times': 557,\n",
       " 'tower': 558,\n",
       " 'white': 559,\n",
       " 'after': 560,\n",
       " 'great': 561,\n",
       " 'fair': 562,\n",
       " 'csgo': 563,\n",
       " 'glhf': 564,\n",
       " 'lie': 565,\n",
       " 'barking': 566,\n",
       " 'yourself': 567,\n",
       " 'east': 568,\n",
       " 'west': 569,\n",
       " 'enigma': 570,\n",
       " '20': 571,\n",
       " '30': 572,\n",
       " 'small': 573,\n",
       " 'id': 574,\n",
       " 'show': 575,\n",
       " '1st': 576,\n",
       " 'jajaja': 577,\n",
       " 'meta': 578,\n",
       " 'wat': 579,\n",
       " 'doom': 580,\n",
       " 'eat': 581,\n",
       " 'zeus': 582,\n",
       " 'ahahha': 583,\n",
       " 'boring': 584,\n",
       " 'smart': 585,\n",
       " 'af': 586,\n",
       " 'underlord': 587,\n",
       " 'winter': 588,\n",
       " 'fine': 589,\n",
       " 'rosh': 590,\n",
       " 'jesus': 591,\n",
       " 'going': 592,\n",
       " '5k': 593,\n",
       " 'feel': 594,\n",
       " 'dmg': 595,\n",
       " 'party': 596,\n",
       " 'supports': 597,\n",
       " 'avg': 598,\n",
       " 'while': 599,\n",
       " 'early': 600,\n",
       " 'ahhaha': 601,\n",
       " 'lang': 602,\n",
       " 'acc': 603,\n",
       " 'buyer': 604,\n",
       " 'winning': 605,\n",
       " 'scared': 606,\n",
       " 'hahaa': 607,\n",
       " 'feeder': 608,\n",
       " 'spec': 609,\n",
       " 'seen': 610,\n",
       " 'calling': 611,\n",
       " 'fcking': 612,\n",
       " 'idc': 613,\n",
       " 'huh': 614,\n",
       " 'damn': 615,\n",
       " '12': 616,\n",
       " 'any': 617,\n",
       " 'slardar': 618,\n",
       " 'taht': 619,\n",
       " 'way': 620,\n",
       " 'necro': 621,\n",
       " 'buyback': 622,\n",
       " 'understand': 623,\n",
       " 'zero': 624,\n",
       " 'doing': 625,\n",
       " 'died': 626,\n",
       " '50': 627,\n",
       " 'nc': 628,\n",
       " 'jsut': 629,\n",
       " 'name': 630,\n",
       " 'together': 631,\n",
       " 'braindead': 632,\n",
       " 'jug': 633,\n",
       " 'money': 634,\n",
       " 'getting': 635,\n",
       " 'bots': 636,\n",
       " 'attack': 637,\n",
       " 'fuc': 638,\n",
       " 'garbage': 639,\n",
       " 'cancers': 640,\n",
       " 'ruin': 641,\n",
       " 'creep': 642,\n",
       " 'alch': 643,\n",
       " 'unbelievable': 644,\n",
       " '2nd': 645,\n",
       " 'comend': 646,\n",
       " 'core': 647,\n",
       " 'cum': 648,\n",
       " 'could': 649,\n",
       " 'dc': 650,\n",
       " 'keep': 651,\n",
       " 'worse': 652,\n",
       " 'each': 653,\n",
       " 'lock': 654,\n",
       " 'since': 655,\n",
       " 'pathetic': 656,\n",
       " 'pretend': 657,\n",
       " 'sense': 658,\n",
       " 'm': 659,\n",
       " 'abandon': 660,\n",
       " 'part': 661,\n",
       " 'bois': 662,\n",
       " 'tango': 663,\n",
       " 'ahahah': 664,\n",
       " 'ako': 665,\n",
       " 'w8': 666,\n",
       " 'mates': 667,\n",
       " 'iq': 668,\n",
       " 'says': 669,\n",
       " 'fuk': 670,\n",
       " 'damage': 671,\n",
       " 'calm': 672,\n",
       " 'lady': 673,\n",
       " 'tusk': 674,\n",
       " 'com': 675,\n",
       " '14': 676,\n",
       " 'years': 677,\n",
       " 'fck': 678,\n",
       " 'idiots': 679,\n",
       " 'bane': 680,\n",
       " 'tq': 681,\n",
       " 'dulan': 682,\n",
       " 'role': 683,\n",
       " 'steal': 684,\n",
       " 'ahead': 685,\n",
       " 'v': 686,\n",
       " 'bitches': 687,\n",
       " 'faggots': 688,\n",
       " 'son': 689,\n",
       " 'hope': 690,\n",
       " 'vagina': 691,\n",
       " 'dicks': 692,\n",
       " 'thinks': 693,\n",
       " 'whore': 694,\n",
       " 'ure': 695,\n",
       " 'unvoker': 696,\n",
       " 'ehh': 697,\n",
       " 'reprot': 698,\n",
       " 'commended': 699,\n",
       " 'zzz': 700,\n",
       " 'someone': 701,\n",
       " 'else': 702,\n",
       " 'hitting': 703,\n",
       " 'amazing': 704,\n",
       " 'bruh': 705,\n",
       " 'drow': 706,\n",
       " 'remember': 707,\n",
       " 'griefers': 708,\n",
       " 'trap': 709,\n",
       " 'ee': 710,\n",
       " 'cour': 711,\n",
       " 'ping': 712,\n",
       " 'hand': 713,\n",
       " 'ahahaha': 714,\n",
       " 'happening': 715,\n",
       " 'asking': 716,\n",
       " 'works': 717,\n",
       " 'yours': 718,\n",
       " 'emd': 719,\n",
       " \"'ve\": 720,\n",
       " 'looking': 721,\n",
       " 'bristle': 722,\n",
       " 'alright': 723,\n",
       " 'gryo': 724,\n",
       " 'pango': 725,\n",
       " 'save': 726,\n",
       " 'wombo': 727,\n",
       " 'combo': 728,\n",
       " 'aggro': 729,\n",
       " 'switch': 730,\n",
       " 'snow': 731,\n",
       " 'exactly': 732,\n",
       " 'dis': 733,\n",
       " 'jus': 734,\n",
       " 'ded': 735,\n",
       " 'm9': 736,\n",
       " 'yah': 737,\n",
       " 'skrra': 738,\n",
       " 'spirit': 739,\n",
       " 'fukked': 740,\n",
       " 'ima': 741,\n",
       " 'carries': 742,\n",
       " 'frost': 743,\n",
       " 'kewl': 744,\n",
       " 'oi': 745,\n",
       " 'wrong': 746,\n",
       " 'ayy': 747,\n",
       " 'world': 748,\n",
       " 'their': 749,\n",
       " 'own': 750,\n",
       " 'bullshit': 751,\n",
       " 'sth': 752,\n",
       " 'born': 753,\n",
       " 'brain': 754,\n",
       " 'little': 755,\n",
       " 'blind': 756,\n",
       " 'nap': 757,\n",
       " 'low': 758,\n",
       " 'problems': 759,\n",
       " '45': 760,\n",
       " 'naga': 761,\n",
       " 'turbo': 762,\n",
       " 's': 763,\n",
       " 'continue': 764,\n",
       " 'guess': 765,\n",
       " 'ppl': 766,\n",
       " 'yep': 767,\n",
       " 'lads': 768,\n",
       " 'christmas': 769,\n",
       " 'autistic': 770,\n",
       " 'till': 771,\n",
       " 'enemy': 772,\n",
       " 'plays': 773,\n",
       " 'whining': 774,\n",
       " 'commends': 775,\n",
       " 'fucks': 776,\n",
       " 'tards': 777,\n",
       " 'muuuu': 778,\n",
       " 'realzie': 779,\n",
       " 'started': 780,\n",
       " 'feels': 781,\n",
       " 'ofc': 782,\n",
       " 'idk': 783,\n",
       " 'waht': 784,\n",
       " 'live': 785,\n",
       " 'such': 786,\n",
       " 'talker': 787,\n",
       " 'realize': 788,\n",
       " 'change': 789,\n",
       " 'high': 790,\n",
       " 'something': 791,\n",
       " 'wut': 792,\n",
       " 'loosing': 793,\n",
       " 'aim': 794,\n",
       " 'pl': 795,\n",
       " 'boost': 796,\n",
       " 'paid': 797,\n",
       " 'put': 798,\n",
       " 'yooo': 799,\n",
       " 'alone': 800,\n",
       " 'xdddddddddd': 801,\n",
       " 'magnus': 802,\n",
       " 'dickhead': 803,\n",
       " 'ancient': 804,\n",
       " 'fiend': 805,\n",
       " 'creeps': 806,\n",
       " 'cocky': 807,\n",
       " 'mi': 808,\n",
       " 'un': 809,\n",
       " 'puto': 810,\n",
       " 'el': 811,\n",
       " 'miren': 812,\n",
       " 'con': 813,\n",
       " 'indoker': 814,\n",
       " 'maldita': 815,\n",
       " 'opm': 816,\n",
       " 'invokerr': 817,\n",
       " 'makes': 818,\n",
       " 'ahha': 819,\n",
       " 'yo': 820,\n",
       " 'juked': 821,\n",
       " 'burned': 822,\n",
       " 'nie': 823,\n",
       " 'w': 824,\n",
       " 'mial': 825,\n",
       " 'though': 826,\n",
       " 'players': 827,\n",
       " 'b': 828,\n",
       " 'close': 829,\n",
       " 'sentry': 830,\n",
       " 'fingering': 831,\n",
       " 'mum': 832,\n",
       " 'abort': 833,\n",
       " 'finger': 834,\n",
       " 'once': 835,\n",
       " 'gone': 836,\n",
       " 'kinda': 837,\n",
       " 'script': 838,\n",
       " 'kind': 839,\n",
       " 'impossible': 840,\n",
       " 'pussies': 841,\n",
       " 'noting': 842,\n",
       " 'omfg': 843,\n",
       " '300': 844,\n",
       " 'salty': 845,\n",
       " 'pangolier': 846,\n",
       " 'whine': 847,\n",
       " 'bought': 848,\n",
       " 'midas': 849,\n",
       " 'recon': 850,\n",
       " 'dk': 851,\n",
       " 'taco': 852,\n",
       " 'reporting': 853,\n",
       " 'sunstrikes': 854,\n",
       " 'sunstrike': 855,\n",
       " 'english': 856,\n",
       " 'servers': 857,\n",
       " 'least': 858,\n",
       " 'arcana': 859,\n",
       " 'gave': 860,\n",
       " 'learn': 861,\n",
       " 'holding': 862,\n",
       " 'hardly': 863,\n",
       " 'kept': 864,\n",
       " 'muted': 865,\n",
       " 'account': 866,\n",
       " 'pos': 867,\n",
       " 'io': 868,\n",
       " 'prank': 869,\n",
       " 'tide': 870,\n",
       " 'late': 871,\n",
       " 'pay': 872,\n",
       " 'came': 873,\n",
       " 'alr': 874,\n",
       " 'losing': 875,\n",
       " 'tp': 876,\n",
       " 'ahahahahah': 877,\n",
       " 'none': 878,\n",
       " 'loss': 879,\n",
       " 'piece': 880,\n",
       " 'universe': 881,\n",
       " 'legend': 882,\n",
       " 'weak': 883,\n",
       " 'believe': 884,\n",
       " '1on5': 885,\n",
       " 'hp': 886,\n",
       " 'smarter': 887,\n",
       " 'maybe': 888,\n",
       " 'serious': 889,\n",
       " 'seriously': 890,\n",
       " 'fuckers': 891,\n",
       " 'sick': 892,\n",
       " 'suicide': 893,\n",
       " 'sb': 894,\n",
       " 'strong': 895,\n",
       " 'offlaner': 896,\n",
       " 'shitty': 897,\n",
       " 'youre': 898,\n",
       " 'shaman': 899,\n",
       " 'allies': 900,\n",
       " 'worry': 901,\n",
       " 'drug': 902,\n",
       " 'two': 903,\n",
       " 'sound': 904,\n",
       " 'e': 905,\n",
       " 'those': 906,\n",
       " 'f': 907,\n",
       " 'tho': 908,\n",
       " 'ive': 909,\n",
       " 'before': 910,\n",
       " 'tbh': 911,\n",
       " 'sounds': 912,\n",
       " 'dusa': 913,\n",
       " 'tilted': 914,\n",
       " 'aba': 915,\n",
       " 'wish': 916,\n",
       " 'joker': 917,\n",
       " 'dipshit': 918,\n",
       " 'carried': 919,\n",
       " 'realy': 920,\n",
       " '1k': 921,\n",
       " 'study': 922,\n",
       " 'move': 923,\n",
       " 'forgot': 924,\n",
       " 'sucks': 925,\n",
       " 'cmon': 926,\n",
       " '3v1': 927,\n",
       " 'peace': 928,\n",
       " 'throne': 929,\n",
       " 'wid': 930,\n",
       " 'tried': 931,\n",
       " 'looool': 932,\n",
       " 'aid': 933,\n",
       " '28': 934,\n",
       " 'later': 935,\n",
       " 'hehehe': 936,\n",
       " 'nnoob': 937,\n",
       " 'sup': 938,\n",
       " 'should': 939,\n",
       " 'stay': 940,\n",
       " 'terrorist': 941,\n",
       " 'foq': 942,\n",
       " 'boom': 943,\n",
       " 'dizasta': 944,\n",
       " 'tang': 945,\n",
       " 'paper': 946,\n",
       " 'mag': 947,\n",
       " 'j': 948,\n",
       " 'rec': 949,\n",
       " 'safe': 950,\n",
       " 'venge': 951,\n",
       " 'hlep': 952,\n",
       " 'dager': 953,\n",
       " 'respect': 954,\n",
       " 'supposed': 955,\n",
       " 'pheonix': 956,\n",
       " 'urself': 957,\n",
       " 'forever': 958,\n",
       " 'oracle': 959,\n",
       " 'scepter': 960,\n",
       " 'riki': 961,\n",
       " 'ah': 962,\n",
       " 'touch': 963,\n",
       " 'focus': 964,\n",
       " 'safelane': 965,\n",
       " 'monkeys': 966,\n",
       " 'jst': 967,\n",
       " 'ban': 968,\n",
       " 'berd': 969,\n",
       " 'roberd': 970,\n",
       " 'freefarm': 971,\n",
       " 'jebaited': 972,\n",
       " 'roshan': 973,\n",
       " 'torrent': 974,\n",
       " 't2': 975,\n",
       " 'most': 976,\n",
       " 'person': 977,\n",
       " 'pizdec': 978,\n",
       " 'idea': 979,\n",
       " 'deso': 980,\n",
       " 'teams': 981,\n",
       " 'average': 982,\n",
       " 'totally': 983,\n",
       " 'arent': 984,\n",
       " 'wudnt': 985,\n",
       " 'ru': 986,\n",
       " 'rq': 987,\n",
       " 'wood': 988,\n",
       " 'dd': 989,\n",
       " 'animal': 990,\n",
       " 'nooob': 991,\n",
       " 'disruptor': 992,\n",
       " 'ending': 993,\n",
       " 'crying': 994,\n",
       " 'reason': 995,\n",
       " 'chrono': 996,\n",
       " 'fountain': 997,\n",
       " 'dad': 998,\n",
       " 'faster': 999,\n",
       " 'parasite': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_file, tokenizer_file = word_embedding_and_tokenizer_file_name(50, True)\n",
    "print(word_embedding_file)\n",
    "print(tokenizer_file)\n",
    "#load word embedding\n",
    "word_embedding = np.loadtxt(word_embedding_file, delimiter=',')\n",
    "#load tokenizer \n",
    "with open(tokenizer_file, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    320\n",
       "1    180\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/dota2_chat_final.csv\", encoding=\"latin_1\", usecols=[\"category\", \"match\", \"slot\", \"text\"])\n",
    "data[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes dog. yeah . fast and furious. too fas. hah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>no idiot. we too pro. lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>HAHAH. COMMEND ME TY. EZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>so ya mama likes dick ehh?. figures. ur not ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>reprot. SAD. fucking reported axe. WORST HOOK ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  match  slot                                               text\n",
       "0         0      2     0  yes dog. yeah . fast and furious. too fas. hah...\n",
       "1         0      2     2                          no idiot. we too pro. lol\n",
       "2         0      2     4                           HAHAH. COMMEND ME TY. EZ\n",
       "3         1      6     0  so ya mama likes dick ehh?. figures. ur not ev...\n",
       "4         1      6     1  reprot. SAD. fucking reported axe. WORST HOOK ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>547.542000</td>\n",
       "      <td>4.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.480481</td>\n",
       "      <td>810.300145</td>\n",
       "      <td>2.853384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>126.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>263.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3472.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category        match        slot\n",
       "count  500.000000   500.000000  500.000000\n",
       "mean     0.360000   547.542000    4.478000\n",
       "std      0.480481   810.300145    2.853384\n",
       "min      0.000000     2.000000    0.000000\n",
       "25%      0.000000   126.500000    2.000000\n",
       "50%      0.000000   263.500000    5.000000\n",
       "75%      1.000000   389.000000    7.000000\n",
       "max      1.000000  3472.000000    9.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = data.drop(columns=['category', 'match', 'slot']).to_numpy().flatten()\n",
    "y_raw = data.drop(columns=['text', 'match', 'slot']).to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_raw, y_raw, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.08130081300813\n",
      "17.73127753303965\n"
     ]
    }
   ],
   "source": [
    "toxic_sentences_train = []\n",
    "non_toxic_sentences_train = []\n",
    "for index in range(len(X_train_raw)):\n",
    "    word_length = len(word_tokenize(X_train_raw[index]))\n",
    "    if(y_train_raw[index] == 1):\n",
    "        toxic_sentences_train.append(word_length)\n",
    "    else:\n",
    "        non_toxic_sentences_train.append(word_length)\n",
    "        \n",
    "\n",
    "print(np.average(toxic_sentences_train))\n",
    "print(np.average(non_toxic_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.75438596491228\n",
      "20.021505376344088\n"
     ]
    }
   ],
   "source": [
    "toxic_sentences_test = []\n",
    "non_toxic_sentences_test = []\n",
    "for index in range(len(X_test_raw)):\n",
    "    word_length = len(word_tokenize(X_test_raw[index]))\n",
    "    if(y_test_raw[index] == 1):\n",
    "        toxic_sentences_test.append(word_length)\n",
    "    else:\n",
    "        non_toxic_sentences_test.append(word_length)\n",
    "        \n",
    "\n",
    "print(np.average(toxic_sentences_test))\n",
    "print(np.average(non_toxic_sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gg', 'go next. nice',\n",
       "       'gg. wow gyro got aghas. end. wombo combo. ez top',\n",
       "       'WELL PLAYED SHIT CUNTS',\n",
       "       'noob. fuck pussys . go. BG fucking idiots go play normal all pick. 36 min. haha noob',\n",
       "       'i will show this game to all my support. how to win. oracle win this shit solo',\n",
       "       'so ya mama likes dick ehh?. figures. ur not even a good hooker kid. passive shadow blade?. gg',\n",
       "       'lol. why even try?. Weaver',\n",
       "       'EZ. medusa. hahaha. lol. YEAH. good to throw . AHAHA. when you got noob carry :D . AHHAA. paasive . only 2nd ans ulti. WT. WTF. paassive medusa. noob . sk . fucking noob medusa. SAD',\n",
       "       'wtf. ok', '++. nice bkb. Ez wk ez mid ez mirana. boring ass game',\n",
       "       'nice comming ta. Cm me. Ez. delet dota. Ez',\n",
       "       'fuck your moms. lich offlane. says. it. all. hahahahahahahaha',\n",
       "       'bro. U CAN. SEE AXE?. CALL?. gg',\n",
       "       'gg. we havent support. this invo so fucking asshole. AFK . I AM AFK. SO END FAST. NO HOPE. Cry ??? i said we need support. why you pick invo then  ???. are you lost your mind ??. KID. waity 1  kiniute. dont care bitch. just end . end fast . never want tagaian',\n",
       "       'oomg. noob team. fuck my team. fuck my teaaaaaaaaaaaam. uselees invoke',\n",
       "       \"end. can't win with wr support and invoker offlane. report this invoker\",\n",
       "       'sorry nex. i killed u . almost . gg',\n",
       "       'SUPPORT MEEPO BOIS. He is a fucking terrorist. So it makes sense. cAN YOU ASK THE HUSKAR TO STOP. I dont know how meepo works and im being bullied. Dick in me',\n",
       "       'these bashes. lol. their 2 stack was perfect aparently. pro tier plays. from the offlane. gg I guess',\n",
       "       'Ez mid ,ez game. :*',\n",
       "       'GG. MID. REPORT MID. REPORTEN AL MID. REPORT . MID. PLS. REPORTEN A MI MID . DE MRDA. ES UN PUTO CIEGO . LOL. LE METI AL CREP. EL ULTI JAJAJA. REPORTEN. AL INVOKER. MIREN ESA. MRDA. REPORT MID. PLS. GG. CON EL INDOKER. DE MRDA. GG. MIREN A INDOKER. MUERETE. MALDITA. RATA. TIENE. 384 de opm . MALDITA. RATA. 387 de opm y va mid con invoker . EN MID. Y CONCHA. TU MADRE. QUE CHUCHA HACIAS . TU SALIR . DE TU TORRE. A MORIR . RATA DE MRDA. ACABEN. GG. REPORTEN AL MID DE MRDA. QUE ME TOCO',\n",
       "       'gege. i think. lmao. brainless. im done ty. 10 min bf. ^^. our first pick. sad. gege. im already abbandon . end it faster ty Xd. so hard take 3 hero XD. i mean use. XD. cumoout. lol',\n",
       "       \"gotta love to see ur underlord now. no really. do u see ur mama. imma piss on her. nice stick. niuce one. hi. nice winter. so pro. do u realzie. ur mom is fucking slut. nice one. retard. instead of moving. u killed es. waw. 2-5. 5 death. 13 mins. stop feeding dog. ahahaha. k. haha. shes fine. how's ur. urs. 21-16. fuck ur sister. haha. ahaha. aiight man. sa. sad. stfu. now u taljk. i killed 10 times. 16 kills. dont talk. when i die. retards. Lmao. retards. u cant win. they really think. puck can do anything. with his eul's. 9k ulti. underlord. 2-9. its ok. ur support. so its ok if u feed. we rosh. ROFL. ROFLLLLLLLLLLLLLLLL. winter. bb. go back boys. taking viper. into pa. now im ursa into . Underlord. simply. man. thats it. yeah man. last gtame. this pa. u was like. ur pro an. man. u picked. viper. u started man. u said. this pa. hhahahhaha. feels good. to win mmr. and feels bad. for u man. haha. ofc ez. u countered me\",\n",
       "       'k. tard?. ). ). )',\n",
       "       'sto)p cheat ulty 0 dmg ss stop cheat. ok sentry . 0reaction team 0 def team 0 info team pls ff enmys. only cheat. stop cheat trool. ss to',\n",
       "       'y me rubick?. poor me haha. give a bless. wp. commend me bro. hahahah',\n",
       "       'gg. AND?. IDC!. AND ?. UR HANDSOME WITH THAT ?. BECAUSE UR FUCKIN PRO. HEHE. PRO. WHY SO FAST LIKE BOT. FIGHT HIM HAHA. GG. SORRY FOR FEEDING MID. HEHE. PROEST TA EVER. SHUT UP UR PRO. FUCK UR REFRACT LOL. END :). END HEHE. BLAME ME TEAM :(. HEHE. WIPE. IM PASSIVBE EHHE. |YAH. HEHE GG. HEHE GG. GG REPORT ME SIR',\n",
       "       'lol. ayyyyy yooo chill. leave me alone. gg wp',\n",
       "       'now the natures buys right click items. fuck me',\n",
       "       'Good luck, get fucked.. gg',\n",
       "       'lower. you guys 3 top ...but did not any use',\n",
       "       \"lol . xdx. lol. xdxd. blame my noob teammates. lol jajajajajaja suckers got solo killed. lol noob. suck a noob. lol. my team lost. i won solo . lol noob PA. nope. lol. ez kill. now im the pro. you pussies better quit. commend pls. no joke lol poor gays. lol. lol'. poor BKB. lol. lol. lol. cute. lol noob. jajaja\",\n",
       "       'jus. end ursa. just end. rofl. this fucking ursa try hard. ursa is a cunt haha. gg',\n",
       "       'its toxic\\\\. your friend?. doomed?. GO FF. IM DONE WITH MOTHWEGR FUICKER. SVEN FUCK YOUR FAMILY. wnhy derf?. ok. commedn em ty. lol doom. nice feed',\n",
       "       'so ez. EZ jungle. HAHA',\n",
       "       'never pick my rune again bitch!. he gave up xD. sad he will never reach Walhalla :-D. xD. man riki you die like paper. for that shit i will make you bleed riki. Void Ulti and run like a bitch? :-D. omg. WTF was that :-D',\n",
       "       'WHO IS BEING A MEME. GGWP. EZ',\n",
       "       'Can you guys report lc please. thx bru. First time playing offlane. ez. report lc. thx. end. here. oh. sad game',\n",
       "       'happy new year _). GG',\n",
       "       'gg. gg. hahaha. fucking bb. mother fucker. kid. see mother fucker. duel. wp',\n",
       "       'ACTUALLY KUNKA WHO DOESNT HAVE SHADOW BLADE IS IDIOT. i have now SB im not idiot anymore. gg',\n",
       "       'fucktard russian. .... jebem ti poreklo mrtvo. you russians are retards. but this sniper. is most retarded. person i have ever played with. pizdec. pizdec. you all russian retards?. f WEIO ggf . d As. Das. a. im out. bye',\n",
       "       'ursa is goo dmid tho. ive never heard of storm mid before tbh. ursa stop . leave them alone. im gonna report ursa for ability abuse. ursa feeding. report',\n",
       "       '?. gg wp report OD. whining in chat. ?. ?. ?. Que tal?. ?. Whats wrong?. ?. ?. Que?. ?. ?. ?. ?. This WK ;). +. ez mid and top',\n",
       "       'lag', 'gg. you guys were winning early game. gg noobs',\n",
       "       'dont forget to report sf. sf. unisntall',\n",
       "       'u r so bad SF.  _. dickhead.  _.  _',\n",
       "       'WHO WANTS TO LIVE FOREVER. clan mcloud for lyf. omni does. so much damage. holy shit. LOL. I KNOW RIGHT',\n",
       "       'Yes. I didnt realise you had impetus, coz you were using frost arrows. fuuuck. I didnt mean to press it :(. Okay... you were literally just standing there this time. like, the fuck?. What were you doing?. It was there for ages, and I got it.... Yes. I am. just end guys. Seriously. Stop being dicks and fountain camping. Thank you',\n",
       "       \"Dota 2, where a 45 minute game takes 90 minutes. 25 min of queue, 20 minutes of disconnects, and 45 minutes of game. I'm relaxed. Making a joke mate. You mad mate?\",\n",
       "       'is this game serious. this game. SERIOUSLY MOTHER FUCKERS. wet. GG',\n",
       "       'yo. where the fuck is snow. snowy terrain. juked . bby. nope. u burned my mana. good still. funny situation. almost pooped myself xD. when u burned mana. kamil stoch to ciota i chuj Xd. no . Xd. on nic nie ma w 23 minuty. a huskar mial 1/4. xDDDDD. nie sraj. druzyna gierki Ci przegrywa. a tak to bys mial 6k?. :/',\n",
       "       'please i begging you . report void ', 'sad. carry',\n",
       "       'haha. LOL. HAHA. ???. ?????????. ?. fat boi. haha. nice deny. ?. worth?. ?. worth?. easy. noob jugg. 1/8?. tsk. getting destroyed by herald axe. haha. ???. say that when u win. lol. ok. i shall wait. xd. jugg?. haha. lol. ez jugg. haha. ?. cuz i scared. ez. jugg. jugg?. ez. ez jugg. jugg. omni me. ?. haha. ez',\n",
       "       'fuck this shit nap team. rly',\n",
       "       'PUTIN SUCK DICK FAGGOT RETARD AUTIST CAR TRUNK CLIFF FUCKER DIE . long we have waited. now we jebaited. fight me. if u got balls. faggots. i coached that ember btw. press 1 if u think legion is a retard. 1',\n",
       "       'can win. let us end fast',\n",
       "       'so lucky. Sick wardtrap. Wtf are you doing',\n",
       "       'fucking danny. ayy. idk what teals talking abou. what the fuck. whys ursa talking. did u play earlier. STOP IT. waht is hapenign. what the fuck. sniped. what a world we live in. MAN URSA IS SUCH A TALKER. Jesus. im not 18 yet. MOO. MAN THIS URSA. dudes. dude',\n",
       "       'nice combo. reported. just cuz i look. like a terrorist. doesnt mean i am part of you',\n",
       "       'lol. i get it. jesus resurrects. u tried',\n",
       "       'gg. monkey blocks my hook, i said he was going to pick huskar what a game.. twice i get this shitty brood as  a core.',\n",
       "       'It fucked me more than you.. RAGE',\n",
       "       \"why would heralds pick mid. dead herald mid. again top. by selling ur mom's anus?. 7-27 still they out of base. at t2. wait dont racks\",\n",
       "       '). can we guys. together. report our midder. thx. hes hero. can solo. win this game. but he braindead',\n",
       "       'i like how my team saying how im losing when im under lvl once i reconnected. WHAT A JOE. JOKE. stfu lc mid. is ez to win. U GOT A 2nd move. TO COUNTER MY MISS. but my team dont know tha. they nv seen lc mid. what a joke. sorry lc come back is real. my es from last game came through. its cool. lol i carry him last game. he carry me this game. sounds fair. anyway tinker counter your teamate but u counter me. GG',\n",
       "       'AHAHAHAHAH. insert coin, game over. AHAHAHAHAh',\n",
       "       'lel. huh. calm lady boy. owakowak. NGAKAK. CALM LADY BOY. dont cry lady boy. DONT CRY. dont touch my support. COME. I AM NOT UR CLASS MAN. UR NOT MY BRACKET. i am not ur clas',\n",
       "       'what happened. the picks were fine. we were jsut edgy. we were gonna mid kunkka. ursas just a king. did anything except feed lol. sounds good. ggwp',\n",
       "       'yes when u die 21 timkes. times. idiot. bristle. report. 21 death. is it olympic games?. +. GG. report bristle',\n",
       "       'pangolier mid. feeder hardlane. they whine cuz i bought midas. well, were we able to do shit?. ok. pro',\n",
       "       'haha. SVEN. HATE ME?. HAHAHA. AHHAHA. NOOBS. SVEN. HAHHA. HAHAHA. STROM. CARRY U. DATS ALL. HAHA. NOOB. HAHA. EZ. EZ. SVEN. EZ. HAHAH',\n",
       "       'GG WP. lol. ((((. (((((((((((((((((. leg wanna party rating?. Lol no. Fuk this game. ????. Rofl',\n",
       "       'end please. Pick pho as offlane rofl. how to win then no one can kill weaver and am',\n",
       "       'report creep. im mid feeding guys. punishment for these kids. no i wont. im just makin sure they dotn win. 1- they flame each other. 2- they are 1.5k mmr below me. 3- they are bad. and 4- they afk farm next to me. so im punishing them. this is the only way that u can bring noobs to play. intentional feed they calm down. Focus on flaming u. DAzzle. furi skill level = u x 109. venge is on a completely different level',\n",
       "       'have fun, lads. sorry guys', 'lag the fuck up',\n",
       "       'wp. ?. :D. problems?. this invo rekking. gg wp. i ended. as i said. gg wp. commends',\n",
       "       '?. wow. small id also pro like that. i will show u the real sven. injoker. dun use small id to lie other ppl. u still injoler. me?. lol. make sure u can kill me 1st. this is lie kids',\n",
       "       'ez mid ez ame. report pudge. not fun . pudge need dd. hahah. this pudge so fucking stupid. iq monkey. 10 max. end. gl next . go end guys  . lol . rubick buy wards in ths mode. wtf ?. ez russia ',\n",
       "       'axe is axe', 'dc od. wait please. Thanks. GG',\n",
       "       \"afker. 30 sec no response i go. we'll just go and report this dude. he returned\",\n",
       "       'that stick lion. felt bad right?',\n",
       "       \"gg no0 tea. gg no team. ez mid. ez my mid. ez sniper's feeder. report. report invokerr. Ez mid report. only that. report invokerr. please. so report invo. somen te. Ez\",\n",
       "       'YOOO. lul. wp wp . you fookin clown magnus',\n",
       "       'ABORT ABORT. finger you to death',\n",
       "       'dont kill me i will commend . commend > 25 mmr . cmon man think about it . idc mmr i care commends. xD. me?. jebaited.com . nice hero bro. bulldog fans 1 hero pool LUL . gg',\n",
       "       'gl hf. please dont kill me. im kinda learning here. 2 slow. nop. tiny. small favor. STOP THROWING ME. get a hobby or smthing. lol. ninja. fu. gg. ez',\n",
       "       'fuck off mid . he cant do any more. sure gg. 17minutes fuck off rd??. die. Ez. MMSP. FUCK OFF MID. WAHT you can ??. fuck off mid farm and die. Ez. ez mid. fuck offf. report this mid. he wanna compare support. ez fucker',\n",
       "       'heartshaker. man that silence on axe. Gg',\n",
       "       'ok. noob lc. useless lc and dark. gg i dont have team ]. lc and moph useless. ez lc. the must noob lc ever. loot at this moph with 2 skadi lo. lol. pls report my team . pls report moph and lc. pls report lc and moph',\n",
       "       'Fuck yourself. Quit. bb zeus. Oo cumback. check this. Miss mis miss. :D. cry bitch. :D',\n",
       "       'its took 3 year',\n",
       "       '?. i will commend u sniper if u commend me too. gg',\n",
       "       'Lol. all of us no tango. just axe. Dafaq?. Lol. epic fail. foq. Hah. Lol. foq. Lol. hahaha. Haha. Lol. haha ggwp. party???. Haha. boom. panis. ahahah. boom. Dead. lmao. wow. blame pa ako?. haha. Lol. hala oii. anu ba yann. aray. sakit amp. Lol. Haha. Aw. bweset. gang bang. that pudge]. cum back??. Lol. aw. Aw. ggwp. haha. Lol. haha. its dizasta!!!. ohh its from universe its a dizasta!! ppd!!. Awww. wp gg. well played good game. ggwp. haha',\n",
       "       'was a pleasure 1v4 mid though. worked out well',\n",
       "       'buy dominator and necro. finish fast pls. can u just finish fast pls?. braindamaged dog. just go and finish this game. why u farm',\n",
       "       'im tilted af wid sf. m so done. practise wid bots how to grave teammates at what time. dreamy girl',\n",
       "       \"Rofl. Shut the fuck up man. Yes. its funny. We have ember. on safe lane. Who thinks he is miracle. dude. You're. so fucking shit. player. you're . Shit. man. You're not in eu. TOP. 200. EU. ROFL. blink for what?. i got . Ward?. Sry. i already. Have. A anceint acc. top 200 eu. is most retareded person i ever seen. no you cant. its sad game. Everyone think he is miracle. ^^. you tusk asking . you to buy . ward. nothing more. ember no need. linken\",\n",
       "       'lol. slardar. gg. recon. coming. please wait. am more',\n",
       "       'nice carry husk ',\n",
       "       'GG FUCKING TEAM. GG TEAM. USeless FUCKING BARA. USELESs. GGWP. END. PLEASE. PLEASE END THIS GAE. ILL REPORT MIRANA. POLEASE LET HELP US. LET MIRANA GO TO LP. EA GAME. EZ. END. IDIOT. EZ. EZ. GG EZ tEAM. ILL SWAP COMMEND JUST REPORT OUR TROLL. EZ. EZ. Reported troll. WOW BARA. WOWO. GGWP. EZ GAME. FUCKING IDIOT TEAMMATES. OUT ROLL IS FUCKING FARMING. SO FUCKING IDIOT',\n",
       "       'dont listen to him', 'wait. my puck .  have lag',\n",
       "       'fuck this team',\n",
       "       'whatever m,an. i give u. fucking retards. watching bot . waiting for void to fucking chrono. fucker jerking off. gg. im fucking done. fucking . feed the sf. and blame. srsly. 0-2. 1-6. joke supports. 5-0. fucking trash supports. just report them. 1-4 3-9 0-3. so pro. yea im fucking done. this is literally a fucking 2v8 game. me and void. aginst these  fucking retards. lol we lost. gg. fucking carry so hard. fucking shit ass lion. WHO THE FUCK NEEDS LION. WHO THE FUCK NEEDS PUDGE. GG',\n",
       "       'nice feed top trol and spectra',\n",
       "       'afk sorry. viper whats ur base dm. Dmg. gg xd. 1 by 1. ;/. gg',\n",
       "       'THERE CAN ONLY BE ONE. GLHF. rude. <3. ah well. gg',\n",
       "       \"Thank u for the laugh Lads. Commend Troll. For being a troll. xDDDDDDDDDDDDDDDDDD. Merry Christmas. He's autistic\",\n",
       "       'put that wards', '^. pay to win. GEGE',\n",
       "       'gl and hf. ff. can u pls finish this ?. just go mid. it is useless to play with this medusa. medusa is just here to ruin this game. no. ty for ruin this game medusa. bg',\n",
       "       'new meta es, blink for wat. we sure lose, but we never give up, we will def till the end. fucking hard game ever when got lucky timing skill on enemy. lol es, blink for wat',\n",
       "       'we have support invoker here. first pick invoker average last hits 100. and he wants mid. easyreport?. 100 avg last hits. first pick mid invoker. im getting ready a report for mid. 100 last hits mid. wow. OVER 20 games. over 20 games. i use supp each game more than 100 last hits. fucking invoker. FIRST PICK MID. 100 avg last hits. useless cunt. useless pa. fail dog with so much supp. brainless cunt. so fucking useless. 0-3. lol. pretend now. 0-3 barking useless. and pretend. a team of useless dogs. useless carry. fucking pa dog. our pa. totally useless. totally. lvl 18 rubik. lvl 16 pa. fucking dog. nah. report offlane and pa. idiot. see luna. fucking idiot offlaner. useless cunt. gg morons. useless pa. 0 dmg',\n",
       "       '?. XD?. ??. XDD. U . are fucking retard ursa. u know that?. URSA. HELLO. URSA. TELL SOMEYHING. HOW YOUR ROSH. how you feel to loosing second game. vs me. ?. ??. and u loosing game. for teammates. wp man. XD. you are shit. man. cant win game. as pa. from free farm. now u pick counter vs me. and u think u are good. whatever. pa isnt cancer?. whatever. be happy. dota is all your life. you fucking nerd. u dont even leaving house. fat nigga. whatyever. last game. was ez. vs you',\n",
       "       'get leaver have feeder huh?. like your ex. :D. gg',\n",
       "       'wp. nice try  ', 'Wait', 'HAHA. hes good . HAHA',\n",
       "       'ill out retard your feed dont worry. lol',\n",
       "       '?. ?. my team doesnt deserve to get out of lp. so bye. just goinna serll my items and dc. ppl qwith lowm iq deserve lp forever',\n",
       "       'we w8ed your fucking weaver. have manners trash. russian tards. Really nigga?',\n",
       "       '24 armor lvl 14 hero. balanced af',\n",
       "       'ck you need to study, half your senteces make no sense',\n",
       "       'not our fault u have lag. gg sry about ur lan internet',\n",
       "       'RETARDS VS FEEDERS. LOL. EZ MOERPH. TY. LOL. gg?',\n",
       "       'my fucking tide. running. he not. even hitting. fucker. why. 5k. all suck',\n",
       "       'chaos is all yours boys. if hes there I WILL NOT BE. you have my word. hes a spoiled child that needs a dick in his ass',\n",
       "       'ok. wp bh. i had him muted. what did he say?. gg',\n",
       "       'farming lord AM',\n",
       "       'g. gg. gg. rs. end. End. pinoy trash. ez ancient. kontol. ngentot ya. anjing',\n",
       "       'cmon guys 3v1 really?. how do you like it?. thats what your mum said',\n",
       "       'you speak idiot?. and you speak ignorant?. wtffffffff. I HAD ENOUGH WTF I WAS ABOUT TO ULT. WTF. HOWD YOU DO THAT WTF. RIGHT AFTER I ATTACKDE THE BLADEMAIL APPEARED BUT I STILL DIED. wp lina. tu es mierda. ty . shhhhhhh. i cant do anything idiot gay hero. wp lina. No. Wtf. trabajar. mierda. puto. no lol. i am oklahoman. i am white. i an 14 years old. nah packin 9.5 inches wbu. im 14 white boy i speakl english spanish french german arabic and mandarin. You are part of that peruvian team. same. my invis doesnt come off when i attack. rasonable xD. reasonable peruvian. GG WP COMMEND ROSHAN EZ',\n",
       "       'Report lich for cyring. please report medusa for being useless',\n",
       "       '?. gg. never 1st spec again',\n",
       "       'he says he will be right back. there he is . how the fuck did we win that . is lc talking to tinker?',\n",
       "       'gege. they dont have. buyback. go thorne. throne',\n",
       "       'this is called balance. i hate doin this. i know. i hate myself. alreadyt. its just. tjhis shitty hero.  i hate her. this hero is broken. ill start spamming. we have sf if u didnt notice',\n",
       "       ')). 180. niiice. ahaqha. lose mid in pudge. ez. ahah',\n",
       "       'i missed the hill. LOL. ok', 'fcking noob hero. my god. gg',\n",
       "       'HAHAH. let sd carry. let me afk. HAHAHA. Rerpot sd later. HAHAHAHAHA. Wow. HAHAHa. Sd go. go sd. HAHAHA. HEHEHE. HAHAHAHa. nnoob. g oend. HAHAHAHa. wait let tk abandon. Sd is good. HAHAHA. Cancer sup. HAHAHA',\n",
       "       \"noob. ?. slow brains. slow cock ur dad. couldn't pull out in time. use condoms next time. slow?. cock. still faster than ur dad's cock. use condom next time uncle. stop spreading bad seeds. that doesn't even make any sense. trying to be witty arent' we?. sad. ez mid. yes. parasite. lmao. noob mid. practice more. ss. lmao. fact. ez mi. mid. practice moar. git gud. ez mid. plz. dont kid urself. ss. ezpz mid. yes. ok la. hahaha. still ez mid. parasite\",\n",
       "       'wahahahahaha. end this is bulshit gaming fucking sniper and pudge watching porn. porn gaming. huhu. okie pro pudge. pro team. pro tlga tong mga gago. nag tank na. naka tingin parin. okie ikaw mag tank gago. score lang tinignan. gogong. san utak mo. ??. run. new meta . def mo nalang sniper. d na kasi ako kaya ehh. lol',\n",
       "       \"Kill Slark.. I will block.. Rustard Slark spotted.. Slark top.. Care.. Nice blink with damage taken.. Ez volvo.. Slark is braindead.. + this game is so bugged.. It has become a joke of a game, lately.. 4/4 muted.. Ez game.. No communication=no win.. They're cavemen.. Fucking mongoloid.. How can someone EVEN GET TO STEAL HOOK!?. LIKE FOR REAL!. HOW LOW CAN ONE PUDGE BE!?. How can Rubick steal Hook?. HOW!?!?!?. 4 rustards in my team/.. Ye'.. YOU CAN ROT IMEDEATELY!. YOU FUCKING IMBECILE!. IT'S ALMOST IMPOSIBLE TO GET HOOK!. YOU HOOK/ROT!\",\n",
       "       'nothing to say to this team. wp wd. you deserve thi?? game wd. this sf so bad. sf reported. ty for game. report sf. +. nice afk. gg ty. i afk',\n",
       "       'bots play better u know',\n",
       "       'gg tide builld . 2 hits ? . really . XD. geee gee . Bois . Come back . Nice rosh ',\n",
       "       'nice. team. )). gg. axaxa. omg. OMG. useless. sky and mirana. can end ). :D: D. and?. privet :D. daun. end. afk',\n",
       "       'nice boost . how many did he paid ? . you know you have a guy at 16-0 ?. withh 600 avg gpm',\n",
       "       'first pick. btw', 'end pls. end. Ty',\n",
       "       'LOL. get off my dick pudge. u lost 2 towers 2 kill me. morons. great tp sniper. rough game',\n",
       "       'what a player. wowwwww',\n",
       "       'let the faggot be happy with his imba hero counter. see you in mid game. u actually that fucking bad ursa?. or feed?. do you realize u wont win this game?. how to change aggro :D. hows your mom ursa?. she just told me about your baby sized dick. she is not 18 yet. ur going to jail. high skill bash. if something. that is sad. 16 kills still lose. hehe',\n",
       "       'gg wp. report clock and tinker. or whatever',\n",
       "       'same like u. end. lul. sure. Read he name. just nothing. lul. 3-8 and 6-9',\n",
       "       'hf. gg. noob team. ez. lol wp. EZ NOOB GAME. wp brisle. tattack blade male . be again 10 years kid :D. pudge???. so noob team omg. even hit him is problem?. kokot. pudge feeded. thats it. gg. commend pudge for feeding. ez',\n",
       "       'i see. yeah. fucking hell. nmmice hero. but too bad u r not playing mid. rly. :). i am so bad. sorry. report this lc ty. GG',\n",
       "       '.|.. pl. can u give me bracer back plz. okay thanks you',\n",
       "       'G feed done mid. Pussy. g base?. Pussy?. Pussy?. G end mid. Tierd. 5 asshole move together. die together. Thx. Guys. ASshole. get fucked. So hard. Forgot game. Go study. You guys. Sucks. i was mid ck. gg. Fuc k you asshole. People',\n",
       "       'END. i lag. so mnuch. and us till gankl. fuc kfof bro. GG. GO. CONGfruckngcrats u kill algging ovker. ru not even good. jst kill me. im doNe. ye we had like. 60% packet. entire game',\n",
       "       'gg. ahahhahaha. holy shit. ok. i gotta take some deep breathes. these guys. hoooo. ok. im afk jungle. gg',\n",
       "       'lmfao. lol I was talking to my brother', 'im cancer. wtf. waste',\n",
       "       'nice supports. noob. pgg. pff*. report alche. stupid carry',\n",
       "       'fnoob. zero ward. 1v6. i buy ward. xd. noob team. ez. eam. buy ward. kunaka. you win. useess. kunaka. ez4',\n",
       "       'now he rolls. END . NEXT GAME. FUCK THIS PANGO. SAVE ROLL FOR NEXT GAME. SEE. LOOK AT HIM. PASSIVE. NOW HE ROLL. AGAIN. IF I KNOW HIM IN REAL LIFE. I WOULD HAVE KILL HIM. FUCKING NOOB PANGO',\n",
       "       'MORONS. MORON. dicks. HELP YOUR MID MORONS. DICKHEADS. MORONS. SO BORING. U IDKCHERDS. delete dota. fuck',\n",
       "       'stfu. ur a fucking cunt. stfu. lucky u. stom pro. u fucking noob. so am i lol. by 1?. yah ur pussy eater my boi',\n",
       "       'wow. cool. 0. 0. gg ez pa. pa realy shit. ?. ?. ?. look the shit pa. hahahahahahah . if u want kill pa so ez. trash pa. go dead plz. ez pa trasj. ez go fuck ur mather pla. plz. pa 1k trash',\n",
       "       'DONT FORGET TO REPORT NAGA',\n",
       "       'REALLY. WHAT THE. GG. AHAHHA. REALLY /. WHAT THE',\n",
       "       'enigma script test?. nice cheat',\n",
       "       'why rage when you just win 1k?',\n",
       "       'ez mid. mid?. lol. ez. no gank bara. ez gank . ez mid. ez. lol. gg commend me. xd. xd. lol. VERY. SPEC IN THE BAG!',\n",
       "       'wp. fuckkk youuuuu. ty pudge',\n",
       "       'lolol. 150 bashes. fucking dogs. ez. game',\n",
       "       'end fast. SLOW HAND TINKER. GG CAL. AAAHA. AHAHA. LMAO. it definitely is gg. sf def?. AHAHA. GGWP',\n",
       "       'close game. gg', 'skill bruh. acc buyer',\n",
       "       'CROSS. TORRENT. PLEASE. TODAY?. OOOO',\n",
       "       'rage. worth it. haha. GG TIME MONGOL. nice mid. carregado',\n",
       "       'xd. ROIFL. io mid is prank. ). just a hero. u know. and Io just doenst available on mid cuz like that. gl next. g',\n",
       "       'she is dead anyway. lets stop when she is reviving. gg. just finish',\n",
       "       'lol. haha. ez. haha. void ?. u. u can def haha. haha. haha. haha. ggwp',\n",
       "       'GG. END . TEAM NO DEF ',\n",
       "       \"?. second try lucky. wew. Wew. Wewewewewe. pa is pinoy. pignoi. exactly. you're*. fucking uneducated pinoys. gg. report pa. all. tyy\",\n",
       "       '.i.. you suck. push. kids. Push. luck win. gj',\n",
       "       '??. i did not see u finger me to death even once?. ez',\n",
       "       'alright alright. xhase ?. @@. omng warding. so whats ur new year resolutioin. like i sid we win ',\n",
       "       'gg. End. this mk. no hope. this mk. PANIC MEEPO. NO ONE CARES. IF U CARRY THIS GAME. BECAUSE YOUR LUCKY!. LOL MK. SEEW YOUR ITEM IIDOT. wANNA 1V1 MK ?. aFTER THIS',\n",
       "       \"can u stop sucking dick sf?. disruptor is holding ur dick mid. and u keep complainnig. was es and bane ever top?. how es bane and luna can't handle brew. es and bane were looking to fight non stop. and luna couldn't farm\",\n",
       "       'they speak taco. They cant understand you. reporting lc got it. Stfu LC. stupid taco. ZERO SUNSTRIKES. LC REPORTED. you said zero in the first statement. RETARD. WOAH SUNSTRIKE. DIDNT KNOW YOU KNEW THAT ONE INVOKER. yup np. you english?. gg. TACOTACOTACO. Get on your own servers retard. at least im doing better than LC ?\\\\_(?)_/?. not at all. tacotaco. REPORT LC. ARCANA TO EVERYONE THAT DOES. Then blamed invoker?. sorry pa im not that rich. but fuck LC. Lc gave you ez',\n",
       "       \"Yeah, don't look at the WK. Please.... Just pretend the man doesn't exist...\",\n",
       "       'no carry. sim',\n",
       "       'rq ?. this RU west server is good. :D. ?. report. SF. NOW. JAHAHAHAHAHAHAHAHA. what a fail gamee. jsut report everyone. for not buying bkb. what an idiots. report wood legion. divine 3. legion wood. good fucking g. gg',\n",
       "       'reprot. SAD. fucking reported axe. WORST HOOK IN HISTORY. YOU THOUGHT. IM THE #1 ROAMER NA. STUPIDD PIUDGE. STUPID!. nice ult medusa. commended. DOWNYS  GET DUMKED. ez game. Ty ty . SOOO EZQ!!. COMMEND ME!. COMMAND ME!!',\n",
       "       'rofl. if u know. even ur ember cant last hit lul',\n",
       "       'hehehe. that good. rampage!!!!. fuk you. oh fuk. you too. noooooo. wtf. so sad. gay. shemale. gay. report bb pls toxic. haha. bb fight with me 1vs1. you die. wtf. stop pls. 1v1 wiht me bb. ok. wtf i 1v1 at bb',\n",
       "       'veng. Doesnt . know how to zone the lane',\n",
       "       'No desire to play. go end',\n",
       "       'gg. end. rep bh an pudge. i afk plz get mid. isay get mid why fucking . plz pudge an bh rep. com fucking end. ok end',\n",
       "       'wp. wew. nc try nigga. ggwp. wp', 'dude. i see u. gg. gg',\n",
       "       'russia is big in eu west. but if u yolo like hell its not venges fault ;). u popped his passive. which killed me. :P. sneeze more. gg',\n",
       "       'we are late game', 'wtf. Really. this is jsut stupid, im out',\n",
       "       'Ez team', 'GG. Fuck you for playing huskar',\n",
       "       'TY. report sf and lycan. retard take lycan for hard . second rtard lose lane with help. just end. dont forget report lycan and sf. next game can come to yours team. end. im afk. k. da ti raq. nahuy spamiw twitch',\n",
       "       'ty. REPORTA SHAMAN E RUBICK = COMEND',\n",
       "       \"CLICK. XDDDDDDDDDD. NICE VENGE DUDE. NICE. 'thanks for hlep. __. )). tyty. one day. click. one day\",\n",
       "       'sad. lel. why mad bro',\n",
       "       'end it please. can u end it . ?. cant u see iam afk. ?. who the fuck is this invoker player. can s1 tell me????????????????????',\n",
       "       'DANCE. gg. fking clock. talking more. ez dog. otak cibai. lancau. pro clock. haha',\n",
       "       \"No idea wtf is going. on'. crying so ahard. for no reason. KIL ME . hlep report this ck tq. make me dulan. assshole. already give mid role . now barlking. and some more playing dick. come es. kill me . fuck yo. YOU GYUS. HELP. rEPORT. tHIS CK. aSSHOLE. GIVE MY ROLE. STILL WANT DULAN ME . COME. KILL ME . ALL THE WATN. MID . END. COME. go end. tq. just p. Nani. Already pick first invoker and i playing my role. Picking fucking ck and go mid againist tinker what an asshole. And somemore you want bark and steal item . go ahead. this is make dulan so hard. this is not my fucking fault at all. you want see me feed what. good what. now i'm feeding. Deserve it . to losle. HELP REPORT CK TQ\",\n",
       "       '?. ?. ?. worth?. losing to herald bro. trash talk pa. ?. pls quit alr. at least got money to buy. so stfu. gege',\n",
       "       'ez. i see medusa mid. i see lose. i see noob at satrt]. too much talk. heh. you feel good?. uberlord farm for???',\n",
       "       'aw. hyahha. }aw. wtf. gg. gg game',\n",
       "       'luigi. stream sniper!. techies go kill yourself. xaxa. ole. +',\n",
       "       'come mid. I feed. report pudge. free food. end. i afk no defense. gg',\n",
       "       'GG was fun', 'pro huskar. commend', 'what boost lol. im retard',\n",
       "       'yes, sure. but my teammates wont to kill u((9(. gg wp',\n",
       "       'wk noob. disruptor friend of sniper. 2 russians. gg. report sniper all. gg. all report sniper',\n",
       "       'PUSH. not defending. dodger lc. swap commend ty. swap commend ty',\n",
       "       'GG team mates. HHHHAHA. why quit ?. end. sorender. poor team mates. HAHAHA. HAHHAA. HAHAHA',\n",
       "       'gg. go ff mid. this cant be real. np. trash safelane. trash mid. u cant win this lane. pls go mid. and finish ',\n",
       "       'Sekip. mama. u jus. just. passed. then i ded. ok. fair play. why dont u let me run. ok m9. knife in csgo. im csgo player. lets 1v1 in csgo. i bet my ak. TALASO. u just like bara. come . and we ded. GGWP. GLHF. so rude. me pro',\n",
       "       'let me repick plz. liar. penius. looool. end fast. gg',\n",
       "       'Ec. haha. nca game. commend me', 'YOU SUCK ME COCKY. GG EZ',\n",
       "       'Ez dodge. ez pango. ez . ez. ez',\n",
       "       \"u  don't  need to  focus on me ... 3  carries  out  there ... lol. russian Guardians ... im  done   :). so ....carry  VS  lose  lol. how  can  this  LC  get  4  stars'  Archon ?\",\n",
       "       'ez mid la. fucking idiot mid. cancer. come. 28 mins and I never see sf ult. just end. gtg',\n",
       "       \"ENDS please. COME. please. ????. GG. ends'. dont do that. SEE. God help me. hod. NO fsbikjg. DIE. please. ENDS. ENDS. ENDS. ENDS. NEd. ENDS. MIrana idea so good. mirana always said gho. GO. GO GO. he always go. never deff. we didnt need to deff. coz we are gg. ahah. i think that pinoy player so fuck. without itrem. BKb and deso. ajhajua. ENds . ends. yeah sad. have idiot miranma. who always said go. GO. didnt know how to deff. miranaa player. Dont deff. just end. mirana player. gg. tarras. 5000 hit didnt die. coz they are mirana. go mirana go . go mirana mirana go. wow mirana plater. SEE. Ez mirana\",\n",
       "       \"OMG. IT'S REALLY HAPPENING\",\n",
       "       'np our SF is totaly autistic. ggwp. sf is brainlet. pathetic AM pick in unranked. carried by SF',\n",
       "       \"im gay. christmas is a pagan holiday. kaffir. report my fucking lina. i told him not to play fucking lia. lina. he's garbage at it. idk why he cant just pick fucking viper. he never picks sf counters and always gets rekt. by sfs. im sick of it. ?. fight me faggots. can you tell me. how i kill all of you. and i have no team. to take your base. ?. like how does that happen. m,y necro bought a 50 minute radiance. when ive had one since 19 minutes. and has no bots. do u know how tilting this game is\",\n",
       "       'i go feed. go end fast. ok. i go feed', 'im herald in my heart',\n",
       "       'commend. gank more cm. gg wp comend',\n",
       "       \"is only game. you guys literally have a tri top. shut up . you suck at dota. get over it. learn to not overextend in solo lane. why. >0 suns. nigga he killed me. with 2. already. you dumb and blind?. its pretty dumb you're complaining, you're literally doing the worst on your team and holding them back LC. :C. wow a sunstrike. LOL 0 SUNSTRIKES SHIT INVO. reported. YOU GUYS MUST BE SO THANKFUL TO HAVE THIS AMAZING LC ON YOUR TEAM. ESKEETIT. not a difficult task. SO CLOSE\",\n",
       "       \"snipers don't come better than this my friends. gg wp. pls commend sniper\",\n",
       "       'i ss one hit. next. lucky',\n",
       "       'are you retarded man. well fucking. done. brainless dogs. ez. ez dog. first 10 min. sit at mid. then talk. haha. Cent . did sit at mid. 10 minutes. retard dog:). these words. so memorized. so shut up mate. i dont care. ur tarsah. trash. ur profile repels me. haha. ze. ez. AHAHA',\n",
       "       'how often. are you gonna suicide kill me',\n",
       "       'archon 1 and 2 in mid xDD. SF BB KIDS. see. cancer bb. cancer sf. as well. 1-10 shitt. so fckin noob. like sf. RE[PRT TJAT SHIT. REPORT THAT SHITT. SO NNOOB. KID. LI9KE BB AND JUGG. FCK. playing like LOL. bullshit. xDD. just kidding. just jokinggggg. whyy. alien dude?. whyy. xD. xD. gg. no. CANT COMMEND U. HAHAHAHAHAHA. commend me. support pudge',\n",
       "       'end. watch me kunka. every body can throw. nd. go mid. i will not help deff. end. end. tired playing. noob kunka noob tecis. end. end. md kill me sf. mid. mid. go ahead. like i afraid lol. KUNKA THROW FUCK. WHAT EVER. THANKS. GO AHEAD. NOOB TECIS AND KUNKA WHAT FOR LION AND NP TRY HARD STUPID',\n",
       "       'gg. now we get to see the purple cool aid man slowly punch our base to death. report alch for not having 0.25 seconds per atk. only 0.28. fucking nub',\n",
       "       'wow rly? dood if you dont wanna play then dont qur a game sheesh',\n",
       "       'BS. NS. KEEP. FARMING. LIKE A . MORON. REALLY. F CK THIS. GUY',\n",
       "       'ty. wtf is with taht serwer. jesus gg',\n",
       "       'Wtf. AHAH. AHHA. aim me more. AHHA. gg',\n",
       "       'wait. afk. gg. gg. end . ty. gg carry. report gryo ?. gg. report gryo',\n",
       "       'shdow demon carry',\n",
       "       'Wtf. report. GG. FUCK WIND. REPORT THIS MOTHER FUCKER. FUCK WIND. REPORT. REPORT WIND. FUCK THIS. REPORT WIND',\n",
       "       'pussies. sry', 'SHOULD I STAY OR SHOULD I GO NOW. suicide.exe',\n",
       "       'hghaha. Useless allies. Do I call myself as an allies?. Dumbfuck',\n",
       "       'ahahha. stfu. moron',\n",
       "       \"wow. apparently. aggro doesn't switch. the white snow. didn't see shit. XD. wow. right after. great. exactly\",\n",
       "       'Wtf. halsennnnnnnnnnnnn. whyyy. hey divine. i dont know why. u last pick. LC. like noob. i need rampage. can i. pls. fuck u. LOIL. bb. feed me rampage. we got a deal',\n",
       "       'no idiot. we too pro. lol',\n",
       "       'gj. u can end it. this agha luna will probably kill a creep. hahahhahaha. this trash luna. HAHHAHAHA. just ignore this agha luna. he cant play, farm, talk. ez. carry',\n",
       "       'ez. wait. fucking nuubs. why u want to fucking unpause?. go play league of legends. fucking trash. scare 5vs5?. fucking ruski kebab. wew. no one tp. gege. l0l wk. radi :DDDDDDDDDDDDD. fuck u. nuubs. now we wont wait. l0l. l0l. bye wk. ez win. l0l shaker. :DDDDDDDDDDDD. llululululu. jugg farm 40mins for die. :DDDDDDDDDD. this monky is very angry. lul',\n",
       "       'dude he rage quit. not coming back. leave la idiot',\n",
       "       'fuck u axe. ty. for ur cog. noob. ez win. ez. noob. i told u axe. u are noob. counter pick???. fuckin noob tinker. fuckin nooob',\n",
       "       'da. da. wtf. slaid shou ',\n",
       "       'go end mid you can do it. necro no buyback',\n",
       "       \"wow !. hahaha. chill dude. why so hot headed. I've been there. just by looking at your set you are pro dude. hahaha. ahahahahaha. ggwp\",\n",
       "       'COMMEND ME  TY. TY FOR A GAME',\n",
       "       'wtf ? why ?. yeah 2 bashes noob. die noob. :). fucking lucky team. use my dick. this is a USA server why i have fucking spanish in my team. i hate spanish. 3 vs 5.... i dont think so!!!. why you dont pause ?',\n",
       "       'lol. hf boys, i love you. ggwp',\n",
       "       'jajaja. have fun. u can out. its not scored. in ur dream',\n",
       "       'how are these ranks equal?. END. ???. ???',\n",
       "       'GG guys enjoy ur win ez . can u report meepo plz . GG this fuckin shit moron meepo . GG shit i teram . GG. FF guys report meppo plz . GG WP . GO ff guys . GG. report meepo guys . he is 1 idot player . fuckin boster nooob',\n",
       "       'my team. XD. gg', 'go mid fast. all mid we wont def',\n",
       "       'glhf. <3. ten dota two heroes top. i eat cum. what sound do you think i make when i eat cum. !!!!. gg wp',\n",
       "       \"that's how we roll. kamehameha. gg. ggwp. ggwp\",\n",
       "       'drow . remember me last game. haha. waot. haha',\n",
       "       'gg. Report am. so noob guyz. hays. gg. Rteport am',\n",
       "       'take so long time to end. wa. noob. take so long time kid. ez. noob ahahahaha',\n",
       "       \"fuck u 3x monkeys. pudge lock and this warm shit. shut up a fourth monkey alert. noobbbbie. MORE LUCK. 3v1. sure. HAHAHAHAHA. NICE. ?. U're jst good with 2x+ team mates. ??. ??. 5x monkeys. HAHAHAHA. XDD. hard. game. xD\",\n",
       "       'stop. look at this line up and honestly say your team was worse. You just lost. ur right. storm unconventional mid. ursa classic. lmao. gg',\n",
       "       'Nah just other route. ggwp. Why did you leave Jugger all by himself?',\n",
       "       'zip zap. fuck u am. wow. wooooooooooooooooooooooow. !. kiding me !. 1 hit dager. there is a tip for noob player like am. if u are some kind of retard. ply sup. like protecter. see. we really. Respect him. he is noob. but. team respect him. 1045 crit attack !. come on. i stay in funtain',\n",
       "       'Report. Report monkey. real monkey. FUCK YOU MORON. Report monkey. Real life monkey',\n",
       "       'xfd. Xd. REPORT ALCHE. REPORTEN ALCHE', 'sad',\n",
       "       'ok feed. Haha. AW. haha. sad joker. Ez. EZ. Haha lol. SAD. SAD. haha. end it. end pls. yeah. like you two star. Archon. End ty. haha. this unvoker. unvoker not injo. carry will be call unvoker. haha. bobo. Haha. noob',\n",
       "       ',..... LOL. ggwp. ty for game',\n",
       "       \"g. GG LS. just report LS. Fucking noisyt. cant even help. Lol. ur lucky. got noob LS. he's at base. LOL. Lucky. report LS pls thanks. fucking noob\",\n",
       "       'and then. right. crucial. QQ', 'And report tinker. report tinker',\n",
       "       'fuckin axe. fuck. JUGNERNAUTTTTTTT',\n",
       "       \"you ok dude?. yeah, no.  wasn't in your last game . well he seems to be afk. so.. there's taht. lol gg. you're right. toxic cunt. muting him already. fuck that kid. is he real smart?. can't tell. at first I thought you were a prick, silencer. you right.... damn. ahahahhahas. you saved the huskar. LOL. 30 int... looool. meh. aa carry last pick was dope thoughy. lol. ahahahah\",\n",
       "       'sorry. switch off my cheats. uno des tres. UGANDA',\n",
       "       'GG end mid. report voker pls. gg. invoker. trash mid. average gpm 300. fuck off. report es. cuz top was so ez. no. jungle 2017 deserve report. d7?. u guys dont understand meta. who jungle in 2017?. voker this patch is weak. I report wk. for jungle. GG WP',\n",
       "       'blinlk ? . up ? . what kind of vision is this . impossible game gg . sure ',\n",
       "       'one. ez mid. ez. end fast plz. end ty. wp. prefeable make tinker rq. es. ?. :D. nope. end?. like wtf. they suck. whats hard about ending. im 2 hits. only use he has. why?. im covered by noobs here. ofc i defend. thats the game. and?. not same skill. lol mmr is a number, not skill. if you think mmr and kill are the same, you must b een stupid than all here. ty. reprot me for beeing smarter than you. im happy with that. lc just crying because he sucks. 2 duels 33 min. nope. im already repoted. why would i then:). you trash :D. nah. g end noob. nah rather you have. that cant find th mute button. ahahahah love pudgy that needs rod to hit hooks :D. so sad. nice duel. ty. try get some skills lc. you noob herald. not same skill. :). worst lc i saw. dont hink i just proved it. ++ smart weaver',\n",
       "       \"meow. damn, now they know there are 4 of them bot. they didn't wait. how rude, right. gg\",\n",
       "       'rotated 2 players top. and u got to b. i won. leaped. i juked es. gj man. well. i never seen some1 lose mid that hard with sf. wp. when we lost top tower sf was lvl 7. well wp',\n",
       "       'shutup nerd', 'g. NOOB team. archon. wtf. end. SAD',\n",
       "       'try hard. lol',\n",
       "       'Dont attack my twrs pls. end. I die . because. Me . 10-0. solo carry team. Best team. (none). emo nobo kid. GG. afk. enjoy. they farm all day. buy retard items  i cant blieve. 0 teamplay. let midas slardar fcarm. AHAHHAHAHAHA. 10-3-8 useleso k. im afk. Enjoy ur loss u fuc,kijng garbage piece of shit. 6 death. 9 death. in a game i carry 10-0. AHAHHAHAHAHAHAHAHABAHHA. only. Only. FUCKING FARM. AND WITH FARM BUY FUCKINGF RETARDED ITEMS. TO FARM MORE. FUCKING PIECE OFS HGI GARBAGE PLAYER. GET CANCERS ALL. ALL HAVE ITEM AHAHHAHAHAHAHAHA. TEAM NO ITEM. I CREATE MORE SPAC THAN IN UNIVERSE. TEAM CANT. AHAHAHAHHA. GO END. RETARD IM LEGEND 5 IDIOT BASTARD NOOB. SHUT THE FUCK UP. U FUCKING RUIN GAME. ONLY FARM. I CARRY. 18 KILLS. WITH ASSIST. WHAT U DO. FARM ALL DAY. FEED 6 DEATH ANYWAY TO WEAK ENEMY. AND BE USELESS. Ez game. Team crusader 0. can only farm and do noting. like creep. end. cant believe how alch can have item. unbelievable. Team can feed every noob. And let me die to ward. best team. cant 1on5. just impossible. unbelievable how people can be so bad to lose thsi game. fucking trash of humanity. nice fair matchmaking. give me the 4 worst players in all of dota. 1on5. great. pa still has not 1 single. point. of hp. not one. Not fucking one. Even treads on agi. not one single point. i cant believe how lowskill and stupid people can be. i was smarter on my 2nd day dota. unbelievable. janitors. fucking garbagemans. yeah funny. alch',\n",
       "       'rip farm. he h. he had a hard lane. very hard. lmao. you need trilane. u need sb. vs bbb. pls commend me. guys pls. commend for strong independent offlaner',\n",
       "       '?. WIND REPROTED. aLIEN HERE. LOL I WAS DIENG SO?. help report wind ty. pasive thrd . 4 heros were stick. Axe call one. Alien spoted. passive heross here. lol. me mid me  carry. pls pic ksupport. help reprt wind. ty',\n",
       "       'come . rampage',\n",
       "       'die. bitch. HAHAHA. lol. dont be salty bro. ok. hahaha. Dead?. =(. sad face?. lol this guys are mad. hahaha. kanican. die bitches. gg bitches. youre back blood?. oh stfu. reasons.,com. reasons reasons reasons. HAHAHAHA. thats what u get faggot. hahaha',\n",
       "       'GOD. . techies makes the game boring. For everyone but one.. :). gg',\n",
       "       'every1 throw toys out pram. and storm spends more time typing than anything else. well im reporting storm and dusa. because you both tilted each other. lol',\n",
       "       'MAGINA. MORE LIKE VAGINA. report lcock. magina?. more like vagina. haHAA. stop playing btw',\n",
       "       'he survived . relax',\n",
       "       'well all what i can say. i wish u play with this Tusk next . Report tusk all. random on ranekd match',\n",
       "       'why you play whis shit heroes. pa. reported pa . stupid shit hero . for kids. PEACE OF SHIT . fast . peace of shits. y. gg . report pa ',\n",
       "       'fail. :DD. dont get cocky. fuckers. ez aegis. won game. OMFG. TRASH NOOB HEROEs. ALL OF U. GO TO BED NYX. 2h game. u lose with those heroes. pathetic. noob voker',\n",
       "       'why almost every game for me pudje?',\n",
       "       'youre no diff. sure. pick a shaman more', 'noobs',\n",
       "       'magnus u have some mana 4 me', 'u can end. pudge afk',\n",
       "       'at a price of fb. idiot lol . wp. wp ',\n",
       "       'WP. PL . OK. :D. cant win bro . let us end. xD. warlock has scepter. i has scepter. Game de . :D',\n",
       "       'ASDGKHADFSADSFH. AHDFJOPFDAP]HAFD. ASDHPJOAHFDJOP][. Ty. voker. u spell me. waw. Waw. run. bitch. Waw. wow. invo. alah. i have 2 arab. how . i win. HOW THIS AM. wIN. (. wp doom',\n",
       "       'ggwp. I commend your fucking rat',\n",
       "       'lets not forget 30 min of being called noob in russian, korean, and spanish. quiet you trained monkey. DOTA where everyone is a cunt, regtardless of team',\n",
       "       'fucking ritika man. fucking shit. dont remind pls. bh the new gpm gain has been removed u may leave :). no gpm for ya. madarchod. yea. bb. and suck my dick',\n",
       "       'our spec run away when war. with full hp. GG',\n",
       "       'hi pudge. oh u really there. spe is so fucking over farmed',\n",
       "       'go end. huskar. picker. :). hahah. winter. report. lc. hahahah. axe min 12 vanguard dager. hahahah. report lc. thanks. hahah. invoker. picker. :). take damage ta. hahah. typical. 4 k. dog. hahah. isnt support. fault. not him. this son of whore. invoker. 2 points. quas. hahahah. hahahah. first time i see. in my life',\n",
       "       'Gr8. so sad. 5k gank. LMAO. wut now ?',\n",
       "       'HAHAAHA. Stfu medusa . U suck . Mother fucker. come . Ez. GGWP',\n",
       "       'stupid medusa. FUCKING MEDUSA. NICE. FORCESTAFF. ASSHOLE. this game. will be a year ender. this will be the last game played in 2017. and i got 500 ping. happy new year',\n",
       "       '4 bot. wait?',\n",
       "       'retard. little fairplay rusians shit. dont pause now. y are retard?. or blind?. Stupid russans kid. go EAST. this is west server. y have rusan and east',\n",
       "       'lul. wp. ?. ?. ez. Que?. ^. ?. ?', 'fury lycan boiys. run',\n",
       "       'you forgot the T. wut. are you calling for aid?. Gg',\n",
       "       \"this trash tinker. dont know. how paly with this trash. reAlly. too much talking skill like idioot. fucking asshole mid more. Jacky. Jacky guyss. Jacky son of harlot. jacky son of bitch. this tinker mom is bitch. so fucking awesome. i hope he's whole family die. really. no hopes. he want mid lmao. acc buyers. Laos. poor country. lmao\",\n",
       "       'wow'], dtype=object)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(X_train_raw)\n",
    "y = to_categorical(y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use commend had ursa me till back enemy end got']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[231, 66, 174, 114, 13, 771, 175, 772, 19, 92]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = []\n",
    "[words.extend(sentence) for sentence in X]\n",
    "freqdist = nltk.FreqDist(words)\n",
    "x_plot = [freqdist[word] for word in words]\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x_plot, 50, density=1, facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('FreqDist')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of IQ')\n",
    "plt.axis([min(x_plot)-10, max(x_plot)+ 10, 0, 0.1])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_padded = pad_sequences(X, maxlen=SEQUENCE_LENGTH, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validation Test</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_62 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 4s 13ms/sample - loss: 0.6692 - accuracy: 0.6810 - f1_m: 0.6311\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5778 - accuracy: 0.6918 - f1_m: 0.6892\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 2s 8ms/sample - loss: 0.5522 - accuracy: 0.7491 - f1_m: 0.7515\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.6027 - accuracy: 0.6774 - f1_m: 0.6807\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 2s 8ms/sample - loss: 0.5563 - accuracy: 0.6953 - f1_m: 0.6899\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5291 - accuracy: 0.7240 - f1_m: 0.7245\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5413 - accuracy: 0.7312 - f1_m: 0.7328\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5770 - accuracy: 0.7276 - f1_m: 0.7293\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5561 - accuracy: 0.6989 - f1_m: 0.6988\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5332 - accuracy: 0.7455 - f1_m: 0.7453\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.6143 - accuracy: 0.6197 - f1_m: 0.6815\n",
      "f1_m: 68.15%\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_63 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 4s 14ms/sample - loss: 0.6782 - accuracy: 0.6667 - f1_m: 0.6076\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.6086 - accuracy: 0.7097 - f1_m: 0.7092\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5877 - accuracy: 0.7025 - f1_m: 0.7037\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 2s 8ms/sample - loss: 0.5337 - accuracy: 0.7312 - f1_m: 0.7314\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5008 - accuracy: 0.7814 - f1_m: 0.7787\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5128 - accuracy: 0.7599 - f1_m: 0.7606\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5679 - accuracy: 0.7061 - f1_m: 0.7085\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5118 - accuracy: 0.7491 - f1_m: 0.7488\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5662 - accuracy: 0.7133 - f1_m: 0.7032\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5195 - accuracy: 0.7276 - f1_m: 0.7239\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.5825 - accuracy: 0.7465 - f1_m: 0.7381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 73.81%\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_64 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 4s 14ms/sample - loss: 0.6571 - accuracy: 0.6857 - f1_m: 0.6379\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 3s 9ms/sample - loss: 0.5572 - accuracy: 0.6964 - f1_m: 0.6968\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5596 - accuracy: 0.7036 - f1_m: 0.7025\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.6042 - accuracy: 0.6679 - f1_m: 0.6667\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 2s 8ms/sample - loss: 0.5494 - accuracy: 0.7036 - f1_m: 0.7049\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5919 - accuracy: 0.6929 - f1_m: 0.6887\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5393 - accuracy: 0.7143 - f1_m: 0.7153\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5421 - accuracy: 0.7107 - f1_m: 0.7095\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5548 - accuracy: 0.7107 - f1_m: 0.7083\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5624 - accuracy: 0.6821 - f1_m: 0.6829\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.4932 - accuracy: 0.7000 - f1_m: 0.6910\n",
      "f1_m: 69.10%\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_65 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 4s 13ms/sample - loss: 0.7903 - accuracy: 0.6833 - f1_m: 0.6326\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 3s 9ms/sample - loss: 0.5820 - accuracy: 0.7509 - f1_m: 0.7492\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5655 - accuracy: 0.7189 - f1_m: 0.7160\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5404 - accuracy: 0.7544 - f1_m: 0.7556\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5706 - accuracy: 0.7438 - f1_m: 0.7432\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5521 - accuracy: 0.7438 - f1_m: 0.7432\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5504 - accuracy: 0.7117 - f1_m: 0.7100\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5477 - accuracy: 0.7260 - f1_m: 0.7258\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5409 - accuracy: 0.7402 - f1_m: 0.7387\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5426 - accuracy: 0.7473 - f1_m: 0.7467\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.5083 - accuracy: 0.7391 - f1_m: 0.7562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 75.62%\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_66 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_64 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 4s 14ms/sample - loss: 0.7084 - accuracy: 0.6584 - f1_m: 0.6031\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.6331 - accuracy: 0.6797 - f1_m: 0.6797\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5693 - accuracy: 0.7046 - f1_m: 0.7060\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5789 - accuracy: 0.7331 - f1_m: 0.7357\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5640 - accuracy: 0.6868 - f1_m: 0.6876\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5164 - accuracy: 0.7189 - f1_m: 0.7189\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5679 - accuracy: 0.7011 - f1_m: 0.7035\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5633 - accuracy: 0.7224 - f1_m: 0.7224\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5547 - accuracy: 0.7295 - f1_m: 0.7283\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5510 - accuracy: 0.7224 - f1_m: 0.7224\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.3122 - accuracy: 0.7826 - f1_m: 0.8438\n",
      "f1_m: 84.38%\n",
      "74.21% (+/- 5.80%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario A\n",
    "do_experiment(X_train_raw, y_train_raw, 'lstm', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_67 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 5s 19ms/sample - loss: 0.6912 - accuracy: 0.6703 - f1_m: 0.6340\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5704 - accuracy: 0.7133 - f1_m: 0.7114\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5859 - accuracy: 0.7133 - f1_m: 0.7168\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5869 - accuracy: 0.6667 - f1_m: 0.6676\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5700 - accuracy: 0.6774 - f1_m: 0.6753\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5942 - accuracy: 0.6774 - f1_m: 0.6821\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5580 - accuracy: 0.7097 - f1_m: 0.7106\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5674 - accuracy: 0.6774 - f1_m: 0.6766\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 3s 9ms/sample - loss: 0.5833 - accuracy: 0.6523 - f1_m: 0.6537\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5551 - accuracy: 0.6953 - f1_m: 0.6940\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.6168 - accuracy: 0.7324 - f1_m: 0.6533\n",
      "f1_m: 65.33%\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_68 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 4s 14ms/sample - loss: 0.6346 - accuracy: 0.7133 - f1_m: 0.6697\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.6825 - accuracy: 0.6953 - f1_m: 0.6981\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5550 - accuracy: 0.7384 - f1_m: 0.7370\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5340 - accuracy: 0.7634 - f1_m: 0.7668\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5496 - accuracy: 0.7348 - f1_m: 0.7335\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5505 - accuracy: 0.7563 - f1_m: 0.7571\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.4999 - accuracy: 0.7706 - f1_m: 0.7669\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 3s 9ms/sample - loss: 0.5266 - accuracy: 0.7276 - f1_m: 0.7280\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5368 - accuracy: 0.7312 - f1_m: 0.7301\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 2s 9ms/sample - loss: 0.5730 - accuracy: 0.6846 - f1_m: 0.6809\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.5750 - accuracy: 0.6620 - f1_m: 0.6756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 67.56%\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_69 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 4s 13ms/sample - loss: 0.6637 - accuracy: 0.6857 - f1_m: 0.6467\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.6231 - accuracy: 0.7071 - f1_m: 0.7095\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.6025 - accuracy: 0.6821 - f1_m: 0.6852\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 3s 9ms/sample - loss: 0.5732 - accuracy: 0.7143 - f1_m: 0.7141\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5728 - accuracy: 0.7107 - f1_m: 0.7106\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5760 - accuracy: 0.7000 - f1_m: 0.6944\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 3s 9ms/sample - loss: 0.5648 - accuracy: 0.7000 - f1_m: 0.6991\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5664 - accuracy: 0.7071 - f1_m: 0.7095\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 3s 9ms/sample - loss: 0.5704 - accuracy: 0.7214 - f1_m: 0.7222\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 2s 9ms/sample - loss: 0.5721 - accuracy: 0.7000 - f1_m: 0.6968\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.4173 - accuracy: 0.8286 - f1_m: 0.8750\n",
      "f1_m: 87.50%\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_70 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_68 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 4s 13ms/sample - loss: 0.6587 - accuracy: 0.6548 - f1_m: 0.6062\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.6972 - accuracy: 0.6655 - f1_m: 0.6678\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 2s 8ms/sample - loss: 0.5740 - accuracy: 0.6904 - f1_m: 0.6911\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5345 - accuracy: 0.7295 - f1_m: 0.7303\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5649 - accuracy: 0.7153 - f1_m: 0.7154\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.6422 - accuracy: 0.6263 - f1_m: 0.6237\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 3s 9ms/sample - loss: 0.6094 - accuracy: 0.5943 - f1_m: 0.5954\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 2s 8ms/sample - loss: 0.5752 - accuracy: 0.6904 - f1_m: 0.6882\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 2s 8ms/sample - loss: 0.5331 - accuracy: 0.7438 - f1_m: 0.7422\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5197 - accuracy: 0.7438 - f1_m: 0.7442\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.8813 - accuracy: 0.6667 - f1_m: 0.5917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 59.17%\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_71 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 216,556\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 4s 14ms/sample - loss: 0.6958 - accuracy: 0.6441 - f1_m: 0.5977\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.6397 - accuracy: 0.6584 - f1_m: 0.6550\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.6745 - accuracy: 0.6655 - f1_m: 0.6668\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5924 - accuracy: 0.6975 - f1_m: 0.6981\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5657 - accuracy: 0.7153 - f1_m: 0.7164\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5602 - accuracy: 0.7046 - f1_m: 0.7040\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 3s 9ms/sample - loss: 0.6039 - accuracy: 0.6833 - f1_m: 0.6832\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 2s 8ms/sample - loss: 0.5877 - accuracy: 0.6904 - f1_m: 0.6892\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 2s 9ms/sample - loss: 0.5785 - accuracy: 0.6833 - f1_m: 0.6842\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 3s 9ms/sample - loss: 0.5425 - accuracy: 0.7367 - f1_m: 0.7382\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.6998 - accuracy: 0.5652 - f1_m: 0.5187\n",
      "f1_m: 51.87%\n",
      "66.29% (+/- 11.92%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario B\n",
    "do_experiment(X_train_raw, y_train_raw, 'lstm', 50, dropout_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_72 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_70 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 4s 15ms/sample - loss: 0.6864 - accuracy: 0.7097 - f1_m: 0.6734\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5596 - accuracy: 0.7097 - f1_m: 0.7079\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5155 - accuracy: 0.7527 - f1_m: 0.7563\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5236 - accuracy: 0.7599 - f1_m: 0.7606\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.6807 - accuracy: 0.6057 - f1_m: 0.6113\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5433 - accuracy: 0.7276 - f1_m: 0.7293\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5287 - accuracy: 0.7563 - f1_m: 0.7557\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5503 - accuracy: 0.7312 - f1_m: 0.7287\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5424 - accuracy: 0.7634 - f1_m: 0.7627\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5795 - accuracy: 0.7312 - f1_m: 0.7314\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.6381 - accuracy: 0.7746 - f1_m: 0.6845\n",
      "f1_m: 68.45%\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_73 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 5s 20ms/sample - loss: 0.6473 - accuracy: 0.6846 - f1_m: 0.6544\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.6011 - accuracy: 0.7204 - f1_m: 0.7264\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5708 - accuracy: 0.7025 - f1_m: 0.7064\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5353 - accuracy: 0.7276 - f1_m: 0.7280\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5354 - accuracy: 0.7097 - f1_m: 0.7092\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5263 - accuracy: 0.7455 - f1_m: 0.7440\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5744 - accuracy: 0.7061 - f1_m: 0.7112\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5201 - accuracy: 0.7133 - f1_m: 0.7114\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5494 - accuracy: 0.7168 - f1_m: 0.7175\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 3s 10ms/sample - loss: 0.5707 - accuracy: 0.7061 - f1_m: 0.7030\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.5868 - accuracy: 0.6479 - f1_m: 0.6652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 66.52%\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_74 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_72 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 4s 15ms/sample - loss: 0.6886 - accuracy: 0.6571 - f1_m: 0.6229\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 3s 10ms/sample - loss: 0.6130 - accuracy: 0.6893 - f1_m: 0.6944\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 3s 11ms/sample - loss: 0.5576 - accuracy: 0.7179 - f1_m: 0.7176\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 3s 10ms/sample - loss: 0.5690 - accuracy: 0.7250 - f1_m: 0.7280\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 3s 10ms/sample - loss: 0.5705 - accuracy: 0.7071 - f1_m: 0.7014\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 3s 10ms/sample - loss: 0.5782 - accuracy: 0.7107 - f1_m: 0.7095\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 3s 10ms/sample - loss: 0.5441 - accuracy: 0.7429 - f1_m: 0.7431\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 3s 10ms/sample - loss: 0.5798 - accuracy: 0.7036 - f1_m: 0.7060\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 3s 11ms/sample - loss: 0.5574 - accuracy: 0.7500 - f1_m: 0.7477\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 3s 10ms/sample - loss: 0.5694 - accuracy: 0.6929 - f1_m: 0.6956\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.3599 - accuracy: 0.8143 - f1_m: 0.8646\n",
      "f1_m: 86.46%\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_75 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_73 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 4s 15ms/sample - loss: 0.6439 - accuracy: 0.7117 - f1_m: 0.6575\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5858 - accuracy: 0.7153 - f1_m: 0.7174\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.6523 - accuracy: 0.6868 - f1_m: 0.6886\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5675 - accuracy: 0.7331 - f1_m: 0.7347\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5611 - accuracy: 0.7153 - f1_m: 0.7115\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.6292 - accuracy: 0.7082 - f1_m: 0.7104\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5630 - accuracy: 0.6975 - f1_m: 0.7029\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5355 - accuracy: 0.7402 - f1_m: 0.7436\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5482 - accuracy: 0.7153 - f1_m: 0.7154\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5131 - accuracy: 0.7509 - f1_m: 0.7521\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.4954 - accuracy: 0.7681 - f1_m: 0.7771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 77.71%\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_76 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 4s 15ms/sample - loss: 0.6659 - accuracy: 0.6726 - f1_m: 0.6088\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.6219 - accuracy: 0.6940 - f1_m: 0.6946\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5440 - accuracy: 0.7367 - f1_m: 0.7372\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.6106 - accuracy: 0.6335 - f1_m: 0.6336\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5478 - accuracy: 0.6975 - f1_m: 0.6971\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5472 - accuracy: 0.7011 - f1_m: 0.7015\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5736 - accuracy: 0.7082 - f1_m: 0.7056\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5633 - accuracy: 0.7189 - f1_m: 0.7189\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 3s 10ms/sample - loss: 0.5823 - accuracy: 0.6548 - f1_m: 0.6544\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 3s 11ms/sample - loss: 0.5876 - accuracy: 0.6121 - f1_m: 0.6099\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.5564 - accuracy: 0.6522 - f1_m: 0.7500\n",
      "f1_m: 75.00%\n",
      "74.83% (+/- 7.12%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario C\n",
    "do_experiment(X_train_raw, y_train_raw, 'lstm', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 14s - loss: 0.6931 - accuracy: 0.7500 - f1_m: 0.15 - ETA: 6s - loss: 0.6914 - accuracy: 0.6562 - f1_m: 0.3582 - ETA: 4s - loss: 0.6839 - accuracy: 0.6875 - f1_m: 0.488 - ETA: 2s - loss: 0.6859 - accuracy: 0.6484 - f1_m: 0.499 - ETA: 1s - loss: 0.6796 - accuracy: 0.6562 - f1_m: 0.537 - ETA: 1s - loss: 0.6854 - accuracy: 0.6562 - f1_m: 0.556 - ETA: 0s - loss: 0.6858 - accuracy: 0.6384 - f1_m: 0.553 - ETA: 0s - loss: 0.6802 - accuracy: 0.6602 - f1_m: 0.585 - 3s 11ms/sample - loss: 0.6787 - accuracy: 0.6595 - f1_m: 0.5930\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6132 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6159 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.9062 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.8451 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.7880 - accuracy: 0.6687 - f1_m: 0.668 - ETA: 0s - loss: 0.7543 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.7301 - accuracy: 0.6696 - f1_m: 0.669 - ETA: 0s - loss: 0.6942 - accuracy: 0.6875 - f1_m: 0.687 - 1s 5ms/sample - loss: 0.6992 - accuracy: 0.6918 - f1_m: 0.6932\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6243 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6511 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6415 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 0s - loss: 0.6193 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5827 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5671 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5498 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5951 - accuracy: 0.6875 - f1_m: 0.687 - 1s 5ms/sample - loss: 0.5949 - accuracy: 0.6846 - f1_m: 0.6836\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5081 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.4961 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5537 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5127 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5632 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5593 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5519 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5546 - accuracy: 0.6992 - f1_m: 0.699 - 1s 5ms/sample - loss: 0.5528 - accuracy: 0.7025 - f1_m: 0.7037\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5430 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5632 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5445 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5237 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5364 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5400 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5358 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5506 - accuracy: 0.7070 - f1_m: 0.707 - 1s 5ms/sample - loss: 0.5626 - accuracy: 0.7025 - f1_m: 0.7009\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6621 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6115 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5915 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5776 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5486 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5424 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5341 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5445 - accuracy: 0.6953 - f1_m: 0.695 - 1s 5ms/sample - loss: 0.5494 - accuracy: 0.6953 - f1_m: 0.6954\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4760 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5707 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5728 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5543 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6122 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.6221 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6082 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5997 - accuracy: 0.7188 - f1_m: 0.718 - 1s 5ms/sample - loss: 0.5872 - accuracy: 0.7276 - f1_m: 0.7307\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5415 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5612 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5878 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5744 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5773 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5679 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5845 - accuracy: 0.6786 - f1_m: 0.678 - ETA: 0s - loss: 0.5682 - accuracy: 0.7031 - f1_m: 0.703 - 1s 5ms/sample - loss: 0.5629 - accuracy: 0.6953 - f1_m: 0.6926\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.7668 - accuracy: 0.5312 - f1_m: 0.531 - ETA: 1s - loss: 0.6417 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5907 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5562 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5496 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5567 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5635 - accuracy: 0.6741 - f1_m: 0.674 - ETA: 0s - loss: 0.5650 - accuracy: 0.6719 - f1_m: 0.671 - 1s 5ms/sample - loss: 0.5661 - accuracy: 0.6703 - f1_m: 0.6697\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6499 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6124 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5773 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5715 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5494 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5971 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5890 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5845 - accuracy: 0.7031 - f1_m: 0.703 - 1s 5ms/sample - loss: 0.5878 - accuracy: 0.6989 - f1_m: 0.6975\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 6ms/sample - loss: 0.5906 - accuracy: 0.8310 - f1_m: 0.8378\n",
      "f1_m: 83.78%\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 11s - loss: 0.6932 - accuracy: 0.7500 - f1_m: 0.39 - ETA: 5s - loss: 0.6883 - accuracy: 0.7344 - f1_m: 0.5550 - ETA: 3s - loss: 0.6798 - accuracy: 0.7292 - f1_m: 0.609 - ETA: 2s - loss: 0.6782 - accuracy: 0.6953 - f1_m: 0.605 - ETA: 1s - loss: 0.6712 - accuracy: 0.6875 - f1_m: 0.615 - ETA: 1s - loss: 0.6520 - accuracy: 0.6875 - f1_m: 0.627 - ETA: 0s - loss: 0.6559 - accuracy: 0.6830 - f1_m: 0.631 - ETA: 0s - loss: 0.6442 - accuracy: 0.6953 - f1_m: 0.650 - 3s 9ms/sample - loss: 0.6438 - accuracy: 0.6882 - f1_m: 0.6458\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5490 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5097 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5080 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5239 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5106 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5008 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5124 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5213 - accuracy: 0.7422 - f1_m: 0.742 - 1s 4ms/sample - loss: 0.5315 - accuracy: 0.7491 - f1_m: 0.7515\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5667 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.6064 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5484 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5338 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5672 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5500 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5487 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5346 - accuracy: 0.7461 - f1_m: 0.746 - 1s 5ms/sample - loss: 0.5330 - accuracy: 0.7384 - f1_m: 0.7357\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.9476 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 1s - loss: 0.7806 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6995 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6932 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6685 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.6458 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.6380 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6193 - accuracy: 0.7227 - f1_m: 0.722 - 1s 5ms/sample - loss: 0.6092 - accuracy: 0.7204 - f1_m: 0.7197\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6434 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6314 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5759 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5842 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5937 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5613 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5445 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5501 - accuracy: 0.7305 - f1_m: 0.730 - 1s 4ms/sample - loss: 0.5734 - accuracy: 0.7061 - f1_m: 0.6976\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5156 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5304 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5212 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5080 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.4924 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.5345 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5430 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5436 - accuracy: 0.7422 - f1_m: 0.742 - 1s 4ms/sample - loss: 0.5361 - accuracy: 0.7527 - f1_m: 0.7563\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4143 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 0s - loss: 0.3881 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 0s - loss: 0.5146 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5128 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5206 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.5276 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5219 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5085 - accuracy: 0.7695 - f1_m: 0.769 - 1s 4ms/sample - loss: 0.5210 - accuracy: 0.7670 - f1_m: 0.7662\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4865 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4854 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4986 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5153 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5065 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5030 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5172 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5104 - accuracy: 0.7461 - f1_m: 0.746 - 1s 4ms/sample - loss: 0.4986 - accuracy: 0.7599 - f1_m: 0.7646\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 1s - loss: 0.4632 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4518 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4899 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5245 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5312 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5192 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5159 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5327 - accuracy: 0.7539 - f1_m: 0.753 - 1s 4ms/sample - loss: 0.5361 - accuracy: 0.7455 - f1_m: 0.7426\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5111 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4598 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4951 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5076 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5038 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5076 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5102 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5361 - accuracy: 0.7266 - f1_m: 0.726 - 1s 4ms/sample - loss: 0.5306 - accuracy: 0.7312 - f1_m: 0.7328\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.6457 - accuracy: 0.6620 - f1_m: 0.6756\n",
      "f1_m: 67.56%\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 12s - loss: 0.6932 - accuracy: 0.6562 - f1_m: 0.29 - ETA: 5s - loss: 0.6905 - accuracy: 0.6250 - f1_m: 0.4427 - ETA: 3s - loss: 0.6881 - accuracy: 0.6042 - f1_m: 0.482 - ETA: 2s - loss: 0.6787 - accuracy: 0.6172 - f1_m: 0.526 - ETA: 1s - loss: 0.6227 - accuracy: 0.6687 - f1_m: 0.595 - ETA: 1s - loss: 0.6260 - accuracy: 0.6719 - f1_m: 0.611 - ETA: 0s - loss: 0.6209 - accuracy: 0.6741 - f1_m: 0.622 - ETA: 0s - loss: 0.6222 - accuracy: 0.6875 - f1_m: 0.641 - 3s 10ms/sample - loss: 0.6134 - accuracy: 0.6964 - f1_m: 0.6586\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.7774 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6789 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6769 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6721 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6559 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.6308 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6124 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.6098 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5987 - accuracy: 0.7250 - f1_m: 0.7292\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5507 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4675 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4928 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5187 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5218 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5265 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5285 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5515 - accuracy: 0.7422 - f1_m: 0.742 - 1s 4ms/sample - loss: 0.5494 - accuracy: 0.7393 - f1_m: 0.7384\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5899 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5374 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5259 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5751 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5779 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5812 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.6005 - accuracy: 0.6830 - f1_m: 0.683 - ETA: 0s - loss: 0.5955 - accuracy: 0.6992 - f1_m: 0.699 - 1s 5ms/sample - loss: 0.5934 - accuracy: 0.7000 - f1_m: 0.7002\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4016 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.4815 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5070 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5317 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5405 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5185 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5200 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5391 - accuracy: 0.7461 - f1_m: 0.746 - 1s 4ms/sample - loss: 0.5600 - accuracy: 0.7286 - f1_m: 0.7234\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5907 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5479 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5499 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5462 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5614 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5470 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5597 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5631 - accuracy: 0.7227 - f1_m: 0.722 - 1s 4ms/sample - loss: 0.5611 - accuracy: 0.7286 - f1_m: 0.7303\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6201 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5960 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5762 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6175 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.6100 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.6059 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6038 - accuracy: 0.6830 - f1_m: 0.683 - ETA: 0s - loss: 0.5920 - accuracy: 0.6953 - f1_m: 0.695 - 1s 4ms/sample - loss: 0.5785 - accuracy: 0.7000 - f1_m: 0.7014\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6480 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5968 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5556 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5691 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5693 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5749 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5856 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5728 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.5696 - accuracy: 0.7107 - f1_m: 0.7095\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4394 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5918 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5981 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6026 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5819 - accuracy: 0.6687 - f1_m: 0.668 - ETA: 0s - loss: 0.5609 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5625 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5597 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5507 - accuracy: 0.7179 - f1_m: 0.7199\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5965 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.7030 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 0s - loss: 0.6439 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 0s - loss: 0.6322 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.6221 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.6222 - accuracy: 0.6823 - f1_m: 0.682 - ETA: 0s - loss: 0.6062 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5876 - accuracy: 0.7031 - f1_m: 0.703 - 1s 4ms/sample - loss: 0.5891 - accuracy: 0.7036 - f1_m: 0.7037\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 14ms/sample - loss: 0.4564 - accuracy: 0.7286 - f1_m: 0.7118\n",
      "f1_m: 71.18%\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 12s - loss: 0.6933 - accuracy: 0.5625 - f1_m: 0.13 - ETA: 5s - loss: 0.6907 - accuracy: 0.6875 - f1_m: 0.4744 - ETA: 3s - loss: 0.6875 - accuracy: 0.6667 - f1_m: 0.524 - ETA: 2s - loss: 0.6831 - accuracy: 0.6641 - f1_m: 0.557 - ETA: 1s - loss: 0.6697 - accuracy: 0.6687 - f1_m: 0.583 - ETA: 1s - loss: 0.8314 - accuracy: 0.6719 - f1_m: 0.600 - ETA: 0s - loss: 0.8058 - accuracy: 0.6696 - f1_m: 0.608 - ETA: 0s - loss: 0.7752 - accuracy: 0.6836 - f1_m: 0.630 - 3s 10ms/sample - loss: 0.7631 - accuracy: 0.6762 - f1_m: 0.6270\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5614 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5182 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5872 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5987 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.5935 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.6111 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5984 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5898 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.5753 - accuracy: 0.7117 - f1_m: 0.7149\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6928 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6032 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6140 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.6047 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6070 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5968 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5899 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5859 - accuracy: 0.7266 - f1_m: 0.726 - 1s 5ms/sample - loss: 0.5750 - accuracy: 0.7295 - f1_m: 0.7303\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5678 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5556 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5863 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5741 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5821 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5821 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5778 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5864 - accuracy: 0.6914 - f1_m: 0.691 - 1s 4ms/sample - loss: 0.5842 - accuracy: 0.6940 - f1_m: 0.6946\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - ETA: 1s - loss: 0.4999 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4815 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5048 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5164 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5124 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5301 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5329 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5383 - accuracy: 0.7617 - f1_m: 0.761 - 1s 5ms/sample - loss: 0.5418 - accuracy: 0.7544 - f1_m: 0.7526\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5187 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5015 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5714 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5774 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5748 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5790 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5744 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5704 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.5740 - accuracy: 0.7117 - f1_m: 0.7110\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4801 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5518 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5573 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5424 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5010 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5499 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5592 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5504 - accuracy: 0.7383 - f1_m: 0.738 - 1s 4ms/sample - loss: 0.5482 - accuracy: 0.7402 - f1_m: 0.7407\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6871 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6896 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6516 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6199 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.6246 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.6136 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5989 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5871 - accuracy: 0.7188 - f1_m: 0.718 - 1s 4ms/sample - loss: 0.5746 - accuracy: 0.7260 - f1_m: 0.7278\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5161 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.4960 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5053 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5197 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5382 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5271 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5064 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5415 - accuracy: 0.7539 - f1_m: 0.753 - 1s 4ms/sample - loss: 0.5517 - accuracy: 0.7402 - f1_m: 0.7368\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6294 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5922 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5532 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5568 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5752 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5677 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5587 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5530 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5536 - accuracy: 0.7153 - f1_m: 0.7164\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.6273 - accuracy: 0.6957 - f1_m: 0.6687\n",
      "f1_m: 66.87%\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 366,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 11s - loss: 0.6931 - accuracy: 0.8125 - f1_m: 0.24 - ETA: 5s - loss: 0.6853 - accuracy: 0.7812 - f1_m: 0.4970 - ETA: 3s - loss: 0.6766 - accuracy: 0.7500 - f1_m: 0.560 - ETA: 2s - loss: 0.6852 - accuracy: 0.7188 - f1_m: 0.576 - ETA: 1s - loss: 0.6776 - accuracy: 0.7375 - f1_m: 0.623 - ETA: 1s - loss: 0.6749 - accuracy: 0.7240 - f1_m: 0.629 - ETA: 0s - loss: 0.6745 - accuracy: 0.7054 - f1_m: 0.624 - ETA: 0s - loss: 0.6751 - accuracy: 0.6875 - f1_m: 0.616 - 3s 9ms/sample - loss: 0.6786 - accuracy: 0.6690 - f1_m: 0.6013\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6584 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6711 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6633 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6538 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.6566 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.6579 - accuracy: 0.6510 - f1_m: 0.651 - ETA: 0s - loss: 0.6520 - accuracy: 0.6518 - f1_m: 0.651 - ETA: 0s - loss: 0.6375 - accuracy: 0.6680 - f1_m: 0.668 - 1s 4ms/sample - loss: 0.6260 - accuracy: 0.6726 - f1_m: 0.6737\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4620 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4640 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.6110 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.6175 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6251 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.6229 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6241 - accuracy: 0.6786 - f1_m: 0.678 - ETA: 0s - loss: 0.6307 - accuracy: 0.6758 - f1_m: 0.675 - 1s 4ms/sample - loss: 0.6203 - accuracy: 0.6797 - f1_m: 0.6807\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4601 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5729 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5794 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5416 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5388 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5493 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5312 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5359 - accuracy: 0.7266 - f1_m: 0.726 - 1s 5ms/sample - loss: 0.5281 - accuracy: 0.7367 - f1_m: 0.7392\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.8420 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 0s - loss: 0.7377 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6708 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6322 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6267 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.6106 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.6218 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.6051 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5930 - accuracy: 0.7153 - f1_m: 0.7164\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4753 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4909 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4903 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5182 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5551 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5451 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5391 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5625 - accuracy: 0.6953 - f1_m: 0.695 - 1s 5ms/sample - loss: 0.5782 - accuracy: 0.6726 - f1_m: 0.6669\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6707 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6633 - accuracy: 0.5469 - f1_m: 0.546 - ETA: 0s - loss: 0.6578 - accuracy: 0.5312 - f1_m: 0.531 - ETA: 0s - loss: 0.6372 - accuracy: 0.5312 - f1_m: 0.531 - ETA: 0s - loss: 0.5939 - accuracy: 0.5875 - f1_m: 0.587 - ETA: 0s - loss: 0.6206 - accuracy: 0.5677 - f1_m: 0.567 - ETA: 0s - loss: 0.6270 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 0s - loss: 0.6157 - accuracy: 0.5703 - f1_m: 0.570 - 1s 4ms/sample - loss: 0.6221 - accuracy: 0.5694 - f1_m: 0.5692\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6298 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5540 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5689 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5536 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5617 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5578 - accuracy: 0.6823 - f1_m: 0.682 - ETA: 0s - loss: 0.5570 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5559 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.5565 - accuracy: 0.6975 - f1_m: 0.6971\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4717 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5081 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.4911 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5488 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5631 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5811 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.5550 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.5760 - accuracy: 0.6836 - f1_m: 0.683 - 1s 4ms/sample - loss: 0.5774 - accuracy: 0.6833 - f1_m: 0.6832\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6004 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6101 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6871 - accuracy: 0.5729 - f1_m: 0.572 - ETA: 0s - loss: 0.6738 - accuracy: 0.5547 - f1_m: 0.554 - ETA: 0s - loss: 0.6689 - accuracy: 0.5688 - f1_m: 0.568 - ETA: 0s - loss: 0.6621 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 0s - loss: 0.6462 - accuracy: 0.5893 - f1_m: 0.589 - ETA: 0s - loss: 0.6555 - accuracy: 0.6016 - f1_m: 0.601 - 1s 4ms/sample - loss: 0.6659 - accuracy: 0.5943 - f1_m: 0.5925\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.5795 - accuracy: 0.6522 - f1_m: 0.7500\n",
      "f1_m: 75.00%\n",
      "72.88% (+/- 6.17%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario D\n",
    "do_experiment(X_train_raw, y_train_raw, 'lstm', 100, dropout_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gg. wow gyro got aghas. end. wombo combo. ez top'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_54 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_54 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 11s - loss: 0.6931 - accuracy: 0.7188 - f1_m: 0.28 - ETA: 5s - loss: 0.6874 - accuracy: 0.7031 - f1_m: 0.4866 - ETA: 3s - loss: 0.6834 - accuracy: 0.6667 - f1_m: 0.522 - ETA: 2s - loss: 0.7015 - accuracy: 0.6328 - f1_m: 0.524 - ETA: 1s - loss: 0.6922 - accuracy: 0.6750 - f1_m: 0.588 - ETA: 0s - loss: 0.6901 - accuracy: 0.6562 - f1_m: 0.584 - ETA: 0s - loss: 0.6876 - accuracy: 0.6473 - f1_m: 0.585 - ETA: 0s - loss: 0.6810 - accuracy: 0.6641 - f1_m: 0.609 - 3s 9ms/sample - loss: 0.6705 - accuracy: 0.6846 - f1_m: 0.6436\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.7300 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6465 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6261 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6228 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6136 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.6247 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6128 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6061 - accuracy: 0.7070 - f1_m: 0.707 - 1s 4ms/sample - loss: 0.5975 - accuracy: 0.7061 - f1_m: 0.7058\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.8337 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6989 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6277 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6244 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.6138 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5836 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6060 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.6017 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.6057 - accuracy: 0.6953 - f1_m: 0.6940\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5788 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6185 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6384 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6248 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6185 - accuracy: 0.6313 - f1_m: 0.631 - ETA: 0s - loss: 0.6138 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 0s - loss: 0.5984 - accuracy: 0.6518 - f1_m: 0.651 - ETA: 0s - loss: 0.5840 - accuracy: 0.6836 - f1_m: 0.683 - 1s 4ms/sample - loss: 0.5738 - accuracy: 0.6882 - f1_m: 0.6898\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5900 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6549 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6303 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6039 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5940 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.6017 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5973 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5818 - accuracy: 0.7227 - f1_m: 0.722 - 1s 4ms/sample - loss: 0.5834 - accuracy: 0.7204 - f1_m: 0.7197\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5613 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6605 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6418 - accuracy: 0.6146 - f1_m: 0.614 - ETA: 0s - loss: 0.6542 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 0s - loss: 0.6621 - accuracy: 0.5813 - f1_m: 0.581 - ETA: 0s - loss: 0.6746 - accuracy: 0.5677 - f1_m: 0.567 - ETA: 0s - loss: 0.6629 - accuracy: 0.5804 - f1_m: 0.580 - ETA: 0s - loss: 0.6501 - accuracy: 0.6016 - f1_m: 0.601 - 1s 4ms/sample - loss: 0.6402 - accuracy: 0.6129 - f1_m: 0.6168\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6024 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5854 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6125 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.6114 - accuracy: 0.6328 - f1_m: 0.632 - ETA: 0s - loss: 0.5985 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.6008 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6061 - accuracy: 0.6518 - f1_m: 0.651 - ETA: 0s - loss: 0.6114 - accuracy: 0.6484 - f1_m: 0.648 - 1s 4ms/sample - loss: 0.6201 - accuracy: 0.6487 - f1_m: 0.6489\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5313 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5530 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5752 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6123 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.6129 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.6111 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6247 - accuracy: 0.6384 - f1_m: 0.638 - ETA: 0s - loss: 0.6292 - accuracy: 0.6133 - f1_m: 0.613 - 1s 4ms/sample - loss: 0.6300 - accuracy: 0.6201 - f1_m: 0.6224\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.6268 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6394 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6297 - accuracy: 0.6484 - f1_m: 0.648 - ETA: 0s - loss: 0.6305 - accuracy: 0.6438 - f1_m: 0.643 - ETA: 0s - loss: 0.6397 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6333 - accuracy: 0.6473 - f1_m: 0.647 - ETA: 0s - loss: 0.6304 - accuracy: 0.6406 - f1_m: 0.640 - 1s 4ms/sample - loss: 0.6241 - accuracy: 0.6487 - f1_m: 0.6516\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6175 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5820 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5670 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5761 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.5783 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6051 - accuracy: 0.6652 - f1_m: 0.665 - ETA: 0s - loss: 0.6058 - accuracy: 0.6641 - f1_m: 0.664 - 1s 4ms/sample - loss: 0.6118 - accuracy: 0.6559 - f1_m: 0.6531\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.8991 - accuracy: 0.6479 - f1_m: 0.5536\n",
      "f1_m: 55.36%\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_55 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 16s - loss: 0.6931 - accuracy: 0.6875 - f1_m: 0.26 - ETA: 7s - loss: 0.6920 - accuracy: 0.6094 - f1_m: 0.3990 - ETA: 4s - loss: 0.6857 - accuracy: 0.6562 - f1_m: 0.516 - ETA: 3s - loss: 0.6671 - accuracy: 0.6953 - f1_m: 0.590 - ETA: 2s - loss: 0.6999 - accuracy: 0.7000 - f1_m: 0.615 - ETA: 1s - loss: 0.6880 - accuracy: 0.6875 - f1_m: 0.617 - ETA: 0s - loss: 0.6855 - accuracy: 0.6786 - f1_m: 0.618 - ETA: 0s - loss: 0.6813 - accuracy: 0.6719 - f1_m: 0.619 - 3s 12ms/sample - loss: 0.6837 - accuracy: 0.6631 - f1_m: 0.6133\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.6321 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6386 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6232 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5957 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6339 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.7578 - accuracy: 0.6510 - f1_m: 0.651 - ETA: 0s - loss: 0.7374 - accuracy: 0.6473 - f1_m: 0.647 - ETA: 0s - loss: 0.7053 - accuracy: 0.6680 - f1_m: 0.668 - 1s 4ms/sample - loss: 0.6832 - accuracy: 0.6810 - f1_m: 0.6855\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5054 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5580 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5556 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5559 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5691 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5707 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5631 - accuracy: 0.7344 - f1_m: 0.734 - 1s 4ms/sample - loss: 0.5555 - accuracy: 0.7419 - f1_m: 0.7446\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6729 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6204 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6064 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6014 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5790 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5738 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5684 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5594 - accuracy: 0.7344 - f1_m: 0.734 - 1s 4ms/sample - loss: 0.5489 - accuracy: 0.7384 - f1_m: 0.7397\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4963 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5810 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5972 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5816 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5495 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5967 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5873 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5805 - accuracy: 0.7031 - f1_m: 0.703 - 1s 4ms/sample - loss: 0.5689 - accuracy: 0.7168 - f1_m: 0.7216\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5371 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6199 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5844 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5843 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5873 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5611 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5343 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5298 - accuracy: 0.7578 - f1_m: 0.757 - 1s 4ms/sample - loss: 0.5471 - accuracy: 0.7527 - f1_m: 0.7509\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5144 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5213 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5062 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5125 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5050 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5153 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.5218 - accuracy: 0.7695 - f1_m: 0.769 - 1s 4ms/sample - loss: 0.5324 - accuracy: 0.7670 - f1_m: 0.7662\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5304 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5080 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5004 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.4839 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4645 - accuracy: 0.8177 - f1_m: 0.817 - ETA: 0s - loss: 0.4984 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.5061 - accuracy: 0.7891 - f1_m: 0.789 - 1s 4ms/sample - loss: 0.5156 - accuracy: 0.7814 - f1_m: 0.7787\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5371 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5297 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4967 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5087 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5393 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5335 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5415 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5486 - accuracy: 0.7344 - f1_m: 0.734 - 1s 4ms/sample - loss: 0.5651 - accuracy: 0.7133 - f1_m: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6132 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5527 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5837 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5755 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5710 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5848 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5766 - accuracy: 0.7227 - f1_m: 0.722 - 1s 4ms/sample - loss: 0.5684 - accuracy: 0.7312 - f1_m: 0.7341\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.5954 - accuracy: 0.6761 - f1_m: 0.6860\n",
      "f1_m: 68.60%\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_56 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 12s - loss: 0.6940 - accuracy: 0.6562 - f1_m: 0.39 - ETA: 5s - loss: 0.6859 - accuracy: 0.6875 - f1_m: 0.5555 - ETA: 3s - loss: 0.6766 - accuracy: 0.6979 - f1_m: 0.609 - ETA: 2s - loss: 0.6518 - accuracy: 0.6953 - f1_m: 0.629 - ETA: 1s - loss: 0.7467 - accuracy: 0.6750 - f1_m: 0.622 - ETA: 1s - loss: 0.7225 - accuracy: 0.6719 - f1_m: 0.627 - ETA: 0s - loss: 0.7204 - accuracy: 0.6652 - f1_m: 0.627 - ETA: 0s - loss: 0.7107 - accuracy: 0.6641 - f1_m: 0.631 - 3s 9ms/sample - loss: 0.7060 - accuracy: 0.6607 - f1_m: 0.6304\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.6316 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5817 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6215 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5980 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6194 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.6173 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.6130 - accuracy: 0.6964 - f1_m: 0.696 - ETA: 0s - loss: 0.6115 - accuracy: 0.6953 - f1_m: 0.695 - 1s 4ms/sample - loss: 0.6023 - accuracy: 0.7000 - f1_m: 0.7014\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6655 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6367 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6198 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.6192 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.6647 - accuracy: 0.6823 - f1_m: 0.682 - ETA: 0s - loss: 0.6572 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6562 - accuracy: 0.6836 - f1_m: 0.683 - 1s 4ms/sample - loss: 0.6474 - accuracy: 0.6929 - f1_m: 0.6956\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.6829 - accuracy: 0.5312 - f1_m: 0.531 - ETA: 0s - loss: 0.6546 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 0s - loss: 0.6037 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6000 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5918 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.6115 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.6157 - accuracy: 0.6518 - f1_m: 0.651 - ETA: 0s - loss: 0.6009 - accuracy: 0.6641 - f1_m: 0.664 - 1s 4ms/sample - loss: 0.5887 - accuracy: 0.6750 - f1_m: 0.6782\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.8532 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6416 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5771 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5524 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5854 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5679 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5816 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.6074 - accuracy: 0.7031 - f1_m: 0.703 - 1s 4ms/sample - loss: 0.5989 - accuracy: 0.7143 - f1_m: 0.7176\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.5728 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5598 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5540 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5576 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5618 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5560 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5408 - accuracy: 0.7422 - f1_m: 0.742 - 1s 4ms/sample - loss: 0.5672 - accuracy: 0.7321 - f1_m: 0.7292\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.6845 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5957 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6136 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5937 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.5735 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5627 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5686 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5810 - accuracy: 0.6875 - f1_m: 0.687 - 1s 4ms/sample - loss: 0.5769 - accuracy: 0.6929 - f1_m: 0.6944\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.5547 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5663 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.5689 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5671 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5455 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5511 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5498 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5505 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5481 - accuracy: 0.7179 - f1_m: 0.7199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.6021 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.7038 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6713 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.6372 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5941 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5821 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5717 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5843 - accuracy: 0.6914 - f1_m: 0.691 - 1s 4ms/sample - loss: 0.5780 - accuracy: 0.7000 - f1_m: 0.7025\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.5913 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5496 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5845 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6066 - accuracy: 0.6328 - f1_m: 0.632 - ETA: 0s - loss: 0.6086 - accuracy: 0.6313 - f1_m: 0.631 - ETA: 0s - loss: 0.6037 - accuracy: 0.6510 - f1_m: 0.651 - ETA: 0s - loss: 0.5872 - accuracy: 0.6607 - f1_m: 0.660 - ETA: 0s - loss: 0.5759 - accuracy: 0.6758 - f1_m: 0.675 - 1s 4ms/sample - loss: 0.5731 - accuracy: 0.6750 - f1_m: 0.6748\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.4849 - accuracy: 0.6857 - f1_m: 0.6806\n",
      "f1_m: 68.06%\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_57 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 11s - loss: 0.6930 - accuracy: 0.7500 - f1_m: 0.29 - ETA: 5s - loss: 0.6866 - accuracy: 0.7188 - f1_m: 0.4927 - ETA: 3s - loss: 0.6774 - accuracy: 0.6979 - f1_m: 0.547 - ETA: 2s - loss: 0.6470 - accuracy: 0.7031 - f1_m: 0.590 - ETA: 1s - loss: 0.6340 - accuracy: 0.7125 - f1_m: 0.622 - ETA: 0s - loss: 0.7314 - accuracy: 0.6979 - f1_m: 0.622 - ETA: 0s - loss: 0.7129 - accuracy: 0.7009 - f1_m: 0.636 - ETA: 0s - loss: 0.6925 - accuracy: 0.7070 - f1_m: 0.650 - 3s 9ms/sample - loss: 0.6716 - accuracy: 0.7189 - f1_m: 0.6716\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.7654 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6437 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6225 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6273 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6408 - accuracy: 0.6500 - f1_m: 0.650 - ETA: 0s - loss: 0.6316 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6185 - accuracy: 0.6830 - f1_m: 0.683 - ETA: 0s - loss: 0.6102 - accuracy: 0.6836 - f1_m: 0.683 - 1s 4ms/sample - loss: 0.6100 - accuracy: 0.6904 - f1_m: 0.6921\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4485 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5237 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5492 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5463 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5513 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5848 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5890 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5799 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5742 - accuracy: 0.7082 - f1_m: 0.7075\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5469 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5377 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5220 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5340 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5457 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5448 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5625 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.5602 - accuracy: 0.7260 - f1_m: 0.7287\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6087 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6127 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5817 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5644 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5536 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5470 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5420 - accuracy: 0.7227 - f1_m: 0.722 - 1s 4ms/sample - loss: 0.5325 - accuracy: 0.7295 - f1_m: 0.7312\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4149 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4400 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.4390 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5532 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5778 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5938 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6064 - accuracy: 0.6652 - f1_m: 0.665 - ETA: 0s - loss: 0.6020 - accuracy: 0.6680 - f1_m: 0.668 - 1s 4ms/sample - loss: 0.6041 - accuracy: 0.6690 - f1_m: 0.6693\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6514 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6208 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6086 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5751 - accuracy: 0.6484 - f1_m: 0.648 - ETA: 0s - loss: 0.5816 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5848 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.5943 - accuracy: 0.6696 - f1_m: 0.669 - ETA: 0s - loss: 0.5906 - accuracy: 0.6641 - f1_m: 0.664 - 1s 4ms/sample - loss: 0.5801 - accuracy: 0.6762 - f1_m: 0.6792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.6492 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6110 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6182 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5925 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5896 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5855 - accuracy: 0.6823 - f1_m: 0.682 - ETA: 0s - loss: 0.5766 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5713 - accuracy: 0.7031 - f1_m: 0.703 - 1s 4ms/sample - loss: 0.5666 - accuracy: 0.7046 - f1_m: 0.7050\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.4050 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4898 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6216 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6158 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6181 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.5930 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5642 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5601 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.5900 - accuracy: 0.6868 - f1_m: 0.6799\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.5119 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5820 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.5639 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 0s - loss: 0.5677 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.5911 - accuracy: 0.6313 - f1_m: 0.631 - ETA: 0s - loss: 0.5902 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.5777 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5722 - accuracy: 0.6523 - f1_m: 0.652 - 1s 4ms/sample - loss: 0.5813 - accuracy: 0.6406 - f1_m: 0.6376\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 6ms/sample - loss: 0.5841 - accuracy: 0.4928 - f1_m: 0.5792\n",
      "f1_m: 57.92%\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_58 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 16s - loss: 0.6931 - accuracy: 0.6250 - f1_m: 0.13 - ETA: 7s - loss: 0.6888 - accuracy: 0.6250 - f1_m: 0.3807 - ETA: 4s - loss: 0.6798 - accuracy: 0.6667 - f1_m: 0.503 - ETA: 3s - loss: 0.6601 - accuracy: 0.6719 - f1_m: 0.549 - ETA: 2s - loss: 1.0108 - accuracy: 0.6625 - f1_m: 0.564 - ETA: 1s - loss: 0.9584 - accuracy: 0.6354 - f1_m: 0.554 - ETA: 0s - loss: 0.9145 - accuracy: 0.6384 - f1_m: 0.568 - ETA: 0s - loss: 0.8791 - accuracy: 0.6445 - f1_m: 0.583 - 3s 11ms/sample - loss: 0.8601 - accuracy: 0.6406 - f1_m: 0.5853\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.5670 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.6301 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6401 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6293 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.6079 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5875 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5631 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6046 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.6242 - accuracy: 0.6940 - f1_m: 0.6926\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5282 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4586 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4896 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5116 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5264 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5620 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5598 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5594 - accuracy: 0.7383 - f1_m: 0.738 - 1s 4ms/sample - loss: 0.5606 - accuracy: 0.7402 - f1_m: 0.7407\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5920 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5498 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5602 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5580 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5917 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5899 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5817 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5863 - accuracy: 0.7031 - f1_m: 0.703 - 1s 4ms/sample - loss: 0.5781 - accuracy: 0.7046 - f1_m: 0.7050\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.7747 - accuracy: 0.5000 - f1_m: 0.500 - ETA: 0s - loss: 0.5680 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5254 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5543 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5531 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.5685 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.5666 - accuracy: 0.6696 - f1_m: 0.669 - ETA: 0s - loss: 0.5751 - accuracy: 0.6602 - f1_m: 0.660 - 1s 4ms/sample - loss: 0.5932 - accuracy: 0.6406 - f1_m: 0.6357\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5800 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5891 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5579 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5792 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5925 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5764 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5942 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.5867 - accuracy: 0.7224 - f1_m: 0.7243\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4502 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5024 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4891 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4980 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5367 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5473 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5636 - accuracy: 0.6741 - f1_m: 0.674 - ETA: 0s - loss: 0.5602 - accuracy: 0.6797 - f1_m: 0.679 - 1s 4ms/sample - loss: 0.5566 - accuracy: 0.6975 - f1_m: 0.7019\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5738 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.4798 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4825 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5190 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5382 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5308 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5573 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5606 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5681 - accuracy: 0.7011 - f1_m: 0.6986\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5430 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5167 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5437 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5338 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5758 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5703 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5769 - accuracy: 0.7070 - f1_m: 0.707 - 1s 4ms/sample - loss: 0.5767 - accuracy: 0.7082 - f1_m: 0.7085\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5738 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6092 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5661 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5702 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5524 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5551 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5387 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5316 - accuracy: 0.7305 - f1_m: 0.730 - 1s 4ms/sample - loss: 0.5525 - accuracy: 0.7153 - f1_m: 0.7115\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 6ms/sample - loss: 0.5249 - accuracy: 0.6087 - f1_m: 0.6625\n",
      "f1_m: 66.25%\n",
      "63.24% (+/- 5.50%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario E\n",
    "# X_train_raw\n",
    "do_experiment(X_train_raw, y_train_raw, 'lstm', 50, entity_masking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_64 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_64 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 12s - loss: 0.6935 - accuracy: 0.5625 - f1_m: 0.29 - ETA: 5s - loss: 0.6884 - accuracy: 0.6094 - f1_m: 0.4740 - ETA: 3s - loss: 0.6762 - accuracy: 0.6562 - f1_m: 0.566 - ETA: 2s - loss: 0.6763 - accuracy: 0.6406 - f1_m: 0.572 - ETA: 1s - loss: 0.6731 - accuracy: 0.6500 - f1_m: 0.595 - ETA: 1s - loss: 0.6795 - accuracy: 0.6250 - f1_m: 0.579 - ETA: 0s - loss: 0.6735 - accuracy: 0.6518 - f1_m: 0.613 - ETA: 0s - loss: 0.6666 - accuracy: 0.6562 - f1_m: 0.622 - 3s 10ms/sample - loss: 0.6645 - accuracy: 0.6595 - f1_m: 0.6305\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5654 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4012 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.6496 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 1.0506 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.9789 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.9152 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.8496 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.8220 - accuracy: 0.7031 - f1_m: 0.703 - 1s 4ms/sample - loss: 0.7922 - accuracy: 0.7168 - f1_m: 0.7216\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5254 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5270 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5414 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5503 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5488 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5223 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5144 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5300 - accuracy: 0.7422 - f1_m: 0.742 - 1s 4ms/sample - loss: 0.5438 - accuracy: 0.7276 - f1_m: 0.7225\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.7273 - accuracy: 0.5000 - f1_m: 0.500 - ETA: 0s - loss: 0.6935 - accuracy: 0.5469 - f1_m: 0.546 - ETA: 0s - loss: 0.6473 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6028 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.5905 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5764 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5448 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5520 - accuracy: 0.7305 - f1_m: 0.730 - 1s 4ms/sample - loss: 0.5617 - accuracy: 0.7240 - f1_m: 0.7218\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.6303 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6010 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5433 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5594 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5538 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5673 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5835 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5797 - accuracy: 0.7188 - f1_m: 0.718 - 1s 4ms/sample - loss: 0.5689 - accuracy: 0.7240 - f1_m: 0.7258\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6159 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6112 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.5764 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5630 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5452 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5836 - accuracy: 0.6786 - f1_m: 0.678 - ETA: 0s - loss: 0.5847 - accuracy: 0.6797 - f1_m: 0.679 - 1s 4ms/sample - loss: 0.5775 - accuracy: 0.6953 - f1_m: 0.7008\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5458 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6274 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6961 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 0s - loss: 0.6622 - accuracy: 0.6375 - f1_m: 0.637 - ETA: 0s - loss: 0.6698 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6647 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6516 - accuracy: 0.6602 - f1_m: 0.660 - 1s 4ms/sample - loss: 0.6387 - accuracy: 0.6631 - f1_m: 0.6641\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.6206 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.5465 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5935 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5730 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.6076 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.6197 - accuracy: 0.6823 - f1_m: 0.682 - ETA: 0s - loss: 0.6050 - accuracy: 0.6964 - f1_m: 0.696 - ETA: 0s - loss: 0.6128 - accuracy: 0.6836 - f1_m: 0.683 - 1s 4ms/sample - loss: 0.6017 - accuracy: 0.6953 - f1_m: 0.6994\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.4945 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.6929 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6833 - accuracy: 0.6146 - f1_m: 0.614 - ETA: 0s - loss: 0.6730 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6587 - accuracy: 0.5813 - f1_m: 0.581 - ETA: 0s - loss: 0.6448 - accuracy: 0.5677 - f1_m: 0.567 - ETA: 0s - loss: 0.6375 - accuracy: 0.5759 - f1_m: 0.575 - ETA: 0s - loss: 0.6413 - accuracy: 0.5586 - f1_m: 0.558 - 1s 4ms/sample - loss: 0.6326 - accuracy: 0.5699 - f1_m: 0.5738\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 0s - loss: 0.5154 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5503 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5468 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5265 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5546 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5884 - accuracy: 0.6518 - f1_m: 0.651 - ETA: 0s - loss: 0.5991 - accuracy: 0.6445 - f1_m: 0.644 - 1s 4ms/sample - loss: 0.6094 - accuracy: 0.6380 - f1_m: 0.6357\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 17ms/sample - loss: 0.7022 - accuracy: 0.6761 - f1_m: 0.6116\n",
      "f1_m: 61.16%\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_65 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 11s - loss: 0.6924 - accuracy: 0.7812 - f1_m: 0.34 - ETA: 5s - loss: 0.6838 - accuracy: 0.7656 - f1_m: 0.5457 - ETA: 3s - loss: 0.6835 - accuracy: 0.6979 - f1_m: 0.551 - ETA: 2s - loss: 0.6790 - accuracy: 0.6797 - f1_m: 0.569 - ETA: 1s - loss: 0.6427 - accuracy: 0.7000 - f1_m: 0.612 - ETA: 0s - loss: 1.0655 - accuracy: 0.6458 - f1_m: 0.572 - ETA: 0s - loss: 0.9880 - accuracy: 0.6562 - f1_m: 0.593 - ETA: 0s - loss: 0.9484 - accuracy: 0.6602 - f1_m: 0.605 - 3s 9ms/sample - loss: 0.9200 - accuracy: 0.6595 - f1_m: 0.6104\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4767 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5173 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5193 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.4841 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5635 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5606 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5662 - accuracy: 0.7422 - f1_m: 0.742 - 1s 4ms/sample - loss: 0.5672 - accuracy: 0.7455 - f1_m: 0.7467\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5143 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5592 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5199 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5155 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5048 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5062 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5310 - accuracy: 0.7383 - f1_m: 0.738 - 1s 4ms/sample - loss: 0.5407 - accuracy: 0.7276 - f1_m: 0.7239\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.6130 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5949 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.6274 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.6463 - accuracy: 0.6687 - f1_m: 0.668 - ETA: 0s - loss: 0.6285 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.6130 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.6051 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.6122 - accuracy: 0.6918 - f1_m: 0.6892\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5124 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4868 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4697 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.4940 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5395 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5356 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5322 - accuracy: 0.7578 - f1_m: 0.757 - 1s 4ms/sample - loss: 0.5487 - accuracy: 0.7455 - f1_m: 0.7412\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5447 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5876 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5951 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6092 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.6055 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5890 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5735 - accuracy: 0.7188 - f1_m: 0.718 - 1s 4ms/sample - loss: 0.5717 - accuracy: 0.7204 - f1_m: 0.7210\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5760 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5409 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5158 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5482 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5299 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5475 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5435 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5560 - accuracy: 0.7305 - f1_m: 0.730 - 1s 4ms/sample - loss: 0.5530 - accuracy: 0.7348 - f1_m: 0.7363\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5948 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5347 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5496 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5526 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5365 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5148 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5454 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5550 - accuracy: 0.7188 - f1_m: 0.718 - 1s 4ms/sample - loss: 0.5462 - accuracy: 0.7312 - f1_m: 0.7355\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5330 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5348 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5716 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5536 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5559 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5648 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5520 - accuracy: 0.7266 - f1_m: 0.726 - 1s 4ms/sample - loss: 0.5531 - accuracy: 0.7240 - f1_m: 0.7231\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5971 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6165 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5919 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5651 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5676 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5489 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5562 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5539 - accuracy: 0.7266 - f1_m: 0.726 - 1s 4ms/sample - loss: 0.5467 - accuracy: 0.7384 - f1_m: 0.7425\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 6ms/sample - loss: 0.5985 - accuracy: 0.6620 - f1_m: 0.6756\n",
      "f1_m: 67.56%\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_66 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 11s - loss: 0.6930 - accuracy: 0.7812 - f1_m: 0.31 - ETA: 5s - loss: 0.6841 - accuracy: 0.7812 - f1_m: 0.5462 - ETA: 3s - loss: 0.6735 - accuracy: 0.7708 - f1_m: 0.614 - ETA: 2s - loss: 0.6933 - accuracy: 0.7188 - f1_m: 0.601 - ETA: 1s - loss: 0.6914 - accuracy: 0.6875 - f1_m: 0.593 - ETA: 0s - loss: 0.6888 - accuracy: 0.6771 - f1_m: 0.598 - ETA: 0s - loss: 0.6840 - accuracy: 0.6830 - f1_m: 0.615 - ETA: 0s - loss: 0.6813 - accuracy: 0.6797 - f1_m: 0.620 - 2s 9ms/sample - loss: 0.6811 - accuracy: 0.6714 - f1_m: 0.6167\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.6700 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6523 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6550 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.6332 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5860 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5779 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.6111 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.6080 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.6120 - accuracy: 0.7071 - f1_m: 0.7049\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.5661 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5395 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5736 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5755 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5827 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5943 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.6005 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.6040 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.6012 - accuracy: 0.7036 - f1_m: 0.7049\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6005 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5760 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6157 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6025 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5785 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5549 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5819 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5802 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5682 - accuracy: 0.7214 - f1_m: 0.7245\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.6334 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5987 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5839 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5720 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6260 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.6131 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.6110 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.6089 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.6086 - accuracy: 0.7036 - f1_m: 0.7049\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.5957 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5927 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5392 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5773 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5806 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5812 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5706 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5656 - accuracy: 0.7188 - f1_m: 0.718 - 1s 4ms/sample - loss: 0.5649 - accuracy: 0.7214 - f1_m: 0.7222\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.4988 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5987 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5646 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5420 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5565 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5837 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5795 - accuracy: 0.6741 - f1_m: 0.674 - ETA: 0s - loss: 0.5701 - accuracy: 0.6836 - f1_m: 0.683 - 1s 4ms/sample - loss: 0.5728 - accuracy: 0.6857 - f1_m: 0.6863\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.4796 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5493 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5536 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5503 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5291 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5895 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5952 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5936 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.5858 - accuracy: 0.7286 - f1_m: 0.7326\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4944 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5700 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5612 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5915 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5745 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5778 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5743 - accuracy: 0.7227 - f1_m: 0.722 - 1s 4ms/sample - loss: 0.5764 - accuracy: 0.7214 - f1_m: 0.7211\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.6524 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6157 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5958 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5736 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.6444 - accuracy: 0.6687 - f1_m: 0.668 - ETA: 0s - loss: 0.6298 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6150 - accuracy: 0.6830 - f1_m: 0.683 - ETA: 0s - loss: 0.5882 - accuracy: 0.7031 - f1_m: 0.703 - 1s 4ms/sample - loss: 0.5862 - accuracy: 0.7036 - f1_m: 0.7037\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.4645 - accuracy: 0.7000 - f1_m: 0.6910\n",
      "f1_m: 69.10%\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_67 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 11s - loss: 0.6939 - accuracy: 0.7500 - f1_m: 0.22 - ETA: 5s - loss: 0.6900 - accuracy: 0.6719 - f1_m: 0.4105 - ETA: 3s - loss: 0.6835 - accuracy: 0.6771 - f1_m: 0.502 - ETA: 2s - loss: 0.6652 - accuracy: 0.6953 - f1_m: 0.564 - ETA: 1s - loss: 0.9174 - accuracy: 0.6625 - f1_m: 0.558 - ETA: 1s - loss: 0.8712 - accuracy: 0.6719 - f1_m: 0.584 - ETA: 0s - loss: 0.8441 - accuracy: 0.6562 - f1_m: 0.581 - ETA: 0s - loss: 0.8167 - accuracy: 0.6680 - f1_m: 0.602 - 3s 9ms/sample - loss: 0.8000 - accuracy: 0.6726 - f1_m: 0.6157\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6007 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6459 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6439 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6581 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6591 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6504 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 0s - loss: 0.6547 - accuracy: 0.6295 - f1_m: 0.629 - ETA: 0s - loss: 0.6426 - accuracy: 0.6523 - f1_m: 0.652 - 1s 4ms/sample - loss: 0.6267 - accuracy: 0.6726 - f1_m: 0.6776\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5183 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.8154 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.7178 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.7695 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.7258 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.6981 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6682 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6817 - accuracy: 0.7031 - f1_m: 0.703 - 1s 4ms/sample - loss: 0.6787 - accuracy: 0.7046 - f1_m: 0.7050\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5988 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5749 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5614 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5372 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5484 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5609 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5572 - accuracy: 0.7305 - f1_m: 0.730 - 1s 4ms/sample - loss: 0.5753 - accuracy: 0.7189 - f1_m: 0.7160\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - ETA: 1s - loss: 0.5831 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5861 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6001 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5915 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5883 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5962 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5837 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5735 - accuracy: 0.7109 - f1_m: 0.710 - 1s 4ms/sample - loss: 0.5632 - accuracy: 0.7189 - f1_m: 0.7208\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6711 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5830 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6042 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5705 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5744 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5713 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5726 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5673 - accuracy: 0.7227 - f1_m: 0.722 - 1s 4ms/sample - loss: 0.5748 - accuracy: 0.7117 - f1_m: 0.7090\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.9062 - f1_m: 0.906 - ETA: 0s - loss: 0.5144 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 0s - loss: 0.5247 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5273 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5363 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5416 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5433 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.5514 - accuracy: 0.7773 - f1_m: 0.777 - 1s 4ms/sample - loss: 0.5403 - accuracy: 0.7865 - f1_m: 0.7887\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6617 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.8471 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 0s - loss: 0.7622 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.7012 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6692 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.6647 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6446 - accuracy: 0.6786 - f1_m: 0.678 - ETA: 0s - loss: 0.6335 - accuracy: 0.6797 - f1_m: 0.679 - 1s 4ms/sample - loss: 0.6244 - accuracy: 0.6833 - f1_m: 0.6842\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4717 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5188 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5159 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5297 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.4870 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5374 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5333 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5422 - accuracy: 0.7383 - f1_m: 0.738 - 1s 4ms/sample - loss: 0.5472 - accuracy: 0.7331 - f1_m: 0.7318\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.7513 - accuracy: 0.4375 - f1_m: 0.437 - ETA: 0s - loss: 0.6913 - accuracy: 0.5000 - f1_m: 0.500 - ETA: 0s - loss: 0.6838 - accuracy: 0.5417 - f1_m: 0.541 - ETA: 0s - loss: 0.6649 - accuracy: 0.5547 - f1_m: 0.554 - ETA: 0s - loss: 0.6310 - accuracy: 0.6062 - f1_m: 0.606 - ETA: 0s - loss: 0.6152 - accuracy: 0.6302 - f1_m: 0.630 - ETA: 0s - loss: 0.6117 - accuracy: 0.6384 - f1_m: 0.638 - ETA: 0s - loss: 0.5966 - accuracy: 0.6523 - f1_m: 0.652 - 1s 4ms/sample - loss: 0.5967 - accuracy: 0.6619 - f1_m: 0.6643\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.6228 - accuracy: 0.6667 - f1_m: 0.5917\n",
      "f1_m: 59.17%\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_68 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "lstm_68 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 215,106\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 20s - loss: 0.6932 - accuracy: 0.7188 - f1_m: 0.34 - ETA: 9s - loss: 0.6917 - accuracy: 0.6406 - f1_m: 0.4515 - ETA: 5s - loss: 0.6918 - accuracy: 0.6042 - f1_m: 0.478 - ETA: 3s - loss: 0.6873 - accuracy: 0.6328 - f1_m: 0.538 - ETA: 2s - loss: 0.6855 - accuracy: 0.6250 - f1_m: 0.549 - ETA: 1s - loss: 0.6700 - accuracy: 0.6458 - f1_m: 0.582 - ETA: 0s - loss: 0.7591 - accuracy: 0.6562 - f1_m: 0.602 - ETA: 0s - loss: 0.7422 - accuracy: 0.6602 - f1_m: 0.612 - 4s 13ms/sample - loss: 0.7252 - accuracy: 0.6655 - f1_m: 0.6248\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6890 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6336 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6596 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6740 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6486 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.6365 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6422 - accuracy: 0.6652 - f1_m: 0.665 - ETA: 0s - loss: 0.6194 - accuracy: 0.6875 - f1_m: 0.687 - 1s 4ms/sample - loss: 0.6080 - accuracy: 0.6975 - f1_m: 0.7000\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.7808 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5961 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5724 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5659 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5877 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.6013 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5984 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.6118 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.6043 - accuracy: 0.7260 - f1_m: 0.7287\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5910 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5889 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5905 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5783 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5689 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5737 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5700 - accuracy: 0.7148 - f1_m: 0.714 - 1s 4ms/sample - loss: 0.5615 - accuracy: 0.7224 - f1_m: 0.7243\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5606 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5676 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5445 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5650 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5593 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5595 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5544 - accuracy: 0.7227 - f1_m: 0.722 - 1s 4ms/sample - loss: 0.5751 - accuracy: 0.7117 - f1_m: 0.7090\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6230 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5973 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5867 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5918 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5790 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5592 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5553 - accuracy: 0.7500 - f1_m: 0.750 - 1s 4ms/sample - loss: 0.5627 - accuracy: 0.7402 - f1_m: 0.7378\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.7140 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5840 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5588 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5383 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.5768 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5767 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5628 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5780 - accuracy: 0.7461 - f1_m: 0.746 - 1s 4ms/sample - loss: 0.5762 - accuracy: 0.7331 - f1_m: 0.7299\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.6277 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5657 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6098 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5988 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6302 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.6056 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5919 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5918 - accuracy: 0.6914 - f1_m: 0.691 - 1s 4ms/sample - loss: 0.5848 - accuracy: 0.6975 - f1_m: 0.6990\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.4338 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.4436 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5047 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5232 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5170 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5228 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5151 - accuracy: 0.7461 - f1_m: 0.746 - 1s 4ms/sample - loss: 0.5256 - accuracy: 0.7402 - f1_m: 0.7387\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6502 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6314 - accuracy: 0.6146 - f1_m: 0.614 - ETA: 0s - loss: 0.5932 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5798 - accuracy: 0.6500 - f1_m: 0.650 - ETA: 0s - loss: 0.5812 - accuracy: 0.6510 - f1_m: 0.651 - ETA: 0s - loss: 0.6005 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5998 - accuracy: 0.6523 - f1_m: 0.652 - 1s 4ms/sample - loss: 0.6009 - accuracy: 0.6548 - f1_m: 0.6554\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.3725 - accuracy: 0.8406 - f1_m: 0.8854\n",
      "f1_m: 88.54%\n",
      "69.11% (+/- 10.41%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario F\n",
    "# X_train_raw\n",
    "do_experiment(X_train_raw, y_train_raw, 'lstm', 50, entity_masking=True, dropout_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_59 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_59 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 11s - loss: 0.6931 - accuracy: 0.7812 - f1_m: 0.36 - ETA: 5s - loss: 0.6868 - accuracy: 0.7188 - f1_m: 0.5099 - ETA: 3s - loss: 0.6768 - accuracy: 0.6875 - f1_m: 0.548 - ETA: 2s - loss: 0.7068 - accuracy: 0.6953 - f1_m: 0.590 - ETA: 1s - loss: 0.6955 - accuracy: 0.7000 - f1_m: 0.616 - ETA: 1s - loss: 0.6862 - accuracy: 0.7083 - f1_m: 0.638 - ETA: 0s - loss: 0.6738 - accuracy: 0.7143 - f1_m: 0.654 - ETA: 0s - loss: 0.6699 - accuracy: 0.7070 - f1_m: 0.654 - 3s 10ms/sample - loss: 0.6668 - accuracy: 0.7025 - f1_m: 0.6545\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6187 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6260 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6340 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.6295 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6164 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.6025 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6522 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6426 - accuracy: 0.6602 - f1_m: 0.660 - 1s 5ms/sample - loss: 0.6302 - accuracy: 0.6703 - f1_m: 0.6738\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5941 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6757 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6339 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6203 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.6132 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5955 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5747 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5786 - accuracy: 0.7266 - f1_m: 0.726 - 1s 5ms/sample - loss: 0.5836 - accuracy: 0.7276 - f1_m: 0.7280\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5613 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5220 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5654 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5581 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5751 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5561 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5372 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5389 - accuracy: 0.7578 - f1_m: 0.757 - 1s 5ms/sample - loss: 0.5265 - accuracy: 0.7670 - f1_m: 0.7702\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5627 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.6787 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6460 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6359 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6291 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.6165 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5915 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5974 - accuracy: 0.6953 - f1_m: 0.695 - 1s 5ms/sample - loss: 0.6068 - accuracy: 0.6918 - f1_m: 0.6905\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5342 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5574 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5437 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5610 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5558 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5721 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5626 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5464 - accuracy: 0.7305 - f1_m: 0.730 - 1s 5ms/sample - loss: 0.5282 - accuracy: 0.7455 - f1_m: 0.7508\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5217 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5555 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5242 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5542 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5923 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.6059 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6011 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5937 - accuracy: 0.7227 - f1_m: 0.722 - 1s 5ms/sample - loss: 0.5853 - accuracy: 0.7312 - f1_m: 0.7341\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5071 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5404 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5069 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5047 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5607 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5780 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5729 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5738 - accuracy: 0.7383 - f1_m: 0.738 - 1s 5ms/sample - loss: 0.5835 - accuracy: 0.7312 - f1_m: 0.7287\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6675 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6612 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6155 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5809 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.6189 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.6376 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.6369 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6186 - accuracy: 0.7031 - f1_m: 0.703 - 1s 5ms/sample - loss: 0.6051 - accuracy: 0.7097 - f1_m: 0.7120\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5545 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6075 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5627 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5648 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5580 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5474 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5630 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5705 - accuracy: 0.6914 - f1_m: 0.691 - 1s 5ms/sample - loss: 0.5698 - accuracy: 0.6989 - f1_m: 0.7015\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 8ms/sample - loss: 0.6360 - accuracy: 0.7465 - f1_m: 0.6637\n",
      "f1_m: 66.37%\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_60 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 15s - loss: 0.6930 - accuracy: 0.8125 - f1_m: 0.25 - ETA: 7s - loss: 0.6887 - accuracy: 0.7188 - f1_m: 0.4407 - ETA: 4s - loss: 0.6855 - accuracy: 0.6771 - f1_m: 0.491 - ETA: 2s - loss: 0.6715 - accuracy: 0.6719 - f1_m: 0.532 - ETA: 1s - loss: 0.7407 - accuracy: 0.6687 - f1_m: 0.557 - ETA: 1s - loss: 0.7093 - accuracy: 0.6979 - f1_m: 0.605 - ETA: 0s - loss: 0.7015 - accuracy: 0.6875 - f1_m: 0.608 - ETA: 0s - loss: 0.6906 - accuracy: 0.6875 - f1_m: 0.618 - 3s 11ms/sample - loss: 0.6780 - accuracy: 0.6810 - f1_m: 0.6170\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3796 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 0s - loss: 0.3748 - accuracy: 0.8594 - f1_m: 0.859 - ETA: 0s - loss: 0.4821 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5049 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.4731 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.5496 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5562 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5540 - accuracy: 0.7227 - f1_m: 0.722 - 1s 5ms/sample - loss: 0.5766 - accuracy: 0.7133 - f1_m: 0.7100\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6421 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6038 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5765 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5471 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5448 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5279 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5499 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5515 - accuracy: 0.7227 - f1_m: 0.722 - 1s 5ms/sample - loss: 0.5389 - accuracy: 0.7348 - f1_m: 0.7390\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5503 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5920 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6115 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6165 - accuracy: 0.6172 - f1_m: 0.617 - ETA: 0s - loss: 0.6003 - accuracy: 0.6375 - f1_m: 0.637 - ETA: 0s - loss: 0.5712 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.5408 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.5317 - accuracy: 0.6992 - f1_m: 0.699 - 1s 5ms/sample - loss: 0.5359 - accuracy: 0.6918 - f1_m: 0.6892\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5010 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5804 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5680 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5325 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5708 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5575 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5418 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5510 - accuracy: 0.7383 - f1_m: 0.738 - 1s 5ms/sample - loss: 0.5465 - accuracy: 0.7455 - f1_m: 0.7480\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6200 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5662 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5267 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5176 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5291 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5562 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5477 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5431 - accuracy: 0.7422 - f1_m: 0.742 - 1s 5ms/sample - loss: 0.5472 - accuracy: 0.7348 - f1_m: 0.7322\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4289 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.5295 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5138 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5384 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5302 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5198 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5437 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5446 - accuracy: 0.7266 - f1_m: 0.726 - 1s 5ms/sample - loss: 0.5488 - accuracy: 0.7168 - f1_m: 0.7135\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5186 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5496 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5530 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5620 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5695 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5469 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5508 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5418 - accuracy: 0.7383 - f1_m: 0.738 - 1s 5ms/sample - loss: 0.5351 - accuracy: 0.7348 - f1_m: 0.7335\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3112 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.5850 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5662 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.6161 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6139 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5998 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5820 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5563 - accuracy: 0.7188 - f1_m: 0.718 - 1s 5ms/sample - loss: 0.5415 - accuracy: 0.7240 - f1_m: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5903 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5658 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5762 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5802 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.5748 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5553 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5385 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5447 - accuracy: 0.7266 - f1_m: 0.726 - 1s 5ms/sample - loss: 0.5251 - accuracy: 0.7384 - f1_m: 0.7425\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.7210 - accuracy: 0.6761 - f1_m: 0.6860\n",
      "f1_m: 68.60%\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_61 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 18s - loss: 0.6937 - accuracy: 0.6250 - f1_m: 0.09 - ETA: 8s - loss: 0.6875 - accuracy: 0.6562 - f1_m: 0.3925 - ETA: 5s - loss: 0.6833 - accuracy: 0.6354 - f1_m: 0.459 - ETA: 3s - loss: 0.6659 - accuracy: 0.6484 - f1_m: 0.516 - ETA: 2s - loss: 0.7694 - accuracy: 0.6562 - f1_m: 0.550 - ETA: 1s - loss: 0.7659 - accuracy: 0.6458 - f1_m: 0.557 - ETA: 0s - loss: 0.7530 - accuracy: 0.6429 - f1_m: 0.567 - ETA: 0s - loss: 0.7368 - accuracy: 0.6602 - f1_m: 0.594 - 4s 13ms/sample - loss: 0.7270 - accuracy: 0.6643 - f1_m: 0.6069\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5889 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.6183 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6069 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6001 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5956 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.6015 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5936 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.6081 - accuracy: 0.6875 - f1_m: 0.687 - 1s 5ms/sample - loss: 0.6085 - accuracy: 0.6929 - f1_m: 0.6944\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5364 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5665 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5692 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5822 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5856 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5907 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5805 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5860 - accuracy: 0.7188 - f1_m: 0.718 - 1s 5ms/sample - loss: 0.5872 - accuracy: 0.7107 - f1_m: 0.7083\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6375 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.9491 - accuracy: 0.4688 - f1_m: 0.468 - ETA: 0s - loss: 0.8390 - accuracy: 0.5417 - f1_m: 0.541 - ETA: 0s - loss: 0.7682 - accuracy: 0.5859 - f1_m: 0.585 - ETA: 0s - loss: 0.7009 - accuracy: 0.6438 - f1_m: 0.643 - ETA: 0s - loss: 0.6926 - accuracy: 0.6510 - f1_m: 0.651 - ETA: 0s - loss: 0.6659 - accuracy: 0.6652 - f1_m: 0.665 - ETA: 0s - loss: 0.6726 - accuracy: 0.6641 - f1_m: 0.664 - 1s 5ms/sample - loss: 0.6667 - accuracy: 0.6679 - f1_m: 0.6690\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5555 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.6534 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6128 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6124 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5932 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5612 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5647 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5593 - accuracy: 0.7344 - f1_m: 0.734 - 1s 5ms/sample - loss: 0.5654 - accuracy: 0.7214 - f1_m: 0.7176\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6790 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6290 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6038 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5736 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5526 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5371 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5539 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5590 - accuracy: 0.7266 - f1_m: 0.726 - 1s 5ms/sample - loss: 0.5504 - accuracy: 0.7357 - f1_m: 0.7384\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6246 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5530 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5374 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5015 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5311 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5577 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5462 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5435 - accuracy: 0.7344 - f1_m: 0.734 - 1s 5ms/sample - loss: 0.5394 - accuracy: 0.7357 - f1_m: 0.7361\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6204 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5647 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5680 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5643 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5605 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5832 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5743 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5554 - accuracy: 0.7383 - f1_m: 0.738 - 1s 5ms/sample - loss: 0.5518 - accuracy: 0.7357 - f1_m: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5024 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.4303 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5452 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5658 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5638 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5663 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5614 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5826 - accuracy: 0.7031 - f1_m: 0.703 - 1s 5ms/sample - loss: 0.5796 - accuracy: 0.7071 - f1_m: 0.7083\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6436 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5472 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6268 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6108 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5931 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5636 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5660 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5694 - accuracy: 0.7305 - f1_m: 0.730 - 1s 5ms/sample - loss: 0.5572 - accuracy: 0.7393 - f1_m: 0.7419\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.4707 - accuracy: 0.6857 - f1_m: 0.6806\n",
      "f1_m: 68.06%\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_62 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 12s - loss: 0.6930 - accuracy: 0.6875 - f1_m: 0.19 - ETA: 5s - loss: 0.6865 - accuracy: 0.7031 - f1_m: 0.4546 - ETA: 3s - loss: 0.6778 - accuracy: 0.7083 - f1_m: 0.542 - ETA: 2s - loss: 0.6777 - accuracy: 0.6797 - f1_m: 0.555 - ETA: 1s - loss: 0.6783 - accuracy: 0.6562 - f1_m: 0.556 - ETA: 1s - loss: 0.6743 - accuracy: 0.6615 - f1_m: 0.578 - ETA: 0s - loss: 0.6656 - accuracy: 0.6696 - f1_m: 0.598 - ETA: 0s - loss: 0.6701 - accuracy: 0.6719 - f1_m: 0.609 - 3s 10ms/sample - loss: 0.6612 - accuracy: 0.6797 - f1_m: 0.6264\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 1.1711 - accuracy: 0.5000 - f1_m: 0.500 - ETA: 0s - loss: 0.8960 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 0s - loss: 0.7828 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 0s - loss: 0.7140 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6817 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.6726 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6660 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.6435 - accuracy: 0.6992 - f1_m: 0.699 - 1s 5ms/sample - loss: 0.6330 - accuracy: 0.7046 - f1_m: 0.7060\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6764 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6892 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6530 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6137 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5920 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5859 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5815 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5661 - accuracy: 0.7148 - f1_m: 0.714 - 1s 5ms/sample - loss: 0.5549 - accuracy: 0.7224 - f1_m: 0.7243\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5790 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5376 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5158 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5121 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5644 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5611 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5647 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5651 - accuracy: 0.7070 - f1_m: 0.707 - 1s 5ms/sample - loss: 0.5522 - accuracy: 0.7224 - f1_m: 0.7262\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5753 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5436 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5617 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5516 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5229 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5361 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5432 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5304 - accuracy: 0.7188 - f1_m: 0.718 - 1s 5ms/sample - loss: 0.5534 - accuracy: 0.7082 - f1_m: 0.7056\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.7460 - accuracy: 0.5000 - f1_m: 0.500 - ETA: 0s - loss: 0.6376 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6022 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5748 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5704 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5525 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5326 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5390 - accuracy: 0.7305 - f1_m: 0.730 - 1s 5ms/sample - loss: 0.5341 - accuracy: 0.7331 - f1_m: 0.7337\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4750 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5097 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5695 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6092 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5994 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5834 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5539 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5814 - accuracy: 0.7148 - f1_m: 0.714 - 1s 5ms/sample - loss: 0.6021 - accuracy: 0.7011 - f1_m: 0.6976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4861 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5219 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.4711 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5045 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5126 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5345 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5338 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5280 - accuracy: 0.7461 - f1_m: 0.746 - 1s 5ms/sample - loss: 0.5270 - accuracy: 0.7438 - f1_m: 0.7432\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4675 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4691 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.4992 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5173 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5339 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5450 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5399 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5342 - accuracy: 0.7305 - f1_m: 0.730 - 1s 5ms/sample - loss: 0.5353 - accuracy: 0.7331 - f1_m: 0.7337\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4904 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5061 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5158 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5426 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5645 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5606 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5644 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5491 - accuracy: 0.7227 - f1_m: 0.722 - 1s 5ms/sample - loss: 0.5563 - accuracy: 0.7224 - f1_m: 0.7224\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.4860 - accuracy: 0.7536 - f1_m: 0.7667\n",
      "f1_m: 76.67%\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_63 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 12s - loss: 0.6931 - accuracy: 0.9375 - f1_m: 0.16 - ETA: 5s - loss: 0.6863 - accuracy: 0.8281 - f1_m: 0.4427 - ETA: 3s - loss: 0.6852 - accuracy: 0.7396 - f1_m: 0.482 - ETA: 2s - loss: 0.6781 - accuracy: 0.7188 - f1_m: 0.526 - ETA: 1s - loss: 0.7048 - accuracy: 0.6938 - f1_m: 0.539 - ETA: 1s - loss: 0.6964 - accuracy: 0.6927 - f1_m: 0.564 - ETA: 0s - loss: 0.6944 - accuracy: 0.6875 - f1_m: 0.577 - ETA: 0s - loss: 0.6864 - accuracy: 0.6875 - f1_m: 0.591 - 3s 10ms/sample - loss: 0.6706 - accuracy: 0.6904 - f1_m: 0.6055\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4874 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5374 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5560 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5451 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5620 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5749 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5755 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.5843 - accuracy: 0.6797 - f1_m: 0.679 - 1s 5ms/sample - loss: 0.5792 - accuracy: 0.6797 - f1_m: 0.6797\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5655 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.5950 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.5889 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6200 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6218 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.6155 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6125 - accuracy: 0.6652 - f1_m: 0.665 - ETA: 0s - loss: 0.6028 - accuracy: 0.6680 - f1_m: 0.668 - 1s 5ms/sample - loss: 0.5775 - accuracy: 0.6868 - f1_m: 0.6915\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.7890 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6820 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6107 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.6014 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5978 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.6020 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.6053 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5893 - accuracy: 0.7305 - f1_m: 0.730 - 1s 5ms/sample - loss: 0.5892 - accuracy: 0.7224 - f1_m: 0.7204\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5543 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6022 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6040 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5697 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5737 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5948 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5870 - accuracy: 0.6964 - f1_m: 0.696 - ETA: 0s - loss: 0.5918 - accuracy: 0.6875 - f1_m: 0.687 - 1s 5ms/sample - loss: 0.5813 - accuracy: 0.6975 - f1_m: 0.7000\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4778 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5965 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5769 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5567 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5680 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5684 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5834 - accuracy: 0.6696 - f1_m: 0.669 - ETA: 0s - loss: 0.5726 - accuracy: 0.6875 - f1_m: 0.687 - 1s 5ms/sample - loss: 0.5685 - accuracy: 0.6940 - f1_m: 0.6956\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4249 - accuracy: 0.9062 - f1_m: 0.906 - ETA: 0s - loss: 0.5616 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5813 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.6042 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6024 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5878 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5780 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5832 - accuracy: 0.7148 - f1_m: 0.714 - 1s 5ms/sample - loss: 0.5768 - accuracy: 0.7260 - f1_m: 0.7287\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4629 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5322 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.5172 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 0s - loss: 0.5435 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.5416 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.5692 - accuracy: 0.6510 - f1_m: 0.651 - ETA: 0s - loss: 0.5743 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5869 - accuracy: 0.6562 - f1_m: 0.656 - 1s 5ms/sample - loss: 0.5869 - accuracy: 0.6548 - f1_m: 0.6544\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5764 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5882 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5758 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5674 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5752 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5694 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5739 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5676 - accuracy: 0.7188 - f1_m: 0.718 - 1s 5ms/sample - loss: 0.5526 - accuracy: 0.7224 - f1_m: 0.7233\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5221 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5070 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5884 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5587 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5608 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5441 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5596 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5627 - accuracy: 0.7109 - f1_m: 0.710 - 1s 5ms/sample - loss: 0.5681 - accuracy: 0.7011 - f1_m: 0.6986\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.3704 - accuracy: 0.8261 - f1_m: 0.8750\n",
      "f1_m: 87.50%\n",
      "73.44% (+/- 7.88%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario G\n",
    "do_experiment(X_train_raw, y_train_raw, 'lstm', 100, entity_masking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_74 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 12s - loss: 0.6931 - accuracy: 0.8125 - f1_m: 0.38 - ETA: 5s - loss: 0.6900 - accuracy: 0.7188 - f1_m: 0.5030 - ETA: 3s - loss: 0.6843 - accuracy: 0.6979 - f1_m: 0.554 - ETA: 2s - loss: 0.6914 - accuracy: 0.6562 - f1_m: 0.548 - ETA: 1s - loss: 0.6863 - accuracy: 0.6687 - f1_m: 0.582 - ETA: 1s - loss: 0.6803 - accuracy: 0.6719 - f1_m: 0.600 - ETA: 0s - loss: 0.6750 - accuracy: 0.6696 - f1_m: 0.608 - ETA: 0s - loss: 0.6890 - accuracy: 0.6719 - f1_m: 0.617 - 3s 10ms/sample - loss: 0.6858 - accuracy: 0.6738 - f1_m: 0.6266\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6617 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6804 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 0s - loss: 0.6628 - accuracy: 0.6146 - f1_m: 0.614 - ETA: 0s - loss: 0.6561 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6406 - accuracy: 0.6438 - f1_m: 0.643 - ETA: 0s - loss: 0.6273 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6243 - accuracy: 0.6696 - f1_m: 0.669 - ETA: 0s - loss: 0.6185 - accuracy: 0.6875 - f1_m: 0.687 - 1s 5ms/sample - loss: 0.6034 - accuracy: 0.6989 - f1_m: 0.7029\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4643 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.6086 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6331 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6242 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.6116 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6292 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6350 - accuracy: 0.6830 - f1_m: 0.683 - ETA: 0s - loss: 0.6408 - accuracy: 0.6719 - f1_m: 0.671 - 1s 5ms/sample - loss: 0.6445 - accuracy: 0.6595 - f1_m: 0.6552\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6045 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5736 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5388 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5546 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5701 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5847 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5998 - accuracy: 0.6607 - f1_m: 0.660 - ETA: 0s - loss: 0.6051 - accuracy: 0.6562 - f1_m: 0.656 - 1s 5ms/sample - loss: 0.6088 - accuracy: 0.6523 - f1_m: 0.6510\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6231 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.5551 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5132 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5176 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5233 - accuracy: 0.7937 - f1_m: 0.793 - ETA: 0s - loss: 0.5352 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5361 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.5436 - accuracy: 0.7695 - f1_m: 0.769 - 1s 5ms/sample - loss: 0.5567 - accuracy: 0.7527 - f1_m: 0.7468\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6233 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.5983 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5755 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5244 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5036 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5314 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5868 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5928 - accuracy: 0.6836 - f1_m: 0.683 - 1s 5ms/sample - loss: 0.5989 - accuracy: 0.6703 - f1_m: 0.6656\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5796 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.6082 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6183 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.5987 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 0s - loss: 0.6480 - accuracy: 0.5688 - f1_m: 0.568 - ETA: 0s - loss: 0.6518 - accuracy: 0.5677 - f1_m: 0.567 - ETA: 0s - loss: 0.6649 - accuracy: 0.5580 - f1_m: 0.558 - ETA: 0s - loss: 0.6487 - accuracy: 0.5742 - f1_m: 0.574 - 1s 5ms/sample - loss: 0.6388 - accuracy: 0.5842 - f1_m: 0.5877\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5125 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4605 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5203 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5514 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5431 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5547 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5517 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5590 - accuracy: 0.6875 - f1_m: 0.687 - 1s 5ms/sample - loss: 0.5542 - accuracy: 0.6918 - f1_m: 0.6932\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6082 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.5480 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5212 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5222 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5140 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5074 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5261 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5363 - accuracy: 0.7305 - f1_m: 0.730 - 1s 5ms/sample - loss: 0.5426 - accuracy: 0.7240 - f1_m: 0.7218\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5741 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5518 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5272 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5117 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5416 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5374 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5394 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5467 - accuracy: 0.7383 - f1_m: 0.738 - 1s 5ms/sample - loss: 0.5636 - accuracy: 0.7168 - f1_m: 0.7094\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 18ms/sample - loss: 0.6326 - accuracy: 0.7042 - f1_m: 0.7440\n",
      "f1_m: 74.40%\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_75 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 12s - loss: 0.6938 - accuracy: 0.7812 - f1_m: 0.15 - ETA: 5s - loss: 0.6866 - accuracy: 0.7500 - f1_m: 0.4363 - ETA: 3s - loss: 0.6830 - accuracy: 0.7083 - f1_m: 0.499 - ETA: 2s - loss: 0.6764 - accuracy: 0.7031 - f1_m: 0.546 - ETA: 1s - loss: 0.6917 - accuracy: 0.6750 - f1_m: 0.549 - ETA: 1s - loss: 0.6880 - accuracy: 0.6667 - f1_m: 0.562 - ETA: 0s - loss: 0.6855 - accuracy: 0.6607 - f1_m: 0.571 - ETA: 0s - loss: 0.6797 - accuracy: 0.6758 - f1_m: 0.597 - 3s 10ms/sample - loss: 0.6781 - accuracy: 0.6738 - f1_m: 0.6034\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6596 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6283 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6119 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5885 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5686 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5944 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.6054 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.6006 - accuracy: 0.7031 - f1_m: 0.703 - 1s 5ms/sample - loss: 0.6022 - accuracy: 0.6953 - f1_m: 0.6926\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4903 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5420 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5688 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5785 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.5464 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5448 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5602 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5437 - accuracy: 0.7344 - f1_m: 0.734 - 1s 5ms/sample - loss: 0.5633 - accuracy: 0.7384 - f1_m: 0.7397\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5798 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5460 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5646 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5642 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5470 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5330 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5285 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5227 - accuracy: 0.7422 - f1_m: 0.742 - 1s 5ms/sample - loss: 0.5188 - accuracy: 0.7384 - f1_m: 0.7370\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4741 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5010 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5101 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5163 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5338 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5343 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5760 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5709 - accuracy: 0.7305 - f1_m: 0.730 - 1s 5ms/sample - loss: 0.5559 - accuracy: 0.7419 - f1_m: 0.7459\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5400 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5390 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.4543 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.4871 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5407 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5312 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5485 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5486 - accuracy: 0.7500 - f1_m: 0.750 - 1s 5ms/sample - loss: 0.5447 - accuracy: 0.7527 - f1_m: 0.7536\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6243 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5828 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5234 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5565 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5765 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5644 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5455 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5570 - accuracy: 0.7070 - f1_m: 0.707 - 1s 4ms/sample - loss: 0.5511 - accuracy: 0.7097 - f1_m: 0.7106\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6577 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 0s - loss: 0.5880 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5585 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5245 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5034 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5280 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5582 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5478 - accuracy: 0.7070 - f1_m: 0.707 - 1s 5ms/sample - loss: 0.5326 - accuracy: 0.7204 - f1_m: 0.7251\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 1s - loss: 0.4839 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4910 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5308 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5457 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5613 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5566 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5415 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5526 - accuracy: 0.7266 - f1_m: 0.726 - 1s 5ms/sample - loss: 0.5439 - accuracy: 0.7276 - f1_m: 0.7280\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5290 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5562 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5704 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5640 - accuracy: 0.6797 - f1_m: 0.679 - ETA: 0s - loss: 0.5717 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.5768 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.5648 - accuracy: 0.6830 - f1_m: 0.683 - ETA: 0s - loss: 0.5670 - accuracy: 0.6836 - f1_m: 0.683 - 1s 4ms/sample - loss: 0.5622 - accuracy: 0.6918 - f1_m: 0.6946\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.6207 - accuracy: 0.6901 - f1_m: 0.6964\n",
      "f1_m: 69.64%\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_76 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_76 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 12s - loss: 0.6931 - accuracy: 0.7500 - f1_m: 0.19 - ETA: 5s - loss: 0.6884 - accuracy: 0.6875 - f1_m: 0.4077 - ETA: 3s - loss: 0.6775 - accuracy: 0.7188 - f1_m: 0.532 - ETA: 2s - loss: 0.6958 - accuracy: 0.6641 - f1_m: 0.524 - ETA: 1s - loss: 0.6942 - accuracy: 0.6375 - f1_m: 0.525 - ETA: 1s - loss: 0.6907 - accuracy: 0.6406 - f1_m: 0.547 - ETA: 0s - loss: 0.6895 - accuracy: 0.6339 - f1_m: 0.554 - ETA: 0s - loss: 0.6848 - accuracy: 0.6484 - f1_m: 0.578 - 3s 10ms/sample - loss: 0.6819 - accuracy: 0.6500 - f1_m: 0.5883\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6869 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 0s - loss: 0.6812 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 0s - loss: 0.6650 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6419 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.6422 - accuracy: 0.6687 - f1_m: 0.668 - ETA: 0s - loss: 0.6247 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6235 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.6232 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.6230 - accuracy: 0.6964 - f1_m: 0.6956\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5682 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5476 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5097 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5236 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5530 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5734 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5703 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5756 - accuracy: 0.6992 - f1_m: 0.699 - 1s 5ms/sample - loss: 0.6021 - accuracy: 0.6857 - f1_m: 0.6817\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5964 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5911 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5905 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6072 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5969 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5817 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5969 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5899 - accuracy: 0.7266 - f1_m: 0.726 - 1s 4ms/sample - loss: 0.5915 - accuracy: 0.7143 - f1_m: 0.7106\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4297 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.6770 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6395 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.6175 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.6176 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.6212 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6134 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5955 - accuracy: 0.7344 - f1_m: 0.734 - 1s 5ms/sample - loss: 0.5899 - accuracy: 0.7321 - f1_m: 0.7315\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5630 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6569 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6482 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.6166 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6046 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.6077 - accuracy: 0.6823 - f1_m: 0.682 - ETA: 0s - loss: 0.5945 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.5919 - accuracy: 0.6875 - f1_m: 0.687 - 1s 5ms/sample - loss: 0.5861 - accuracy: 0.6964 - f1_m: 0.6991\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6671 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.5564 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5515 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5385 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6045 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5955 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5954 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.5789 - accuracy: 0.7109 - f1_m: 0.710 - 1s 5ms/sample - loss: 0.5865 - accuracy: 0.7036 - f1_m: 0.7014\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5454 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5398 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5146 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5513 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5670 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5668 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5613 - accuracy: 0.6964 - f1_m: 0.696 - ETA: 0s - loss: 0.5681 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.5639 - accuracy: 0.6964 - f1_m: 0.6956\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5680 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5694 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5419 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5614 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5682 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5585 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5844 - accuracy: 0.6964 - f1_m: 0.696 - ETA: 0s - loss: 0.5876 - accuracy: 0.6914 - f1_m: 0.691 - 1s 4ms/sample - loss: 0.5889 - accuracy: 0.6893 - f1_m: 0.6887\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6422 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5998 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5946 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5699 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6075 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.6015 - accuracy: 0.6823 - f1_m: 0.682 - ETA: 0s - loss: 0.5915 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5895 - accuracy: 0.6953 - f1_m: 0.695 - 1s 4ms/sample - loss: 0.5815 - accuracy: 0.6929 - f1_m: 0.6921\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.4764 - accuracy: 0.6857 - f1_m: 0.6806\n",
      "f1_m: 68.06%\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_77 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_77 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 12s - loss: 0.6931 - accuracy: 0.8750 - f1_m: 0.37 - ETA: 6s - loss: 0.6883 - accuracy: 0.7656 - f1_m: 0.5142 - ETA: 3s - loss: 0.6806 - accuracy: 0.7500 - f1_m: 0.582 - ETA: 2s - loss: 0.6482 - accuracy: 0.7422 - f1_m: 0.616 - ETA: 1s - loss: 0.6376 - accuracy: 0.7250 - f1_m: 0.624 - ETA: 1s - loss: 0.6561 - accuracy: 0.7031 - f1_m: 0.619 - ETA: 0s - loss: 0.6581 - accuracy: 0.7054 - f1_m: 0.633 - ETA: 0s - loss: 0.6589 - accuracy: 0.6992 - f1_m: 0.636 - 3s 10ms/sample - loss: 0.6602 - accuracy: 0.6940 - f1_m: 0.6368\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6615 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6502 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6500 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6492 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6647 - accuracy: 0.6687 - f1_m: 0.668 - ETA: 0s - loss: 0.6600 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6569 - accuracy: 0.6741 - f1_m: 0.674 - ETA: 0s - loss: 0.6545 - accuracy: 0.6641 - f1_m: 0.664 - 1s 5ms/sample - loss: 0.6381 - accuracy: 0.6797 - f1_m: 0.6836\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5131 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5659 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5307 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5566 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5577 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5909 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5928 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5792 - accuracy: 0.7500 - f1_m: 0.750 - 1s 5ms/sample - loss: 0.5912 - accuracy: 0.7473 - f1_m: 0.7467\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5809 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5696 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5474 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5690 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5739 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5936 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5856 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5672 - accuracy: 0.7227 - f1_m: 0.722 - 1s 5ms/sample - loss: 0.5552 - accuracy: 0.7260 - f1_m: 0.7268\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - ETA: 1s - loss: 0.7174 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 0s - loss: 0.6771 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6123 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6217 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6106 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.6285 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6111 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.6056 - accuracy: 0.7070 - f1_m: 0.707 - 1s 4ms/sample - loss: 0.6044 - accuracy: 0.7046 - f1_m: 0.7040\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.7726 - accuracy: 0.4062 - f1_m: 0.406 - ETA: 0s - loss: 0.6654 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 0s - loss: 0.6001 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5399 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5554 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5372 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5383 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5325 - accuracy: 0.7344 - f1_m: 0.734 - 1s 4ms/sample - loss: 0.5607 - accuracy: 0.7153 - f1_m: 0.7106\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5599 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5282 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5463 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5317 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5581 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5466 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5522 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5441 - accuracy: 0.7461 - f1_m: 0.746 - 1s 4ms/sample - loss: 0.5601 - accuracy: 0.7331 - f1_m: 0.7299\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6576 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 0s - loss: 0.6288 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5984 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5661 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5544 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5494 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5458 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5497 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.5543 - accuracy: 0.7011 - f1_m: 0.7015\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5239 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5436 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5036 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.5191 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5426 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5345 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5448 - accuracy: 0.6786 - f1_m: 0.678 - ETA: 0s - loss: 0.5537 - accuracy: 0.6680 - f1_m: 0.668 - 1s 4ms/sample - loss: 0.5468 - accuracy: 0.6797 - f1_m: 0.6826\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5439 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6011 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5763 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5283 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5650 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5778 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5929 - accuracy: 0.6696 - f1_m: 0.669 - ETA: 0s - loss: 0.5845 - accuracy: 0.6836 - f1_m: 0.683 - 1s 4ms/sample - loss: 0.5847 - accuracy: 0.6868 - f1_m: 0.6876\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.4968 - accuracy: 0.7246 - f1_m: 0.7458\n",
      "f1_m: 74.58%\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_78 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 363,906\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 18s - loss: 0.6923 - accuracy: 0.8125 - f1_m: 0.36 - ETA: 8s - loss: 0.6872 - accuracy: 0.7812 - f1_m: 0.5568 - ETA: 5s - loss: 0.6847 - accuracy: 0.7188 - f1_m: 0.569 - ETA: 3s - loss: 0.6660 - accuracy: 0.7266 - f1_m: 0.614 - ETA: 2s - loss: 0.7912 - accuracy: 0.7125 - f1_m: 0.622 - ETA: 1s - loss: 0.8476 - accuracy: 0.6719 - f1_m: 0.597 - ETA: 0s - loss: 0.8095 - accuracy: 0.6607 - f1_m: 0.596 - ETA: 0s - loss: 0.7734 - accuracy: 0.6758 - f1_m: 0.619 - 4s 13ms/sample - loss: 0.7602 - accuracy: 0.6762 - f1_m: 0.6264\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.7166 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6393 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6067 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5847 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6035 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.6027 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.6062 - accuracy: 0.6964 - f1_m: 0.696 - ETA: 0s - loss: 0.5990 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.5968 - accuracy: 0.7011 - f1_m: 0.7015\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5832 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6285 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5908 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5834 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.5957 - accuracy: 0.6438 - f1_m: 0.643 - ETA: 0s - loss: 0.5709 - accuracy: 0.6823 - f1_m: 0.682 - ETA: 0s - loss: 0.5630 - accuracy: 0.6964 - f1_m: 0.696 - ETA: 0s - loss: 0.5706 - accuracy: 0.6992 - f1_m: 0.699 - 1s 4ms/sample - loss: 0.5600 - accuracy: 0.7082 - f1_m: 0.7104\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4443 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4423 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4389 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.4465 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.4772 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.4934 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.4916 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.4944 - accuracy: 0.7578 - f1_m: 0.757 - 1s 4ms/sample - loss: 0.5281 - accuracy: 0.7438 - f1_m: 0.7403\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6949 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 0s - loss: 0.6445 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6043 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5711 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5589 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5448 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5249 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5439 - accuracy: 0.7500 - f1_m: 0.750 - 1s 4ms/sample - loss: 0.5409 - accuracy: 0.7509 - f1_m: 0.7511\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6620 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6780 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 0s - loss: 0.6394 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6126 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5938 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5670 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5591 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5520 - accuracy: 0.7305 - f1_m: 0.730 - 1s 4ms/sample - loss: 0.5569 - accuracy: 0.7295 - f1_m: 0.7293\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5661 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5510 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5472 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5709 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5791 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5798 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5664 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5820 - accuracy: 0.7188 - f1_m: 0.718 - 1s 4ms/sample - loss: 0.5833 - accuracy: 0.7153 - f1_m: 0.7144\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5394 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6405 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5722 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5929 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5847 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5784 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5738 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5651 - accuracy: 0.7188 - f1_m: 0.718 - 1s 4ms/sample - loss: 0.5749 - accuracy: 0.7153 - f1_m: 0.7144\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4862 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5265 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5171 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5531 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5485 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5761 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5597 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5515 - accuracy: 0.7227 - f1_m: 0.722 - 1s 5ms/sample - loss: 0.5474 - accuracy: 0.7224 - f1_m: 0.7224\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6556 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 0s - loss: 0.5871 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5757 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5774 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5822 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5783 - accuracy: 0.6302 - f1_m: 0.630 - ETA: 0s - loss: 0.5814 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.5816 - accuracy: 0.6289 - f1_m: 0.628 - 1s 4ms/sample - loss: 0.5811 - accuracy: 0.6299 - f1_m: 0.6301\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.7179 - accuracy: 0.4638 - f1_m: 0.3896\n",
      "f1_m: 38.96%\n",
      "65.13% (+/- 13.34%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario H\n",
    "do_experiment(X_train_raw, y_train_raw, 'lstm', 100, entity_masking=True, dropout_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_82 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 7s 26ms/sample - loss: 0.6421 - accuracy: 0.6667 - f1_m: 0.6703\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5798 - accuracy: 0.7312 - f1_m: 0.7314\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5314 - accuracy: 0.7491 - f1_m: 0.7502\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5145 - accuracy: 0.7742 - f1_m: 0.7758\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5267 - accuracy: 0.7706 - f1_m: 0.7737\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 5s 19ms/sample - loss: 0.5350 - accuracy: 0.7599 - f1_m: 0.7606\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4938 - accuracy: 0.7957 - f1_m: 0.7939\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.4829 - accuracy: 0.7993 - f1_m: 0.8042\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5098 - accuracy: 0.7599 - f1_m: 0.7619\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.4842 - accuracy: 0.7849 - f1_m: 0.7876\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 12ms/sample - loss: 0.6552 - accuracy: 0.7324 - f1_m: 0.6533\n",
      "f1_m: 65.33%\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_83 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 8s 27ms/sample - loss: 0.6851 - accuracy: 0.6631 - f1_m: 0.6655\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5288 - accuracy: 0.7599 - f1_m: 0.7619\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5042 - accuracy: 0.8065 - f1_m: 0.8043\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 5s 16ms/sample - loss: 0.5548 - accuracy: 0.7563 - f1_m: 0.7585\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.4812 - accuracy: 0.7742 - f1_m: 0.7745\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.4706 - accuracy: 0.8172 - f1_m: 0.8188\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.4996 - accuracy: 0.8029 - f1_m: 0.8050\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.4833 - accuracy: 0.7778 - f1_m: 0.7806\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 5s 16ms/sample - loss: 0.4853 - accuracy: 0.7957 - f1_m: 0.7953\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 4s 16ms/sample - loss: 0.4443 - accuracy: 0.8315 - f1_m: 0.8341\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 12ms/sample - loss: 0.6732 - accuracy: 0.7324 - f1_m: 0.7277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 72.77%\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_84 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 9s 32ms/sample - loss: 0.7502 - accuracy: 0.6321 - f1_m: 0.6319\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 5s 16ms/sample - loss: 0.5885 - accuracy: 0.6964 - f1_m: 0.6991\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 4s 16ms/sample - loss: 0.5769 - accuracy: 0.7214 - f1_m: 0.7234\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 5s 16ms/sample - loss: 0.5420 - accuracy: 0.7500 - f1_m: 0.7454\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 5s 17ms/sample - loss: 0.5213 - accuracy: 0.7714 - f1_m: 0.7720\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 5s 16ms/sample - loss: 0.5402 - accuracy: 0.7321 - f1_m: 0.7292\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 5s 17ms/sample - loss: 0.5433 - accuracy: 0.7750 - f1_m: 0.7743\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 5s 16ms/sample - loss: 0.5383 - accuracy: 0.7679 - f1_m: 0.7685\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 5s 16ms/sample - loss: 0.4984 - accuracy: 0.7964 - f1_m: 0.7998\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 5s 17ms/sample - loss: 0.5795 - accuracy: 0.7071 - f1_m: 0.7060\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.5314 - accuracy: 0.7143 - f1_m: 0.7014\n",
      "f1_m: 70.14%\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_85 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 8s 28ms/sample - loss: 0.6945 - accuracy: 0.6797 - f1_m: 0.6787\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5432 - accuracy: 0.7758 - f1_m: 0.7764\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5142 - accuracy: 0.7758 - f1_m: 0.7774\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5128 - accuracy: 0.7651 - f1_m: 0.7621\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5039 - accuracy: 0.7758 - f1_m: 0.7793\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5180 - accuracy: 0.7616 - f1_m: 0.7615\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5115 - accuracy: 0.7722 - f1_m: 0.7710\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5061 - accuracy: 0.7794 - f1_m: 0.7808\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.4839 - accuracy: 0.7829 - f1_m: 0.7804\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.5197 - accuracy: 0.7829 - f1_m: 0.7824\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.3860 - accuracy: 0.8116 - f1_m: 0.8646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 86.46%\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_86 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 7s 26ms/sample - loss: 0.7329 - accuracy: 0.6121 - f1_m: 0.6137\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5900 - accuracy: 0.7367 - f1_m: 0.7362\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5134 - accuracy: 0.7473 - f1_m: 0.7447\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.5241 - accuracy: 0.7794 - f1_m: 0.7799\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 5s 18ms/sample - loss: 0.5193 - accuracy: 0.7544 - f1_m: 0.7565\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.5350 - accuracy: 0.7473 - f1_m: 0.7496\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 4s 16ms/sample - loss: 0.5566 - accuracy: 0.7473 - f1_m: 0.7496\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.4975 - accuracy: 0.7865 - f1_m: 0.7868\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5131 - accuracy: 0.7687 - f1_m: 0.7665\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.5426 - accuracy: 0.7473 - f1_m: 0.7476\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.3823 - accuracy: 0.7826 - f1_m: 0.8438\n",
      "f1_m: 84.38%\n",
      "75.81% (+/- 8.22%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario I\n",
    "do_experiment(X_train_raw, y_train_raw, 'bidirectional', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_87 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 7s 27ms/sample - loss: 0.6616 - accuracy: 0.6631 - f1_m: 0.6627\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.6253 - accuracy: 0.6667 - f1_m: 0.6662\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 5s 16ms/sample - loss: 0.5233 - accuracy: 0.7706 - f1_m: 0.7669\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5510 - accuracy: 0.7455 - f1_m: 0.7440\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 5s 16ms/sample - loss: 0.5424 - accuracy: 0.7670 - f1_m: 0.7702\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 5s 16ms/sample - loss: 0.5274 - accuracy: 0.7491 - f1_m: 0.7488\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 5s 16ms/sample - loss: 0.5556 - accuracy: 0.7240 - f1_m: 0.7231\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5244 - accuracy: 0.7491 - f1_m: 0.7529\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5224 - accuracy: 0.7670 - f1_m: 0.7648\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 4s 16ms/sample - loss: 0.5188 - accuracy: 0.7563 - f1_m: 0.7557\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 35ms/sample - loss: 0.5036 - accuracy: 0.8310 - f1_m: 0.8006\n",
      "f1_m: 80.06%\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_88 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 7s 26ms/sample - loss: 0.7021 - accuracy: 0.6882 - f1_m: 0.6884\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 4s 16ms/sample - loss: 0.5394 - accuracy: 0.7563 - f1_m: 0.7571\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 4s 16ms/sample - loss: 0.5302 - accuracy: 0.7670 - f1_m: 0.7689\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5074 - accuracy: 0.7885 - f1_m: 0.7911\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 5s 17ms/sample - loss: 0.5361 - accuracy: 0.7563 - f1_m: 0.7530\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 4s 16ms/sample - loss: 0.5056 - accuracy: 0.7742 - f1_m: 0.7758\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 5s 16ms/sample - loss: 0.4978 - accuracy: 0.8029 - f1_m: 0.7995\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 4s 16ms/sample - loss: 0.4728 - accuracy: 0.8065 - f1_m: 0.8084\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 4s 16ms/sample - loss: 0.4995 - accuracy: 0.7814 - f1_m: 0.7773\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 5s 16ms/sample - loss: 0.4952 - accuracy: 0.7885 - f1_m: 0.7829\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 12ms/sample - loss: 0.6602 - accuracy: 0.6620 - f1_m: 0.6756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 67.56%\n",
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_89 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 7s 26ms/sample - loss: 0.6587 - accuracy: 0.6679 - f1_m: 0.6701\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 4s 16ms/sample - loss: 0.5934 - accuracy: 0.7429 - f1_m: 0.7477\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 5s 16ms/sample - loss: 0.5351 - accuracy: 0.7786 - f1_m: 0.7789\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 5s 16ms/sample - loss: 0.5641 - accuracy: 0.7750 - f1_m: 0.7755\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 4s 16ms/sample - loss: 0.5556 - accuracy: 0.8107 - f1_m: 0.8125\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 5s 17ms/sample - loss: 0.5368 - accuracy: 0.7714 - f1_m: 0.7708\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 5s 17ms/sample - loss: 0.5313 - accuracy: 0.7821 - f1_m: 0.7812\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 4s 16ms/sample - loss: 0.5466 - accuracy: 0.7500 - f1_m: 0.7500\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 4s 15ms/sample - loss: 0.5095 - accuracy: 0.7679 - f1_m: 0.7697\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 5s 16ms/sample - loss: 0.5239 - accuracy: 0.7607 - f1_m: 0.7593\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.3999 - accuracy: 0.8143 - f1_m: 0.8646\n",
      "f1_m: 86.46%\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_90 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 7s 26ms/sample - loss: 0.6857 - accuracy: 0.7046 - f1_m: 0.7089\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 4s 16ms/sample - loss: 0.5875 - accuracy: 0.7011 - f1_m: 0.7006\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 4s 16ms/sample - loss: 0.6192 - accuracy: 0.7367 - f1_m: 0.7411\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 4s 16ms/sample - loss: 0.5284 - accuracy: 0.7616 - f1_m: 0.7606\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5582 - accuracy: 0.7865 - f1_m: 0.7868\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 4s 16ms/sample - loss: 0.5336 - accuracy: 0.7580 - f1_m: 0.7561\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 4s 16ms/sample - loss: 0.5205 - accuracy: 0.7829 - f1_m: 0.7824\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5210 - accuracy: 0.7758 - f1_m: 0.7754\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.5130 - accuracy: 0.7758 - f1_m: 0.7764\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5001 - accuracy: 0.7687 - f1_m: 0.7714\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.3612 - accuracy: 0.7971 - f1_m: 0.8542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 85.42%\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_91 (Embedding)     (None, 100, 50)           124650    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 308,460\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 124,650\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 9s 33ms/sample - loss: 0.6793 - accuracy: 0.6584 - f1_m: 0.6589\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.5440 - accuracy: 0.7473 - f1_m: 0.7457\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.5704 - accuracy: 0.7224 - f1_m: 0.7224\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5356 - accuracy: 0.7153 - f1_m: 0.7125\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5319 - accuracy: 0.7473 - f1_m: 0.7447\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.4975 - accuracy: 0.7829 - f1_m: 0.7843\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 4s 15ms/sample - loss: 0.5363 - accuracy: 0.7509 - f1_m: 0.7521\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 5s 16ms/sample - loss: 0.5090 - accuracy: 0.7616 - f1_m: 0.7635\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.4915 - accuracy: 0.7758 - f1_m: 0.7754\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 17ms/sample - loss: 0.5192 - accuracy: 0.7651 - f1_m: 0.7621\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 15ms/sample - loss: 0.4751 - accuracy: 0.7681 - f1_m: 0.8333\n",
      "f1_m: 83.33%\n",
      "80.57% (+/- 6.86%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario J\n",
    "do_experiment(X_train_raw, y_train_raw, 'bidirectional', 50, dropout_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_92 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 8s 29ms/sample - loss: 0.7343 - accuracy: 0.6774 - f1_m: 0.6834\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5713 - accuracy: 0.7599 - f1_m: 0.7565\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5096 - accuracy: 0.7491 - f1_m: 0.7515\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5390 - accuracy: 0.7455 - f1_m: 0.7467\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5128 - accuracy: 0.7742 - f1_m: 0.7758\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5421 - accuracy: 0.7419 - f1_m: 0.7418\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5284 - accuracy: 0.7634 - f1_m: 0.7668\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5309 - accuracy: 0.7168 - f1_m: 0.7121\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5017 - accuracy: 0.7993 - f1_m: 0.8042\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5614 - accuracy: 0.7563 - f1_m: 0.7612\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 12ms/sample - loss: 0.5918 - accuracy: 0.8310 - f1_m: 0.8006\n",
      "f1_m: 80.06%\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_93 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 8s 27ms/sample - loss: 0.6677 - accuracy: 0.6165 - f1_m: 0.6203\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5595 - accuracy: 0.7276 - f1_m: 0.7239\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5057 - accuracy: 0.7706 - f1_m: 0.7723\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4835 - accuracy: 0.8065 - f1_m: 0.8057\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5331 - accuracy: 0.7849 - f1_m: 0.7822\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4593 - accuracy: 0.7814 - f1_m: 0.7855\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4552 - accuracy: 0.8100 - f1_m: 0.8078\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4822 - accuracy: 0.7814 - f1_m: 0.7800\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4436 - accuracy: 0.8100 - f1_m: 0.8119\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4612 - accuracy: 0.8315 - f1_m: 0.8300\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 12ms/sample - loss: 0.6203 - accuracy: 0.6479 - f1_m: 0.6652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 66.52%\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_94 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 8s 27ms/sample - loss: 0.6860 - accuracy: 0.6464 - f1_m: 0.6481\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.6468 - accuracy: 0.6929 - f1_m: 0.6910\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.6018 - accuracy: 0.7143 - f1_m: 0.7153\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5437 - accuracy: 0.7357 - f1_m: 0.7350\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5112 - accuracy: 0.7786 - f1_m: 0.7766\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5256 - accuracy: 0.7750 - f1_m: 0.7743\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5390 - accuracy: 0.7500 - f1_m: 0.7454\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5336 - accuracy: 0.7500 - f1_m: 0.7512\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5048 - accuracy: 0.7786 - f1_m: 0.7801\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 6s 20ms/sample - loss: 0.5103 - accuracy: 0.7500 - f1_m: 0.7500\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.3617 - accuracy: 0.8143 - f1_m: 0.8646\n",
      "f1_m: 86.46%\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_95 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 10s 35ms/sample - loss: 0.7747 - accuracy: 0.6584 - f1_m: 0.6579\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.6383 - accuracy: 0.6619 - f1_m: 0.6653\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5474 - accuracy: 0.7189 - f1_m: 0.7218\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5097 - accuracy: 0.7722 - f1_m: 0.7758\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5569 - accuracy: 0.7865 - f1_m: 0.7858\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5118 - accuracy: 0.7544 - f1_m: 0.7546\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5024 - accuracy: 0.7794 - f1_m: 0.7799\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.4864 - accuracy: 0.7972 - f1_m: 0.7953\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5685 - accuracy: 0.7865 - f1_m: 0.7849\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5077 - accuracy: 0.7722 - f1_m: 0.7729\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.3155 - accuracy: 0.8551 - f1_m: 0.8958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 89.58%\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_96 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 8s 29ms/sample - loss: 0.7151 - accuracy: 0.6548 - f1_m: 0.6544\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5778 - accuracy: 0.7651 - f1_m: 0.7640\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5733 - accuracy: 0.7367 - f1_m: 0.7314\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5324 - accuracy: 0.7580 - f1_m: 0.7581\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 5s 18ms/sample - loss: 0.5470 - accuracy: 0.7509 - f1_m: 0.7540\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5823 - accuracy: 0.7260 - f1_m: 0.7287\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.4951 - accuracy: 0.7900 - f1_m: 0.7883\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 6s 20ms/sample - loss: 0.5170 - accuracy: 0.7758 - f1_m: 0.7735\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 18ms/sample - loss: 0.4938 - accuracy: 0.7722 - f1_m: 0.7729\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 18ms/sample - loss: 0.4953 - accuracy: 0.8114 - f1_m: 0.8121\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.4476 - accuracy: 0.7681 - f1_m: 0.8333\n",
      "f1_m: 83.33%\n",
      "81.19% (+/- 7.99%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario K\n",
    "do_experiment(X_train_raw, y_train_raw, 'bidirectional', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_97 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 8s 28ms/sample - loss: 0.6711 - accuracy: 0.6272 - f1_m: 0.6321\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5869 - accuracy: 0.7312 - f1_m: 0.7301\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.6210 - accuracy: 0.7312 - f1_m: 0.7341\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5145 - accuracy: 0.7706 - f1_m: 0.7737\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 5s 19ms/sample - loss: 0.5261 - accuracy: 0.7563 - f1_m: 0.7544\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5231 - accuracy: 0.7814 - f1_m: 0.7787\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4923 - accuracy: 0.7885 - f1_m: 0.7883\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5120 - accuracy: 0.7742 - f1_m: 0.7690\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4970 - accuracy: 0.7384 - f1_m: 0.7384\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5142 - accuracy: 0.7814 - f1_m: 0.7841\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.4546 - accuracy: 0.8310 - f1_m: 0.8378\n",
      "f1_m: 83.78%\n",
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_98 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 8s 29ms/sample - loss: 0.6476 - accuracy: 0.6738 - f1_m: 0.6759\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5250 - accuracy: 0.7706 - f1_m: 0.7723\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5035 - accuracy: 0.7742 - f1_m: 0.7745\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.5018 - accuracy: 0.7957 - f1_m: 0.7926\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4642 - accuracy: 0.8065 - f1_m: 0.8016\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4912 - accuracy: 0.7921 - f1_m: 0.7932\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4811 - accuracy: 0.7993 - f1_m: 0.8015\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4831 - accuracy: 0.8136 - f1_m: 0.8140\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4621 - accuracy: 0.8029 - f1_m: 0.8063\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 5s 18ms/sample - loss: 0.4556 - accuracy: 0.8172 - f1_m: 0.8188\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 12ms/sample - loss: 0.6437 - accuracy: 0.7042 - f1_m: 0.6696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 66.96%\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_99 (Embedding)     (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 9s 33ms/sample - loss: 0.6873 - accuracy: 0.6893 - f1_m: 0.6887\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5755 - accuracy: 0.7179 - f1_m: 0.7187\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5880 - accuracy: 0.7071 - f1_m: 0.7106\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 6s 20ms/sample - loss: 0.5216 - accuracy: 0.7643 - f1_m: 0.7639\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5472 - accuracy: 0.7464 - f1_m: 0.7488\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5690 - accuracy: 0.7107 - f1_m: 0.7095\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5361 - accuracy: 0.7679 - f1_m: 0.7685\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5103 - accuracy: 0.7643 - f1_m: 0.7685\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5111 - accuracy: 0.7893 - f1_m: 0.7917\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 5s 18ms/sample - loss: 0.5129 - accuracy: 0.7857 - f1_m: 0.7847\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.3623 - accuracy: 0.8143 - f1_m: 0.8194\n",
      "f1_m: 81.94%\n",
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_100 (Embedding)    (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 8s 28ms/sample - loss: 0.6716 - accuracy: 0.6797 - f1_m: 0.6778\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.6047 - accuracy: 0.7509 - f1_m: 0.7492\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5301 - accuracy: 0.7509 - f1_m: 0.7550\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.4928 - accuracy: 0.7936 - f1_m: 0.7967\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.4469 - accuracy: 0.7829 - f1_m: 0.7872\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5662 - accuracy: 0.7473 - f1_m: 0.7476\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5173 - accuracy: 0.7722 - f1_m: 0.7749\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5073 - accuracy: 0.7900 - f1_m: 0.7854\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5465 - accuracy: 0.7509 - f1_m: 0.7482\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5194 - accuracy: 0.7580 - f1_m: 0.7581\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 13ms/sample - loss: 0.3710 - accuracy: 0.7971 - f1_m: 0.8542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_m: 85.42%\n",
      "Model: \"sequential_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_101 (Embedding)    (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 8s 29ms/sample - loss: 0.6823 - accuracy: 0.6762 - f1_m: 0.6782\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.6000 - accuracy: 0.7260 - f1_m: 0.7210\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 5s 18ms/sample - loss: 0.5510 - accuracy: 0.7794 - f1_m: 0.7779\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5694 - accuracy: 0.7544 - f1_m: 0.7556\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5004 - accuracy: 0.7936 - f1_m: 0.7918\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5571 - accuracy: 0.7900 - f1_m: 0.7883\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5238 - accuracy: 0.7758 - f1_m: 0.7735\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5000 - accuracy: 0.7829 - f1_m: 0.7814\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5198 - accuracy: 0.7758 - f1_m: 0.7774\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 19ms/sample - loss: 0.5115 - accuracy: 0.7722 - f1_m: 0.7719\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 14ms/sample - loss: 0.3040 - accuracy: 0.8116 - f1_m: 0.8646\n",
      "f1_m: 86.46%\n",
      "80.91% (+/- 7.14%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario L\n",
    "do_experiment(X_train_raw, y_train_raw, 'bidirectional', 100, dropout_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_79 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 21s - loss: 0.6941 - accuracy: 0.4062 - f1_m: 0.40 - ETA: 10s - loss: 0.6858 - accuracy: 0.5312 - f1_m: 0.53 - ETA: 6s - loss: 0.6904 - accuracy: 0.5208 - f1_m: 0.5208 - ETA: 4s - loss: 0.6830 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 2s - loss: 0.6899 - accuracy: 0.6062 - f1_m: 0.606 - ETA: 1s - loss: 0.6595 - accuracy: 0.6302 - f1_m: 0.630 - ETA: 1s - loss: 0.6745 - accuracy: 0.6339 - f1_m: 0.633 - ETA: 0s - loss: 0.6716 - accuracy: 0.6328 - f1_m: 0.632 - 5s 16ms/sample - loss: 0.6588 - accuracy: 0.6487 - f1_m: 0.6543\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5548 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5533 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5361 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5025 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5527 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5676 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5668 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5581 - accuracy: 0.7461 - f1_m: 0.746 - 2s 7ms/sample - loss: 0.5643 - accuracy: 0.7455 - f1_m: 0.7453\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6454 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6542 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5964 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5670 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5685 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5651 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5379 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5245 - accuracy: 0.7617 - f1_m: 0.761 - 2s 7ms/sample - loss: 0.5321 - accuracy: 0.7563 - f1_m: 0.7544\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5465 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5413 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5459 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5080 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5391 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5237 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5403 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5380 - accuracy: 0.7383 - f1_m: 0.738 - 2s 7ms/sample - loss: 0.5300 - accuracy: 0.7491 - f1_m: 0.7529\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5191 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.4596 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4501 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4626 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.4611 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5006 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5092 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5049 - accuracy: 0.7500 - f1_m: 0.750 - 2s 7ms/sample - loss: 0.5130 - accuracy: 0.7491 - f1_m: 0.7488\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5182 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5131 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4925 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.5021 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4846 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.4748 - accuracy: 0.8073 - f1_m: 0.807 - ETA: 0s - loss: 0.4782 - accuracy: 0.8080 - f1_m: 0.808 - ETA: 0s - loss: 0.5145 - accuracy: 0.7773 - f1_m: 0.777 - 2s 7ms/sample - loss: 0.5184 - accuracy: 0.7670 - f1_m: 0.7634\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5393 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.5258 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 1s - loss: 0.5358 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5224 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5211 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.5275 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5291 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5469 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5353 - accuracy: 0.7706 - f1_m: 0.7751\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3999 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.3800 - accuracy: 0.8594 - f1_m: 0.859 - ETA: 1s - loss: 0.4623 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.4598 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 0s - loss: 0.5106 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5323 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5059 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.5023 - accuracy: 0.7852 - f1_m: 0.785 - 2s 7ms/sample - loss: 0.5233 - accuracy: 0.7706 - f1_m: 0.7655\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4797 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.5223 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4888 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4903 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5060 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5196 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5283 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5295 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5235 - accuracy: 0.7634 - f1_m: 0.7654\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5123 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4977 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4984 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.4774 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.4902 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.5110 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5014 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.5014 - accuracy: 0.7734 - f1_m: 0.773 - 2s 7ms/sample - loss: 0.4985 - accuracy: 0.7778 - f1_m: 0.7793\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.4425 - accuracy: 0.8310 - f1_m: 0.8378\n",
      "f1_m: 83.78%\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_80 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 21s - loss: 0.6954 - accuracy: 0.2812 - f1_m: 0.28 - ETA: 10s - loss: 0.6923 - accuracy: 0.4219 - f1_m: 0.42 - ETA: 6s - loss: 0.6867 - accuracy: 0.5000 - f1_m: 0.5000 - ETA: 4s - loss: 0.6776 - accuracy: 0.5469 - f1_m: 0.546 - ETA: 2s - loss: 0.6327 - accuracy: 0.6000 - f1_m: 0.600 - ETA: 1s - loss: 0.5809 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 1s - loss: 0.6898 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6797 - accuracy: 0.6602 - f1_m: 0.660 - 5s 16ms/sample - loss: 0.6762 - accuracy: 0.6559 - f1_m: 0.6544\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5152 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5639 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5323 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 1s - loss: 0.5542 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5417 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5435 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5490 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5599 - accuracy: 0.7227 - f1_m: 0.722 - 2s 7ms/sample - loss: 0.5550 - accuracy: 0.7204 - f1_m: 0.7197\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6411 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5738 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5587 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5342 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.6065 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5895 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5872 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5576 - accuracy: 0.7773 - f1_m: 0.777 - 2s 7ms/sample - loss: 0.5372 - accuracy: 0.7921 - f1_m: 0.7973\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3800 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.3286 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.4488 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 0s - loss: 0.4957 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5155 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5056 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5084 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.5052 - accuracy: 0.7852 - f1_m: 0.785 - 2s 7ms/sample - loss: 0.4963 - accuracy: 0.7921 - f1_m: 0.7945\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5799 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5629 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5527 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5181 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5455 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5266 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5214 - accuracy: 0.7857 - f1_m: 0.785 - ETA: 0s - loss: 0.5181 - accuracy: 0.7812 - f1_m: 0.781 - 2s 7ms/sample - loss: 0.5193 - accuracy: 0.7814 - f1_m: 0.7814\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5103 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.4608 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5436 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5152 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5176 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5234 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5154 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5181 - accuracy: 0.7773 - f1_m: 0.777 - 2s 7ms/sample - loss: 0.5052 - accuracy: 0.7849 - f1_m: 0.7876\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5438 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5636 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5242 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.4726 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.4744 - accuracy: 0.8000 - f1_m: 0.800 - ETA: 0s - loss: 0.4563 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4865 - accuracy: 0.8036 - f1_m: 0.803 - ETA: 0s - loss: 0.4862 - accuracy: 0.8008 - f1_m: 0.800 - 2s 7ms/sample - loss: 0.4848 - accuracy: 0.8029 - f1_m: 0.8036\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4099 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.4137 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4621 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4226 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.4433 - accuracy: 0.8250 - f1_m: 0.825 - ETA: 0s - loss: 0.4392 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 0s - loss: 0.4824 - accuracy: 0.8036 - f1_m: 0.803 - ETA: 0s - loss: 0.4833 - accuracy: 0.8086 - f1_m: 0.808 - 2s 7ms/sample - loss: 0.4756 - accuracy: 0.8100 - f1_m: 0.8105\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5700 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4954 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4562 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4833 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4830 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4566 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4801 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.4734 - accuracy: 0.8047 - f1_m: 0.804 - 2s 7ms/sample - loss: 0.4686 - accuracy: 0.8100 - f1_m: 0.8119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4941 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4636 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5121 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.4923 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4803 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.4479 - accuracy: 0.8073 - f1_m: 0.807 - ETA: 0s - loss: 0.4432 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4544 - accuracy: 0.8086 - f1_m: 0.808 - 2s 7ms/sample - loss: 0.4695 - accuracy: 0.7993 - f1_m: 0.7960\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.6498 - accuracy: 0.7324 - f1_m: 0.7277\n",
      "f1_m: 72.77%\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_81 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 22s - loss: 0.6934 - accuracy: 0.5312 - f1_m: 0.53 - ETA: 10s - loss: 0.6763 - accuracy: 0.6406 - f1_m: 0.64 - ETA: 6s - loss: 0.6709 - accuracy: 0.6562 - f1_m: 0.6562 - ETA: 4s - loss: 0.6669 - accuracy: 0.6484 - f1_m: 0.648 - ETA: 2s - loss: 0.6654 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6656 - accuracy: 0.6510 - f1_m: 0.651 - ETA: 1s - loss: 0.6686 - accuracy: 0.6384 - f1_m: 0.638 - ETA: 0s - loss: 0.6597 - accuracy: 0.6641 - f1_m: 0.664 - 5s 16ms/sample - loss: 0.6625 - accuracy: 0.6714 - f1_m: 0.6736\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6505 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6076 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6026 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6056 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5960 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5827 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5828 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5939 - accuracy: 0.7188 - f1_m: 0.718 - 2s 7ms/sample - loss: 0.5941 - accuracy: 0.7107 - f1_m: 0.7083\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4859 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.4579 - accuracy: 0.8594 - f1_m: 0.859 - ETA: 1s - loss: 0.4650 - accuracy: 0.8646 - f1_m: 0.864 - ETA: 0s - loss: 0.4673 - accuracy: 0.8516 - f1_m: 0.851 - ETA: 0s - loss: 0.5487 - accuracy: 0.8000 - f1_m: 0.800 - ETA: 0s - loss: 0.5350 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5400 - accuracy: 0.7991 - f1_m: 0.799 - ETA: 0s - loss: 0.5426 - accuracy: 0.7891 - f1_m: 0.789 - 2s 7ms/sample - loss: 0.5664 - accuracy: 0.7643 - f1_m: 0.7569\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5667 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5896 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5644 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5731 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5718 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5552 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5524 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5384 - accuracy: 0.7539 - f1_m: 0.753 - 2s 7ms/sample - loss: 0.5312 - accuracy: 0.7571 - f1_m: 0.7581\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5396 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.6160 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5762 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5113 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.4734 - accuracy: 0.8188 - f1_m: 0.818 - ETA: 0s - loss: 0.4933 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5133 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.5143 - accuracy: 0.7891 - f1_m: 0.789 - 2s 7ms/sample - loss: 0.5146 - accuracy: 0.7821 - f1_m: 0.7801\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5986 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5562 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5622 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5390 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5196 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5015 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5041 - accuracy: 0.7723 - f1_m: 0.772 - ETA: 0s - loss: 0.4931 - accuracy: 0.7812 - f1_m: 0.781 - 2s 7ms/sample - loss: 0.4911 - accuracy: 0.7821 - f1_m: 0.7824\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5222 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.6062 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.5828 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5359 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5323 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5366 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5386 - accuracy: 0.7857 - f1_m: 0.785 - ETA: 0s - loss: 0.5380 - accuracy: 0.7812 - f1_m: 0.781 - 2s 7ms/sample - loss: 0.5307 - accuracy: 0.7786 - f1_m: 0.7778\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4510 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4002 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4577 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4537 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4618 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4641 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4792 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.4994 - accuracy: 0.7891 - f1_m: 0.789 - 2s 7ms/sample - loss: 0.4949 - accuracy: 0.7929 - f1_m: 0.7940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4156 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4410 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5095 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5017 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.4970 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.4953 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5052 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.5080 - accuracy: 0.7734 - f1_m: 0.773 - 2s 7ms/sample - loss: 0.5060 - accuracy: 0.7714 - f1_m: 0.7708\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4961 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5657 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5325 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.4951 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5634 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5427 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5463 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5454 - accuracy: 0.7617 - f1_m: 0.761 - 2s 7ms/sample - loss: 0.5402 - accuracy: 0.7643 - f1_m: 0.7650\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.4829 - accuracy: 0.7143 - f1_m: 0.7014\n",
      "f1_m: 70.14%\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_82 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 22s - loss: 0.6936 - accuracy: 0.5312 - f1_m: 0.53 - ETA: 10s - loss: 0.6856 - accuracy: 0.5938 - f1_m: 0.59 - ETA: 6s - loss: 0.6764 - accuracy: 0.6042 - f1_m: 0.6042 - ETA: 4s - loss: 0.6536 - accuracy: 0.6172 - f1_m: 0.617 - ETA: 2s - loss: 0.8492 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.8125 - accuracy: 0.6302 - f1_m: 0.630 - ETA: 1s - loss: 0.7782 - accuracy: 0.6518 - f1_m: 0.651 - ETA: 0s - loss: 0.7541 - accuracy: 0.6719 - f1_m: 0.671 - 5s 16ms/sample - loss: 0.7366 - accuracy: 0.6833 - f1_m: 0.6861\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6672 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5948 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5500 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5428 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5459 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.5646 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5544 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5527 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5627 - accuracy: 0.7473 - f1_m: 0.7447\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5568 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.5606 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5199 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 0s - loss: 0.5044 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.5079 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5347 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5327 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5205 - accuracy: 0.7891 - f1_m: 0.789 - 2s 7ms/sample - loss: 0.5241 - accuracy: 0.7829 - f1_m: 0.7814\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6052 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.6186 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.6020 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 1s - loss: 0.6121 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5990 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5919 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5749 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5630 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5503 - accuracy: 0.7687 - f1_m: 0.7714\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5273 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.4920 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4714 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4720 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4717 - accuracy: 0.8188 - f1_m: 0.818 - ETA: 0s - loss: 0.4686 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4670 - accuracy: 0.8080 - f1_m: 0.808 - ETA: 0s - loss: 0.4874 - accuracy: 0.8008 - f1_m: 0.800 - 2s 7ms/sample - loss: 0.5032 - accuracy: 0.7900 - f1_m: 0.7874\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5719 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5807 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5582 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 1s - loss: 0.5421 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5522 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5320 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5354 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5321 - accuracy: 0.7539 - f1_m: 0.753 - 2s 7ms/sample - loss: 0.5122 - accuracy: 0.7651 - f1_m: 0.7679\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4170 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.4419 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4649 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4951 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4832 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.5085 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5328 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5229 - accuracy: 0.7891 - f1_m: 0.789 - 2s 7ms/sample - loss: 0.5104 - accuracy: 0.7936 - f1_m: 0.7947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6409 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5933 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5714 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.5420 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5477 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5532 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5568 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5497 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5371 - accuracy: 0.7651 - f1_m: 0.7669\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.3944 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4549 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4522 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.4235 - accuracy: 0.8359 - f1_m: 0.835 - ETA: 0s - loss: 0.4803 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5326 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5455 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5329 - accuracy: 0.7773 - f1_m: 0.777 - 2s 7ms/sample - loss: 0.5377 - accuracy: 0.7794 - f1_m: 0.7799\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4898 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5462 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5041 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5071 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4857 - accuracy: 0.7937 - f1_m: 0.793 - ETA: 0s - loss: 0.5089 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5262 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5155 - accuracy: 0.7539 - f1_m: 0.753 - 2s 7ms/sample - loss: 0.5093 - accuracy: 0.7580 - f1_m: 0.7590\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.3592 - accuracy: 0.8116 - f1_m: 0.8646\n",
      "f1_m: 86.46%\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_83 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 29s - loss: 0.6922 - accuracy: 0.6250 - f1_m: 0.62 - ETA: 13s - loss: 0.6815 - accuracy: 0.6562 - f1_m: 0.65 - ETA: 8s - loss: 0.6817 - accuracy: 0.6354 - f1_m: 0.6354 - ETA: 5s - loss: 0.6714 - accuracy: 0.6484 - f1_m: 0.648 - ETA: 3s - loss: 0.7097 - accuracy: 0.6438 - f1_m: 0.643 - ETA: 2s - loss: 0.6970 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 1s - loss: 0.6922 - accuracy: 0.6607 - f1_m: 0.660 - ETA: 0s - loss: 0.6832 - accuracy: 0.6680 - f1_m: 0.668 - 5s 20ms/sample - loss: 0.6693 - accuracy: 0.6797 - f1_m: 0.6826\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6620 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6099 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5628 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.5870 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5938 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.6157 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6124 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5985 - accuracy: 0.7539 - f1_m: 0.753 - 2s 7ms/sample - loss: 0.6076 - accuracy: 0.7438 - f1_m: 0.7412\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5820 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5684 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5698 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5784 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5691 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5608 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5758 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5577 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5657 - accuracy: 0.7438 - f1_m: 0.7403\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5218 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4727 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 1s - loss: 0.4955 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5467 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5449 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5422 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5432 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5349 - accuracy: 0.7539 - f1_m: 0.753 - 2s 7ms/sample - loss: 0.5267 - accuracy: 0.7580 - f1_m: 0.7590\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4332 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4376 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4687 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5115 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5026 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5235 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5206 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5099 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5193 - accuracy: 0.7580 - f1_m: 0.7581\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.3830 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4616 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.4813 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.4997 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5216 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5225 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5130 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5255 - accuracy: 0.7422 - f1_m: 0.742 - 2s 7ms/sample - loss: 0.5263 - accuracy: 0.7473 - f1_m: 0.7486\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4614 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.3900 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4155 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.4319 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.4457 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4570 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4998 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5043 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5099 - accuracy: 0.7509 - f1_m: 0.7492\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.3308 - accuracy: 0.9062 - f1_m: 0.906 - ETA: 1s - loss: 0.3449 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.4435 - accuracy: 0.8333 - f1_m: 0.833 - ETA: 1s - loss: 0.4325 - accuracy: 0.8359 - f1_m: 0.835 - ETA: 0s - loss: 0.4678 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4995 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5130 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5245 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5298 - accuracy: 0.7544 - f1_m: 0.7536\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5365 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5382 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5259 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5095 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5020 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5189 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5310 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5270 - accuracy: 0.7383 - f1_m: 0.738 - 2s 7ms/sample - loss: 0.5210 - accuracy: 0.7473 - f1_m: 0.7496\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4528 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4609 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 1s - loss: 0.4995 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5221 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4996 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.5041 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5069 - accuracy: 0.7857 - f1_m: 0.785 - ETA: 0s - loss: 0.5134 - accuracy: 0.7617 - f1_m: 0.761 - 2s 7ms/sample - loss: 0.5011 - accuracy: 0.7687 - f1_m: 0.7704\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.3837 - accuracy: 0.7681 - f1_m: 0.8333\n",
      "f1_m: 83.33%\n",
      "79.30% (+/- 6.54%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario M\n",
    "do_experiment(X_train_raw, y_train_raw, 'bidirectional', 50, entity_masking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_84 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 23s - loss: 0.6930 - accuracy: 0.4688 - f1_m: 0.46 - ETA: 10s - loss: 0.6970 - accuracy: 0.3750 - f1_m: 0.37 - ETA: 6s - loss: 0.6927 - accuracy: 0.4479 - f1_m: 0.4479 - ETA: 4s - loss: 0.6844 - accuracy: 0.5000 - f1_m: 0.500 - ETA: 2s - loss: 0.6743 - accuracy: 0.5250 - f1_m: 0.525 - ETA: 1s - loss: 0.8414 - accuracy: 0.5208 - f1_m: 0.520 - ETA: 1s - loss: 0.8134 - accuracy: 0.5580 - f1_m: 0.558 - ETA: 0s - loss: 0.7912 - accuracy: 0.5859 - f1_m: 0.585 - 5s 17ms/sample - loss: 0.7752 - accuracy: 0.5986 - f1_m: 0.6030\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6104 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.5852 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5317 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5256 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5241 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5040 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5259 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5454 - accuracy: 0.7383 - f1_m: 0.738 - 2s 7ms/sample - loss: 0.5480 - accuracy: 0.7348 - f1_m: 0.7335\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5546 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5511 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5478 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5426 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5332 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5380 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5552 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5338 - accuracy: 0.7773 - f1_m: 0.777 - 2s 7ms/sample - loss: 0.5209 - accuracy: 0.7849 - f1_m: 0.7876\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6905 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5913 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5823 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.5757 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5327 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5223 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5681 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5594 - accuracy: 0.7656 - f1_m: 0.765 - 2s 7ms/sample - loss: 0.5420 - accuracy: 0.7778 - f1_m: 0.7820\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3899 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.5331 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5219 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5619 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5576 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5281 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5528 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5548 - accuracy: 0.7461 - f1_m: 0.746 - 2s 7ms/sample - loss: 0.5486 - accuracy: 0.7527 - f1_m: 0.7550\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3883 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.5478 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5390 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.5404 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5397 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.5259 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5091 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.5088 - accuracy: 0.7891 - f1_m: 0.789 - 2s 7ms/sample - loss: 0.5188 - accuracy: 0.7814 - f1_m: 0.7787\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4021 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.5197 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5269 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5486 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5461 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5331 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5209 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5017 - accuracy: 0.7812 - f1_m: 0.781 - 2s 7ms/sample - loss: 0.4827 - accuracy: 0.7885 - f1_m: 0.7911\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6627 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6320 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.5938 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5445 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5270 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5227 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5140 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5059 - accuracy: 0.7734 - f1_m: 0.773 - 2s 7ms/sample - loss: 0.5212 - accuracy: 0.7563 - f1_m: 0.7503\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6859 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5629 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5252 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 1s - loss: 0.5121 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5517 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5215 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5303 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5258 - accuracy: 0.7539 - f1_m: 0.753 - 2s 7ms/sample - loss: 0.5281 - accuracy: 0.7527 - f1_m: 0.7523\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5576 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6349 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 1s - loss: 0.5992 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.5720 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5495 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5327 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5217 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.4901 - accuracy: 0.7617 - f1_m: 0.761 - 2s 7ms/sample - loss: 0.5046 - accuracy: 0.7491 - f1_m: 0.7447\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.7655 - accuracy: 0.7324 - f1_m: 0.6533\n",
      "f1_m: 65.33%\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_85 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 29s - loss: 0.6941 - accuracy: 0.4375 - f1_m: 0.43 - ETA: 13s - loss: 0.6738 - accuracy: 0.6250 - f1_m: 0.62 - ETA: 7s - loss: 0.6812 - accuracy: 0.5938 - f1_m: 0.5937 - ETA: 5s - loss: 0.6700 - accuracy: 0.6328 - f1_m: 0.632 - ETA: 3s - loss: 0.6646 - accuracy: 0.6375 - f1_m: 0.637 - ETA: 2s - loss: 0.6561 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6618 - accuracy: 0.6429 - f1_m: 0.642 - ETA: 0s - loss: 0.6605 - accuracy: 0.6445 - f1_m: 0.644 - 5s 19ms/sample - loss: 0.6621 - accuracy: 0.6452 - f1_m: 0.6454\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6635 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.6436 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.5924 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 1s - loss: 0.5821 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5841 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5806 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5817 - accuracy: 0.7054 - f1_m: 0.705 - ETA: 0s - loss: 0.5791 - accuracy: 0.7109 - f1_m: 0.710 - 2s 7ms/sample - loss: 0.5731 - accuracy: 0.7133 - f1_m: 0.7141\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5156 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4580 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5323 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5558 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5335 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5230 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5091 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5197 - accuracy: 0.7656 - f1_m: 0.765 - 2s 7ms/sample - loss: 0.5258 - accuracy: 0.7563 - f1_m: 0.7530\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5805 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5548 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5357 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.4803 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4578 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4843 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4809 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.4814 - accuracy: 0.7930 - f1_m: 0.793 - 2s 7ms/sample - loss: 0.4796 - accuracy: 0.7993 - f1_m: 0.8015\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5127 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5190 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5211 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5023 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5082 - accuracy: 0.8000 - f1_m: 0.800 - ETA: 0s - loss: 0.5010 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4812 - accuracy: 0.8080 - f1_m: 0.808 - ETA: 0s - loss: 0.4817 - accuracy: 0.8086 - f1_m: 0.808 - 2s 7ms/sample - loss: 0.4874 - accuracy: 0.8029 - f1_m: 0.8009\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3638 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4164 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4156 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.3884 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.4268 - accuracy: 0.8188 - f1_m: 0.818 - ETA: 0s - loss: 0.4599 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4681 - accuracy: 0.7991 - f1_m: 0.799 - ETA: 0s - loss: 0.4652 - accuracy: 0.7930 - f1_m: 0.793 - 2s 7ms/sample - loss: 0.4785 - accuracy: 0.7814 - f1_m: 0.7773\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5814 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5165 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4795 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4911 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4829 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.4616 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 0s - loss: 0.4356 - accuracy: 0.8304 - f1_m: 0.830 - ETA: 0s - loss: 0.4613 - accuracy: 0.8164 - f1_m: 0.816 - 2s 7ms/sample - loss: 0.4738 - accuracy: 0.8100 - f1_m: 0.8078\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3733 - accuracy: 0.9062 - f1_m: 0.906 - ETA: 1s - loss: 0.3493 - accuracy: 0.9062 - f1_m: 0.906 - ETA: 1s - loss: 0.4391 - accuracy: 0.8542 - f1_m: 0.854 - ETA: 0s - loss: 0.4635 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.4624 - accuracy: 0.8000 - f1_m: 0.800 - ETA: 0s - loss: 0.4919 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4953 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.4744 - accuracy: 0.7969 - f1_m: 0.796 - 2s 7ms/sample - loss: 0.4591 - accuracy: 0.8065 - f1_m: 0.8098\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 1s - loss: 0.4041 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.3555 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.3940 - accuracy: 0.8542 - f1_m: 0.854 - ETA: 0s - loss: 0.5196 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5335 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5111 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4985 - accuracy: 0.8036 - f1_m: 0.803 - ETA: 0s - loss: 0.5131 - accuracy: 0.7930 - f1_m: 0.793 - 2s 7ms/sample - loss: 0.5039 - accuracy: 0.7993 - f1_m: 0.8015\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4455 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4261 - accuracy: 0.8594 - f1_m: 0.859 - ETA: 1s - loss: 0.4525 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4381 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 0s - loss: 0.4473 - accuracy: 0.8250 - f1_m: 0.825 - ETA: 0s - loss: 0.4540 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 0s - loss: 0.4516 - accuracy: 0.8214 - f1_m: 0.821 - ETA: 0s - loss: 0.4501 - accuracy: 0.8242 - f1_m: 0.824 - 2s 7ms/sample - loss: 0.4667 - accuracy: 0.8208 - f1_m: 0.8196\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 10ms/sample - loss: 0.6917 - accuracy: 0.6620 - f1_m: 0.6756\n",
      "f1_m: 67.56%\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_86 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 23s - loss: 0.6937 - accuracy: 0.4375 - f1_m: 0.43 - ETA: 10s - loss: 0.6860 - accuracy: 0.5312 - f1_m: 0.53 - ETA: 6s - loss: 0.6749 - accuracy: 0.5833 - f1_m: 0.5833 - ETA: 4s - loss: 0.6541 - accuracy: 0.6172 - f1_m: 0.617 - ETA: 2s - loss: 0.6037 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 1s - loss: 0.6353 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 1s - loss: 0.6362 - accuracy: 0.6518 - f1_m: 0.651 - ETA: 0s - loss: 0.6421 - accuracy: 0.6445 - f1_m: 0.644 - 5s 17ms/sample - loss: 0.6420 - accuracy: 0.6464 - f1_m: 0.6470\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6000 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6261 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 1s - loss: 0.6064 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5915 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5894 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.6020 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5937 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5975 - accuracy: 0.7031 - f1_m: 0.703 - 2s 7ms/sample - loss: 0.5924 - accuracy: 0.7071 - f1_m: 0.7083\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4649 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4824 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5475 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5533 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5625 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5718 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5714 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5782 - accuracy: 0.7227 - f1_m: 0.722 - 2s 7ms/sample - loss: 0.5728 - accuracy: 0.7214 - f1_m: 0.7211\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6169 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5841 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5203 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5057 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5653 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5580 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5617 - accuracy: 0.7366 - f1_m: 0.736 - ETA: 0s - loss: 0.5565 - accuracy: 0.7500 - f1_m: 0.750 - 2s 7ms/sample - loss: 0.5461 - accuracy: 0.7607 - f1_m: 0.7639\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.9448 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.7680 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 1s - loss: 0.6885 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6403 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.6480 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.6336 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6146 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5913 - accuracy: 0.7578 - f1_m: 0.757 - 2s 7ms/sample - loss: 0.5897 - accuracy: 0.7536 - f1_m: 0.7523\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6515 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5726 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5349 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5593 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5524 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5161 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5054 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5363 - accuracy: 0.7695 - f1_m: 0.769 - 2s 7ms/sample - loss: 0.5422 - accuracy: 0.7643 - f1_m: 0.7627\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5782 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5113 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 1s - loss: 0.5291 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5133 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.5152 - accuracy: 0.7937 - f1_m: 0.793 - ETA: 0s - loss: 0.5119 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5173 - accuracy: 0.7857 - f1_m: 0.785 - ETA: 0s - loss: 0.5084 - accuracy: 0.7891 - f1_m: 0.789 - 2s 7ms/sample - loss: 0.5170 - accuracy: 0.7857 - f1_m: 0.7847\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4728 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4935 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4993 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5028 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5206 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5446 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5489 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5481 - accuracy: 0.7422 - f1_m: 0.742 - 2s 7ms/sample - loss: 0.5385 - accuracy: 0.7500 - f1_m: 0.7523\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4743 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.6168 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5938 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5735 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5826 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5582 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5417 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5201 - accuracy: 0.7617 - f1_m: 0.761 - 2s 7ms/sample - loss: 0.5176 - accuracy: 0.7643 - f1_m: 0.7650\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6298 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 1.0249 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 1s - loss: 0.8631 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.7839 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.7399 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6923 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.6890 - accuracy: 0.7009 - f1_m: 0.700 - ETA: 0s - loss: 0.6841 - accuracy: 0.6953 - f1_m: 0.695 - 2s 7ms/sample - loss: 0.6612 - accuracy: 0.7143 - f1_m: 0.7199\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.3869 - accuracy: 0.8143 - f1_m: 0.8646\n",
      "f1_m: 86.46%\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_87 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 29s - loss: 0.6913 - accuracy: 0.7188 - f1_m: 0.71 - ETA: 13s - loss: 0.6790 - accuracy: 0.7188 - f1_m: 0.71 - ETA: 8s - loss: 0.6835 - accuracy: 0.6562 - f1_m: 0.6562 - ETA: 5s - loss: 0.6849 - accuracy: 0.6328 - f1_m: 0.632 - ETA: 3s - loss: 0.6793 - accuracy: 0.6438 - f1_m: 0.643 - ETA: 2s - loss: 0.6741 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 1s - loss: 0.7093 - accuracy: 0.6161 - f1_m: 0.616 - ETA: 0s - loss: 0.6918 - accuracy: 0.6367 - f1_m: 0.636 - 6s 20ms/sample - loss: 0.6880 - accuracy: 0.6477 - f1_m: 0.6504\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5679 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6515 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 1s - loss: 0.6480 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6280 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.5686 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5804 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5685 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5977 - accuracy: 0.7188 - f1_m: 0.718 - 2s 7ms/sample - loss: 0.5866 - accuracy: 0.7189 - f1_m: 0.7189\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5453 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.6422 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5938 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.6135 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5858 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5895 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5648 - accuracy: 0.7723 - f1_m: 0.772 - ETA: 0s - loss: 0.5488 - accuracy: 0.7812 - f1_m: 0.781 - 2s 7ms/sample - loss: 0.5505 - accuracy: 0.7794 - f1_m: 0.7789\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5291 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5710 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5687 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5555 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5228 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5152 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5037 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5182 - accuracy: 0.7773 - f1_m: 0.777 - 2s 7ms/sample - loss: 0.5223 - accuracy: 0.7722 - f1_m: 0.7710\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - ETA: 1s - loss: 0.6321 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.5836 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 1s - loss: 0.5762 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 0s - loss: 0.5473 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.5336 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5269 - accuracy: 0.6927 - f1_m: 0.692 - ETA: 0s - loss: 0.5347 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.5192 - accuracy: 0.7070 - f1_m: 0.707 - 2s 7ms/sample - loss: 0.5360 - accuracy: 0.7011 - f1_m: 0.6996\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4539 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4619 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 1s - loss: 0.4614 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5017 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4991 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4950 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5087 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.4991 - accuracy: 0.7656 - f1_m: 0.765 - 2s 7ms/sample - loss: 0.5062 - accuracy: 0.7651 - f1_m: 0.7650\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4744 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4423 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4485 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5194 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5153 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4978 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4915 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5239 - accuracy: 0.7656 - f1_m: 0.765 - 2s 7ms/sample - loss: 0.5323 - accuracy: 0.7616 - f1_m: 0.7606\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5330 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5132 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.4908 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5000 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5474 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5436 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5259 - accuracy: 0.7723 - f1_m: 0.772 - ETA: 0s - loss: 0.5163 - accuracy: 0.7812 - f1_m: 0.781 - 2s 7ms/sample - loss: 0.5123 - accuracy: 0.7865 - f1_m: 0.7878\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6214 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5413 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5013 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5136 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.5239 - accuracy: 0.7937 - f1_m: 0.793 - ETA: 0s - loss: 0.5250 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.5418 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.5276 - accuracy: 0.7969 - f1_m: 0.796 - 2s 7ms/sample - loss: 0.5426 - accuracy: 0.7900 - f1_m: 0.7883\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6022 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6039 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.6091 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5810 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5159 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5443 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5263 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5150 - accuracy: 0.7812 - f1_m: 0.781 - 2s 7ms/sample - loss: 0.5094 - accuracy: 0.7829 - f1_m: 0.7833\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.3238 - accuracy: 0.8261 - f1_m: 0.8750\n",
      "f1_m: 87.50%\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_88 (Embedding)     (None, 100, 50)           123200    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 307,010\n",
      "Trainable params: 183,810\n",
      "Non-trainable params: 123,200\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 22s - loss: 0.6970 - accuracy: 0.3125 - f1_m: 0.31 - ETA: 10s - loss: 0.6927 - accuracy: 0.4531 - f1_m: 0.45 - ETA: 6s - loss: 0.6895 - accuracy: 0.5000 - f1_m: 0.5000 - ETA: 4s - loss: 0.6784 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 2s - loss: 0.6789 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.6728 - accuracy: 0.6146 - f1_m: 0.614 - ETA: 1s - loss: 0.6671 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6945 - accuracy: 0.6055 - f1_m: 0.605 - 5s 16ms/sample - loss: 0.6925 - accuracy: 0.6085 - f1_m: 0.6093\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6348 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6255 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6403 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 1s - loss: 0.6422 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6405 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.6353 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 0s - loss: 0.6231 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.6067 - accuracy: 0.7148 - f1_m: 0.714 - 2s 7ms/sample - loss: 0.6109 - accuracy: 0.7117 - f1_m: 0.7110\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5797 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5181 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5167 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 0s - loss: 0.5669 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.6071 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.6063 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.6141 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.6084 - accuracy: 0.6836 - f1_m: 0.683 - 2s 7ms/sample - loss: 0.6002 - accuracy: 0.6940 - f1_m: 0.6965\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4762 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5654 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.5670 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5583 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5626 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5484 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5372 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5389 - accuracy: 0.7500 - f1_m: 0.750 - 2s 7ms/sample - loss: 0.5458 - accuracy: 0.7473 - f1_m: 0.7467\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5070 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5192 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.4941 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.4775 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5036 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.4991 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5050 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5060 - accuracy: 0.7812 - f1_m: 0.781 - 2s 7ms/sample - loss: 0.5103 - accuracy: 0.7758 - f1_m: 0.7744\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.3578 - accuracy: 0.9062 - f1_m: 0.906 - ETA: 1s - loss: 0.5860 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5650 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.5407 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.5238 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5222 - accuracy: 0.8073 - f1_m: 0.807 - ETA: 0s - loss: 0.5321 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.5293 - accuracy: 0.7930 - f1_m: 0.793 - 2s 7ms/sample - loss: 0.5197 - accuracy: 0.8007 - f1_m: 0.8026\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5400 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5559 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5274 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5042 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5004 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5450 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.5479 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5458 - accuracy: 0.7383 - f1_m: 0.738 - 2s 7ms/sample - loss: 0.5484 - accuracy: 0.7367 - f1_m: 0.7362\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4913 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5988 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5571 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.5411 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.5542 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5523 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5560 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5389 - accuracy: 0.7656 - f1_m: 0.765 - 2s 7ms/sample - loss: 0.5372 - accuracy: 0.7687 - f1_m: 0.7694\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4008 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.4702 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4953 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.5012 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.4847 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.4790 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.4826 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.4904 - accuracy: 0.7852 - f1_m: 0.785 - 2s 7ms/sample - loss: 0.4936 - accuracy: 0.7794 - f1_m: 0.7779\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6223 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5536 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5564 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.5722 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5355 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5572 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5531 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5357 - accuracy: 0.7734 - f1_m: 0.773 - 2s 7ms/sample - loss: 0.5393 - accuracy: 0.7722 - f1_m: 0.7719\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.3082 - accuracy: 0.8261 - f1_m: 0.8750\n",
      "f1_m: 87.50%\n",
      "78.87% (+/- 10.18%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario N\n",
    "do_experiment(X_train_raw, y_train_raw, 'bidirectional', 50, entity_masking=True, dropout_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_89 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 23s - loss: 0.6936 - accuracy: 0.4375 - f1_m: 0.43 - ETA: 11s - loss: 0.6915 - accuracy: 0.5156 - f1_m: 0.51 - ETA: 6s - loss: 0.6809 - accuracy: 0.5729 - f1_m: 0.5729 - ETA: 4s - loss: 0.6766 - accuracy: 0.5859 - f1_m: 0.585 - ETA: 3s - loss: 0.6707 - accuracy: 0.6000 - f1_m: 0.600 - ETA: 1s - loss: 0.6530 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.6327 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6243 - accuracy: 0.6641 - f1_m: 0.664 - 5s 18ms/sample - loss: 0.6240 - accuracy: 0.6667 - f1_m: 0.6676\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5636 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6124 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5820 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.6324 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.6265 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.6328 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.6190 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.6147 - accuracy: 0.7227 - f1_m: 0.722 - 2s 8ms/sample - loss: 0.6092 - accuracy: 0.7204 - f1_m: 0.7197\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3799 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5409 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5655 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5469 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5492 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5232 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5440 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5424 - accuracy: 0.7617 - f1_m: 0.761 - 2s 8ms/sample - loss: 0.5312 - accuracy: 0.7706 - f1_m: 0.7737\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4605 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4350 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4755 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.5407 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5501 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5550 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5508 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5476 - accuracy: 0.7539 - f1_m: 0.753 - 2s 8ms/sample - loss: 0.5387 - accuracy: 0.7563 - f1_m: 0.7571\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5460 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.4741 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4385 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5100 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5202 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5195 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5105 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5094 - accuracy: 0.7734 - f1_m: 0.773 - 2s 8ms/sample - loss: 0.5049 - accuracy: 0.7778 - f1_m: 0.7793\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4409 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4285 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5087 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5161 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5480 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5213 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5212 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5070 - accuracy: 0.7656 - f1_m: 0.765 - 2s 8ms/sample - loss: 0.5191 - accuracy: 0.7599 - f1_m: 0.7579\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3689 - accuracy: 0.9062 - f1_m: 0.906 - ETA: 1s - loss: 0.4835 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.6296 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 1s - loss: 0.5978 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5590 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5359 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5168 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5341 - accuracy: 0.7500 - f1_m: 0.750 - 2s 8ms/sample - loss: 0.5345 - accuracy: 0.7491 - f1_m: 0.7488\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4976 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.5191 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5078 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.5795 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5705 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5593 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.5459 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5320 - accuracy: 0.7578 - f1_m: 0.757 - 2s 8ms/sample - loss: 0.5535 - accuracy: 0.7491 - f1_m: 0.7461\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5044 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4562 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4812 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4773 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4894 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.4818 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5384 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5426 - accuracy: 0.7500 - f1_m: 0.750 - 2s 8ms/sample - loss: 0.5413 - accuracy: 0.7563 - f1_m: 0.7585\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4393 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.5025 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5278 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5196 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.4850 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5041 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5300 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5318 - accuracy: 0.7539 - f1_m: 0.753 - 2s 8ms/sample - loss: 0.5144 - accuracy: 0.7706 - f1_m: 0.7764\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 23ms/sample - loss: 0.4870 - accuracy: 0.8310 - f1_m: 0.8006\n",
      "f1_m: 80.06%\n",
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_90 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 22s - loss: 0.6928 - accuracy: 0.5312 - f1_m: 0.53 - ETA: 10s - loss: 0.6814 - accuracy: 0.6094 - f1_m: 0.60 - ETA: 6s - loss: 0.6695 - accuracy: 0.6354 - f1_m: 0.6354 - ETA: 4s - loss: 0.6731 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 2s - loss: 0.6627 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6453 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 1s - loss: 0.6359 - accuracy: 0.6964 - f1_m: 0.696 - ETA: 0s - loss: 0.6461 - accuracy: 0.6992 - f1_m: 0.699 - 5s 17ms/sample - loss: 0.6388 - accuracy: 0.7025 - f1_m: 0.7037\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 1.0905 - accuracy: 0.4688 - f1_m: 0.468 - ETA: 1s - loss: 0.8313 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 1s - loss: 0.7238 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 1s - loss: 0.6574 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6231 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.6416 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6688 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.6507 - accuracy: 0.6719 - f1_m: 0.671 - 2s 7ms/sample - loss: 0.6480 - accuracy: 0.6703 - f1_m: 0.6697\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5769 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5236 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.4635 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.4641 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4866 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.4783 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.4883 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.4991 - accuracy: 0.7500 - f1_m: 0.750 - 2s 8ms/sample - loss: 0.4963 - accuracy: 0.7563 - f1_m: 0.7585\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3801 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.5006 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5076 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4804 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.5214 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5282 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5286 - accuracy: 0.7723 - f1_m: 0.772 - ETA: 0s - loss: 0.5151 - accuracy: 0.7852 - f1_m: 0.785 - 2s 8ms/sample - loss: 0.5000 - accuracy: 0.7849 - f1_m: 0.7849\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4740 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4876 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5259 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5170 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4997 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.4612 - accuracy: 0.8073 - f1_m: 0.807 - ETA: 0s - loss: 0.4548 - accuracy: 0.8080 - f1_m: 0.808 - ETA: 0s - loss: 0.4705 - accuracy: 0.7969 - f1_m: 0.796 - 2s 8ms/sample - loss: 0.4635 - accuracy: 0.8065 - f1_m: 0.8098\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.3814 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4904 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4516 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.4273 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 0s - loss: 0.4631 - accuracy: 0.8188 - f1_m: 0.818 - ETA: 0s - loss: 0.4709 - accuracy: 0.8073 - f1_m: 0.807 - ETA: 0s - loss: 0.4900 - accuracy: 0.8036 - f1_m: 0.803 - ETA: 0s - loss: 0.4801 - accuracy: 0.8086 - f1_m: 0.808 - 2s 8ms/sample - loss: 0.4886 - accuracy: 0.8065 - f1_m: 0.8057\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5419 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5116 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5652 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5528 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5596 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5361 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5081 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4778 - accuracy: 0.8008 - f1_m: 0.800 - 2s 8ms/sample - loss: 0.4803 - accuracy: 0.7993 - f1_m: 0.7988\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5428 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5406 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4585 - accuracy: 0.8333 - f1_m: 0.833 - ETA: 1s - loss: 0.4518 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 0s - loss: 0.5085 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5044 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4798 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4904 - accuracy: 0.8008 - f1_m: 0.800 - 2s 8ms/sample - loss: 0.4838 - accuracy: 0.8029 - f1_m: 0.8036\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4235 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4843 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4978 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5033 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5034 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.4875 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4597 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4430 - accuracy: 0.8203 - f1_m: 0.820 - 2s 8ms/sample - loss: 0.4642 - accuracy: 0.8100 - f1_m: 0.8065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5979 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5500 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4817 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.4862 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.4787 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.4442 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 0s - loss: 0.4575 - accuracy: 0.8214 - f1_m: 0.821 - ETA: 0s - loss: 0.4604 - accuracy: 0.8203 - f1_m: 0.820 - 2s 8ms/sample - loss: 0.4596 - accuracy: 0.8100 - f1_m: 0.8065\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.7316 - accuracy: 0.7183 - f1_m: 0.7173\n",
      "f1_m: 71.73%\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_91 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 22s - loss: 0.6934 - accuracy: 0.3750 - f1_m: 0.37 - ETA: 10s - loss: 0.6826 - accuracy: 0.5156 - f1_m: 0.51 - ETA: 6s - loss: 0.6674 - accuracy: 0.5833 - f1_m: 0.5833 - ETA: 4s - loss: 0.6521 - accuracy: 0.6172 - f1_m: 0.617 - ETA: 2s - loss: 0.6812 - accuracy: 0.6000 - f1_m: 0.600 - ETA: 1s - loss: 0.6752 - accuracy: 0.6198 - f1_m: 0.619 - ETA: 1s - loss: 0.6750 - accuracy: 0.6116 - f1_m: 0.611 - ETA: 0s - loss: 0.6711 - accuracy: 0.6289 - f1_m: 0.628 - 5s 17ms/sample - loss: 0.6571 - accuracy: 0.6464 - f1_m: 0.6516\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 1.0094 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 1.1371 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 1s - loss: 0.9902 - accuracy: 0.5729 - f1_m: 0.572 - ETA: 1s - loss: 0.8621 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 0s - loss: 0.7889 - accuracy: 0.6687 - f1_m: 0.668 - ETA: 0s - loss: 0.7530 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.7146 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.7114 - accuracy: 0.6797 - f1_m: 0.679 - 2s 8ms/sample - loss: 0.7001 - accuracy: 0.6786 - f1_m: 0.6782\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5637 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.5471 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.5111 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4896 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5024 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5073 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5353 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5385 - accuracy: 0.7500 - f1_m: 0.750 - 2s 8ms/sample - loss: 0.5441 - accuracy: 0.7429 - f1_m: 0.7407\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5482 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5198 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.6135 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 1s - loss: 0.5862 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5974 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5915 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5732 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5572 - accuracy: 0.7656 - f1_m: 0.765 - 2s 8ms/sample - loss: 0.5549 - accuracy: 0.7607 - f1_m: 0.7593\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6260 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.6439 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5879 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5662 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5276 - accuracy: 0.8000 - f1_m: 0.800 - ETA: 0s - loss: 0.5248 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.4980 - accuracy: 0.8036 - f1_m: 0.803 - ETA: 0s - loss: 0.5109 - accuracy: 0.7891 - f1_m: 0.789 - 2s 8ms/sample - loss: 0.5082 - accuracy: 0.7893 - f1_m: 0.7894\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4425 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4203 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 1s - loss: 0.4957 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5191 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5167 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5220 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5204 - accuracy: 0.7723 - f1_m: 0.772 - ETA: 0s - loss: 0.5031 - accuracy: 0.7852 - f1_m: 0.785 - 2s 8ms/sample - loss: 0.5059 - accuracy: 0.7821 - f1_m: 0.7812\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.3647 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4374 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4095 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.4749 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.4635 - accuracy: 0.8000 - f1_m: 0.800 - ETA: 0s - loss: 0.4695 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4901 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.5068 - accuracy: 0.7773 - f1_m: 0.777 - 2s 8ms/sample - loss: 0.5108 - accuracy: 0.7714 - f1_m: 0.7697\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.3991 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4831 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 1s - loss: 0.4834 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.4642 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.4561 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.4979 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.4917 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.5250 - accuracy: 0.7695 - f1_m: 0.769 - 2s 8ms/sample - loss: 0.5254 - accuracy: 0.7714 - f1_m: 0.7720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.4918 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.5114 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4540 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4300 - accuracy: 0.8516 - f1_m: 0.851 - ETA: 0s - loss: 0.4283 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 0s - loss: 0.4809 - accuracy: 0.8073 - f1_m: 0.807 - ETA: 0s - loss: 0.5079 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.5173 - accuracy: 0.7656 - f1_m: 0.765 - 2s 8ms/sample - loss: 0.5100 - accuracy: 0.7714 - f1_m: 0.7731\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5312 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5136 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4999 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4913 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4924 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5158 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5310 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.5189 - accuracy: 0.7539 - f1_m: 0.753 - 2s 8ms/sample - loss: 0.5085 - accuracy: 0.7643 - f1_m: 0.7674\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.4724 - accuracy: 0.7143 - f1_m: 0.7014\n",
      "f1_m: 70.14%\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_92 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 30s - loss: 0.6934 - accuracy: 0.3125 - f1_m: 0.31 - ETA: 14s - loss: 0.6848 - accuracy: 0.4844 - f1_m: 0.48 - ETA: 8s - loss: 0.6720 - accuracy: 0.5521 - f1_m: 0.5521 - ETA: 5s - loss: 0.6685 - accuracy: 0.5703 - f1_m: 0.570 - ETA: 3s - loss: 0.6609 - accuracy: 0.6062 - f1_m: 0.606 - ETA: 2s - loss: 0.7320 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 1s - loss: 0.7202 - accuracy: 0.6295 - f1_m: 0.629 - ETA: 0s - loss: 0.7126 - accuracy: 0.6406 - f1_m: 0.640 - 6s 21ms/sample - loss: 0.7065 - accuracy: 0.6512 - f1_m: 0.6539\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6260 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6148 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6320 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 1s - loss: 0.6368 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6359 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.6305 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.6239 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6153 - accuracy: 0.7227 - f1_m: 0.722 - 2s 8ms/sample - loss: 0.6138 - accuracy: 0.7224 - f1_m: 0.7224\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5744 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5302 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5673 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 1s - loss: 0.5428 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5481 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5270 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5159 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.5301 - accuracy: 0.7930 - f1_m: 0.793 - 2s 8ms/sample - loss: 0.5351 - accuracy: 0.7865 - f1_m: 0.7849\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4962 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5367 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5761 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5786 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5632 - accuracy: 0.7312 - f1_m: 0.731 - ETA: 0s - loss: 0.5348 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5010 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5100 - accuracy: 0.7773 - f1_m: 0.777 - 2s 8ms/sample - loss: 0.5134 - accuracy: 0.7758 - f1_m: 0.7754\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6921 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5837 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5737 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5445 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5135 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.5126 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4885 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5054 - accuracy: 0.8008 - f1_m: 0.800 - 2s 8ms/sample - loss: 0.5035 - accuracy: 0.8007 - f1_m: 0.8007\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.3498 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.5212 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.6629 - accuracy: 0.6771 - f1_m: 0.677 - ETA: 1s - loss: 0.5916 - accuracy: 0.7266 - f1_m: 0.726 - ETA: 0s - loss: 0.5981 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5892 - accuracy: 0.7240 - f1_m: 0.724 - ETA: 0s - loss: 0.6066 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.6028 - accuracy: 0.7148 - f1_m: 0.714 - 2s 8ms/sample - loss: 0.5944 - accuracy: 0.7189 - f1_m: 0.7199\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5240 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5772 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5239 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.5026 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5026 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4910 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 0s - loss: 0.4849 - accuracy: 0.8214 - f1_m: 0.821 - ETA: 0s - loss: 0.5174 - accuracy: 0.7930 - f1_m: 0.793 - 2s 8ms/sample - loss: 0.5130 - accuracy: 0.8007 - f1_m: 0.8026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4024 - accuracy: 0.9375 - f1_m: 0.937 - ETA: 1s - loss: 0.4659 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4754 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.4640 - accuracy: 0.8203 - f1_m: 0.820 - ETA: 0s - loss: 0.4794 - accuracy: 0.8000 - f1_m: 0.800 - ETA: 0s - loss: 0.4630 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4742 - accuracy: 0.8080 - f1_m: 0.808 - ETA: 0s - loss: 0.4727 - accuracy: 0.8086 - f1_m: 0.808 - 2s 8ms/sample - loss: 0.4932 - accuracy: 0.7865 - f1_m: 0.7810\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5565 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5342 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.4955 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5239 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5445 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5181 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5058 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.5009 - accuracy: 0.7812 - f1_m: 0.781 - 2s 8ms/sample - loss: 0.4942 - accuracy: 0.7900 - f1_m: 0.7922\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5026 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5425 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5117 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4764 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.4363 - accuracy: 0.8062 - f1_m: 0.806 - ETA: 0s - loss: 0.4391 - accuracy: 0.8177 - f1_m: 0.817 - ETA: 0s - loss: 0.4649 - accuracy: 0.8080 - f1_m: 0.808 - ETA: 0s - loss: 0.4825 - accuracy: 0.7930 - f1_m: 0.793 - 2s 7ms/sample - loss: 0.4928 - accuracy: 0.7829 - f1_m: 0.7804\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.3758 - accuracy: 0.7971 - f1_m: 0.8542\n",
      "f1_m: 85.42%\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_93 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 22s - loss: 0.6939 - accuracy: 0.3750 - f1_m: 0.37 - ETA: 10s - loss: 0.6804 - accuracy: 0.5469 - f1_m: 0.54 - ETA: 6s - loss: 0.6752 - accuracy: 0.5729 - f1_m: 0.5729 - ETA: 4s - loss: 0.6588 - accuracy: 0.6172 - f1_m: 0.617 - ETA: 2s - loss: 0.7229 - accuracy: 0.6313 - f1_m: 0.631 - ETA: 1s - loss: 0.7089 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 1s - loss: 0.6997 - accuracy: 0.6384 - f1_m: 0.638 - ETA: 0s - loss: 0.7020 - accuracy: 0.6133 - f1_m: 0.613 - 5s 17ms/sample - loss: 0.6977 - accuracy: 0.6085 - f1_m: 0.6074\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6030 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.5718 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.6528 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.6607 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.6428 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.6317 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 0s - loss: 0.6407 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.6369 - accuracy: 0.7109 - f1_m: 0.710 - 2s 8ms/sample - loss: 0.6241 - accuracy: 0.7260 - f1_m: 0.7297\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4740 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.4606 - accuracy: 0.8594 - f1_m: 0.859 - ETA: 1s - loss: 0.5051 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5460 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5509 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5493 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5468 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5830 - accuracy: 0.7422 - f1_m: 0.742 - 2s 7ms/sample - loss: 0.5844 - accuracy: 0.7367 - f1_m: 0.7353\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6067 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5670 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5321 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5221 - accuracy: 0.8047 - f1_m: 0.804 - ETA: 0s - loss: 0.5293 - accuracy: 0.7937 - f1_m: 0.793 - ETA: 0s - loss: 0.5393 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5378 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5486 - accuracy: 0.7656 - f1_m: 0.765 - 2s 8ms/sample - loss: 0.5507 - accuracy: 0.7651 - f1_m: 0.7650\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6005 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6427 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.6238 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6249 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.5840 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5694 - accuracy: 0.7135 - f1_m: 0.713 - ETA: 0s - loss: 0.5533 - accuracy: 0.7321 - f1_m: 0.732 - ETA: 0s - loss: 0.5291 - accuracy: 0.7578 - f1_m: 0.757 - 2s 8ms/sample - loss: 0.5334 - accuracy: 0.7651 - f1_m: 0.7669\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6571 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.6233 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.5729 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5353 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5711 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5656 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 0s - loss: 0.5579 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5576 - accuracy: 0.7383 - f1_m: 0.738 - 2s 8ms/sample - loss: 0.5548 - accuracy: 0.7402 - f1_m: 0.7407\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5505 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.4915 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4988 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.4903 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.4955 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.5029 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5239 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5267 - accuracy: 0.7461 - f1_m: 0.746 - 2s 8ms/sample - loss: 0.5226 - accuracy: 0.7544 - f1_m: 0.7565\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5044 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4899 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4825 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5367 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5400 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5455 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5298 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5092 - accuracy: 0.7734 - f1_m: 0.773 - 2s 7ms/sample - loss: 0.5234 - accuracy: 0.7616 - f1_m: 0.7586\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5126 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4827 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5193 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5094 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5240 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5021 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.5007 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.4978 - accuracy: 0.7969 - f1_m: 0.796 - 2s 7ms/sample - loss: 0.4967 - accuracy: 0.7936 - f1_m: 0.7928\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5946 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5183 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4866 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5100 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5165 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5059 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5150 - accuracy: 0.7723 - f1_m: 0.772 - ETA: 0s - loss: 0.5012 - accuracy: 0.7852 - f1_m: 0.785 - 2s 7ms/sample - loss: 0.5005 - accuracy: 0.7794 - f1_m: 0.7779\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.2908 - accuracy: 0.8116 - f1_m: 0.8646\n",
      "f1_m: 86.46%\n",
      "78.76% (+/- 6.77%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario O\n",
    "do_experiment(X_train_raw, y_train_raw, 'bidirectional', 100, entity_masking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_94 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 22s - loss: 0.6912 - accuracy: 0.6875 - f1_m: 0.68 - ETA: 10s - loss: 0.6900 - accuracy: 0.6562 - f1_m: 0.65 - ETA: 6s - loss: 0.6833 - accuracy: 0.6458 - f1_m: 0.6458 - ETA: 4s - loss: 0.6929 - accuracy: 0.6016 - f1_m: 0.601 - ETA: 2s - loss: 0.6915 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.6890 - accuracy: 0.5990 - f1_m: 0.599 - ETA: 1s - loss: 0.6874 - accuracy: 0.5982 - f1_m: 0.598 - ETA: 0s - loss: 0.6775 - accuracy: 0.6328 - f1_m: 0.632 - 5s 17ms/sample - loss: 0.6637 - accuracy: 0.6452 - f1_m: 0.6495\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5242 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 1.0333 - accuracy: 0.5781 - f1_m: 0.578 - ETA: 1s - loss: 0.9196 - accuracy: 0.6146 - f1_m: 0.614 - ETA: 1s - loss: 0.8705 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 0s - loss: 0.8000 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 0s - loss: 0.7676 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.7570 - accuracy: 0.6518 - f1_m: 0.651 - ETA: 0s - loss: 0.7315 - accuracy: 0.6602 - f1_m: 0.660 - 2s 8ms/sample - loss: 0.7354 - accuracy: 0.6487 - f1_m: 0.6448\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6680 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.5956 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5889 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 1s - loss: 0.5943 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5841 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.5803 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5616 - accuracy: 0.7277 - f1_m: 0.727 - ETA: 0s - loss: 0.5913 - accuracy: 0.7266 - f1_m: 0.726 - 2s 8ms/sample - loss: 0.5871 - accuracy: 0.7312 - f1_m: 0.7328\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4862 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5369 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5388 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5856 - accuracy: 0.6953 - f1_m: 0.695 - ETA: 0s - loss: 0.5713 - accuracy: 0.7063 - f1_m: 0.706 - ETA: 0s - loss: 0.5686 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5773 - accuracy: 0.6830 - f1_m: 0.683 - ETA: 0s - loss: 0.5689 - accuracy: 0.6875 - f1_m: 0.687 - 2s 8ms/sample - loss: 0.5645 - accuracy: 0.6953 - f1_m: 0.6981\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5154 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5444 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 1s - loss: 0.5232 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 1s - loss: 0.5306 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5406 - accuracy: 0.6938 - f1_m: 0.693 - ETA: 0s - loss: 0.5424 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5296 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5328 - accuracy: 0.7305 - f1_m: 0.730 - 2s 8ms/sample - loss: 0.5157 - accuracy: 0.7491 - f1_m: 0.7556\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6160 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5351 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4912 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 1s - loss: 0.5490 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5410 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5478 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5534 - accuracy: 0.7723 - f1_m: 0.772 - ETA: 0s - loss: 0.5458 - accuracy: 0.7734 - f1_m: 0.773 - 2s 8ms/sample - loss: 0.5438 - accuracy: 0.7670 - f1_m: 0.7648\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5175 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.5229 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4863 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.5121 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5027 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.5108 - accuracy: 0.8073 - f1_m: 0.807 - ETA: 0s - loss: 0.5127 - accuracy: 0.8036 - f1_m: 0.803 - ETA: 0s - loss: 0.5294 - accuracy: 0.7930 - f1_m: 0.793 - 2s 7ms/sample - loss: 0.5379 - accuracy: 0.7814 - f1_m: 0.7773\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4714 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.5063 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.6357 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 1s - loss: 0.5835 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5685 - accuracy: 0.7375 - f1_m: 0.737 - ETA: 0s - loss: 0.5660 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5662 - accuracy: 0.7411 - f1_m: 0.741 - ETA: 0s - loss: 0.5545 - accuracy: 0.7500 - f1_m: 0.750 - 2s 8ms/sample - loss: 0.5555 - accuracy: 0.7527 - f1_m: 0.7536\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6085 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.5980 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5433 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5278 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5284 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5242 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5169 - accuracy: 0.7723 - f1_m: 0.772 - ETA: 0s - loss: 0.5261 - accuracy: 0.7617 - f1_m: 0.761 - 2s 8ms/sample - loss: 0.5305 - accuracy: 0.7527 - f1_m: 0.7495\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5014 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5132 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5035 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.4852 - accuracy: 0.8203 - f1_m: 0.820 - ETA: 0s - loss: 0.5168 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5213 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 0s - loss: 0.5173 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5200 - accuracy: 0.7734 - f1_m: 0.773 - 2s 8ms/sample - loss: 0.5157 - accuracy: 0.7778 - f1_m: 0.7793\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.4775 - accuracy: 0.8310 - f1_m: 0.8378\n",
      "f1_m: 83.78%\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_95 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 279 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - ETA: 22s - loss: 0.6918 - accuracy: 0.5938 - f1_m: 0.59 - ETA: 10s - loss: 0.6845 - accuracy: 0.6094 - f1_m: 0.60 - ETA: 6s - loss: 0.6786 - accuracy: 0.6146 - f1_m: 0.6146 - ETA: 4s - loss: 0.6549 - accuracy: 0.6328 - f1_m: 0.632 - ETA: 2s - loss: 0.7474 - accuracy: 0.6313 - f1_m: 0.631 - ETA: 1s - loss: 0.7032 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.7205 - accuracy: 0.6384 - f1_m: 0.638 - ETA: 0s - loss: 0.7148 - accuracy: 0.6328 - f1_m: 0.632 - 5s 17ms/sample - loss: 0.6984 - accuracy: 0.6380 - f1_m: 0.6398\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6954 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 1s - loss: 0.6081 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5487 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5117 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.4941 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5077 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5158 - accuracy: 0.7232 - f1_m: 0.723 - ETA: 0s - loss: 0.5095 - accuracy: 0.7344 - f1_m: 0.734 - 2s 8ms/sample - loss: 0.5285 - accuracy: 0.7204 - f1_m: 0.7156\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.6084 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.6390 - accuracy: 0.5625 - f1_m: 0.562 - ETA: 1s - loss: 0.6216 - accuracy: 0.6146 - f1_m: 0.614 - ETA: 1s - loss: 0.6057 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6275 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 0s - loss: 0.6095 - accuracy: 0.6615 - f1_m: 0.661 - ETA: 0s - loss: 0.5959 - accuracy: 0.6741 - f1_m: 0.674 - ETA: 0s - loss: 0.5745 - accuracy: 0.6914 - f1_m: 0.691 - 2s 8ms/sample - loss: 0.5876 - accuracy: 0.6774 - f1_m: 0.6726\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4950 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5625 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5335 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5392 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5262 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5283 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5203 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5097 - accuracy: 0.7734 - f1_m: 0.773 - 2s 8ms/sample - loss: 0.5089 - accuracy: 0.7670 - f1_m: 0.7648\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.4446 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.6573 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.6255 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 1s - loss: 0.5985 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5650 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.5639 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5798 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5960 - accuracy: 0.7500 - f1_m: 0.750 - 2s 8ms/sample - loss: 0.5812 - accuracy: 0.7527 - f1_m: 0.7536\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5271 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5274 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.4905 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.4818 - accuracy: 0.7891 - f1_m: 0.789 - ETA: 0s - loss: 0.4935 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.4682 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4747 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.4756 - accuracy: 0.7969 - f1_m: 0.796 - 2s 8ms/sample - loss: 0.4703 - accuracy: 0.8029 - f1_m: 0.8050\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5666 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.5036 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4649 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 1s - loss: 0.4972 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.4887 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5004 - accuracy: 0.7865 - f1_m: 0.786 - ETA: 0s - loss: 0.4756 - accuracy: 0.8036 - f1_m: 0.803 - ETA: 0s - loss: 0.4903 - accuracy: 0.7930 - f1_m: 0.793 - 2s 8ms/sample - loss: 0.4906 - accuracy: 0.7849 - f1_m: 0.7822\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5043 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5079 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5256 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.4918 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4674 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 0s - loss: 0.4752 - accuracy: 0.8073 - f1_m: 0.807 - ETA: 0s - loss: 0.4832 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.4845 - accuracy: 0.7969 - f1_m: 0.796 - 2s 8ms/sample - loss: 0.4686 - accuracy: 0.8065 - f1_m: 0.8098\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 1s - loss: 0.5172 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4549 - accuracy: 0.8281 - f1_m: 0.828 - ETA: 1s - loss: 0.4174 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4251 - accuracy: 0.8359 - f1_m: 0.835 - ETA: 0s - loss: 0.4891 - accuracy: 0.8000 - f1_m: 0.800 - ETA: 0s - loss: 0.4674 - accuracy: 0.8021 - f1_m: 0.802 - ETA: 0s - loss: 0.4741 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.4803 - accuracy: 0.7930 - f1_m: 0.793 - 2s 8ms/sample - loss: 0.4832 - accuracy: 0.7885 - f1_m: 0.7870\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - ETA: 1s - loss: 0.5591 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5341 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5017 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 1s - loss: 0.4969 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.4707 - accuracy: 0.7750 - f1_m: 0.775 - ETA: 0s - loss: 0.4783 - accuracy: 0.7917 - f1_m: 0.791 - ETA: 0s - loss: 0.4686 - accuracy: 0.7946 - f1_m: 0.794 - ETA: 0s - loss: 0.4662 - accuracy: 0.7969 - f1_m: 0.796 - 2s 8ms/sample - loss: 0.4663 - accuracy: 0.7957 - f1_m: 0.7953\n",
      "71/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.6239 - accuracy: 0.6620 - f1_m: 0.6384\n",
      "f1_m: 63.84%\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_96 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 280 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - ETA: 22s - loss: 0.6939 - accuracy: 0.4688 - f1_m: 0.46 - ETA: 10s - loss: 0.6937 - accuracy: 0.4844 - f1_m: 0.48 - ETA: 6s - loss: 0.6929 - accuracy: 0.5104 - f1_m: 0.5104 - ETA: 4s - loss: 0.6897 - accuracy: 0.5391 - f1_m: 0.539 - ETA: 2s - loss: 0.6808 - accuracy: 0.5750 - f1_m: 0.575 - ETA: 1s - loss: 0.6438 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 1s - loss: 0.6950 - accuracy: 0.6027 - f1_m: 0.602 - ETA: 0s - loss: 0.7534 - accuracy: 0.6133 - f1_m: 0.613 - 5s 17ms/sample - loss: 0.7441 - accuracy: 0.6179 - f1_m: 0.6192\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.6665 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.5915 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.6193 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 1s - loss: 0.6142 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5979 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.6113 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.6012 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5830 - accuracy: 0.7266 - f1_m: 0.726 - 2s 8ms/sample - loss: 0.5780 - accuracy: 0.7286 - f1_m: 0.7292\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5484 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5229 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5097 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.5293 - accuracy: 0.7422 - f1_m: 0.742 - ETA: 0s - loss: 0.5508 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5575 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5749 - accuracy: 0.7143 - f1_m: 0.714 - ETA: 0s - loss: 0.5747 - accuracy: 0.7188 - f1_m: 0.718 - 2s 8ms/sample - loss: 0.5743 - accuracy: 0.7214 - f1_m: 0.7222\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5958 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5436 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5723 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5795 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.5903 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5872 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5735 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5665 - accuracy: 0.7148 - f1_m: 0.714 - 2s 8ms/sample - loss: 0.5720 - accuracy: 0.7071 - f1_m: 0.7049\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5629 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5599 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5637 - accuracy: 0.7708 - f1_m: 0.770 - ETA: 1s - loss: 0.5443 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5288 - accuracy: 0.7937 - f1_m: 0.793 - ETA: 0s - loss: 0.5360 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5440 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5561 - accuracy: 0.7656 - f1_m: 0.765 - 2s 7ms/sample - loss: 0.5610 - accuracy: 0.7643 - f1_m: 0.7639\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5495 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6107 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 1s - loss: 0.5778 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 1s - loss: 0.5603 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5596 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5410 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 0s - loss: 0.5206 - accuracy: 0.7545 - f1_m: 0.754 - ETA: 0s - loss: 0.4892 - accuracy: 0.7773 - f1_m: 0.777 - 2s 7ms/sample - loss: 0.5153 - accuracy: 0.7643 - f1_m: 0.7604\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.3710 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4657 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.4823 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5244 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5521 - accuracy: 0.7125 - f1_m: 0.712 - ETA: 0s - loss: 0.5566 - accuracy: 0.6979 - f1_m: 0.697 - ETA: 0s - loss: 0.5623 - accuracy: 0.6920 - f1_m: 0.692 - ETA: 0s - loss: 0.5740 - accuracy: 0.6914 - f1_m: 0.691 - 2s 7ms/sample - loss: 0.5808 - accuracy: 0.6893 - f1_m: 0.6887\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.7519 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.6581 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6357 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 1s - loss: 0.6126 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 0s - loss: 0.6009 - accuracy: 0.7000 - f1_m: 0.700 - ETA: 0s - loss: 0.5889 - accuracy: 0.7083 - f1_m: 0.708 - ETA: 0s - loss: 0.5766 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.5661 - accuracy: 0.7266 - f1_m: 0.726 - 2s 8ms/sample - loss: 0.5711 - accuracy: 0.7286 - f1_m: 0.7292\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5024 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5752 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5747 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5579 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5397 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5517 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5419 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.5486 - accuracy: 0.7539 - f1_m: 0.753 - 2s 8ms/sample - loss: 0.5390 - accuracy: 0.7536 - f1_m: 0.7535\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - ETA: 1s - loss: 0.5608 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5012 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5227 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.5195 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5362 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5466 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5473 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5331 - accuracy: 0.7617 - f1_m: 0.761 - 2s 7ms/sample - loss: 0.5337 - accuracy: 0.7643 - f1_m: 0.7650\n",
      "70/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.4635 - accuracy: 0.7429 - f1_m: 0.7222\n",
      "f1_m: 72.22%\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_97 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 30s - loss: 0.6926 - accuracy: 0.5938 - f1_m: 0.59 - ETA: 14s - loss: 0.6797 - accuracy: 0.6719 - f1_m: 0.67 - ETA: 8s - loss: 0.6652 - accuracy: 0.6979 - f1_m: 0.6979 - ETA: 5s - loss: 0.6476 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 3s - loss: 0.7587 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 2s - loss: 0.7467 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.7325 - accuracy: 0.6607 - f1_m: 0.660 - ETA: 0s - loss: 0.7204 - accuracy: 0.6641 - f1_m: 0.664 - 6s 21ms/sample - loss: 0.7168 - accuracy: 0.6548 - f1_m: 0.6525\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6493 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6177 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.6196 - accuracy: 0.7396 - f1_m: 0.739 - ETA: 1s - loss: 0.6227 - accuracy: 0.7109 - f1_m: 0.710 - ETA: 0s - loss: 0.6151 - accuracy: 0.7250 - f1_m: 0.725 - ETA: 0s - loss: 0.6428 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 0s - loss: 0.6285 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.6192 - accuracy: 0.7500 - f1_m: 0.750 - 2s 7ms/sample - loss: 0.6131 - accuracy: 0.7402 - f1_m: 0.7378\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5237 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5714 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5814 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5485 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.5783 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5684 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5778 - accuracy: 0.7455 - f1_m: 0.745 - ETA: 0s - loss: 0.5686 - accuracy: 0.7461 - f1_m: 0.746 - 2s 8ms/sample - loss: 0.5680 - accuracy: 0.7438 - f1_m: 0.7432\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4736 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4818 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.4386 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.4851 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4923 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.5093 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5101 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5071 - accuracy: 0.7773 - f1_m: 0.777 - 2s 8ms/sample - loss: 0.5263 - accuracy: 0.7687 - f1_m: 0.7665\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - ETA: 1s - loss: 0.5958 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6157 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 1s - loss: 0.5640 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5584 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5841 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5608 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5392 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5104 - accuracy: 0.7891 - f1_m: 0.789 - 2s 8ms/sample - loss: 0.5220 - accuracy: 0.7829 - f1_m: 0.7814\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.9027 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.7540 - accuracy: 0.6406 - f1_m: 0.640 - ETA: 1s - loss: 0.6838 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 1s - loss: 0.6215 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.6123 - accuracy: 0.6625 - f1_m: 0.662 - ETA: 0s - loss: 0.6268 - accuracy: 0.6302 - f1_m: 0.630 - ETA: 0s - loss: 0.6223 - accuracy: 0.6339 - f1_m: 0.633 - ETA: 0s - loss: 0.6326 - accuracy: 0.6367 - f1_m: 0.636 - 2s 8ms/sample - loss: 0.6365 - accuracy: 0.6299 - f1_m: 0.6282\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5872 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.6137 - accuracy: 0.7188 - f1_m: 0.718 - ETA: 1s - loss: 0.5801 - accuracy: 0.7292 - f1_m: 0.729 - ETA: 1s - loss: 0.5454 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5234 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5218 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5147 - accuracy: 0.7634 - f1_m: 0.763 - ETA: 0s - loss: 0.4976 - accuracy: 0.7656 - f1_m: 0.765 - 2s 7ms/sample - loss: 0.5073 - accuracy: 0.7580 - f1_m: 0.7561\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4684 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.4702 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5209 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5371 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5138 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5301 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5179 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5191 - accuracy: 0.7695 - f1_m: 0.769 - 2s 7ms/sample - loss: 0.5105 - accuracy: 0.7758 - f1_m: 0.7774\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4891 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4390 - accuracy: 0.8438 - f1_m: 0.843 - ETA: 1s - loss: 0.4679 - accuracy: 0.8229 - f1_m: 0.822 - ETA: 1s - loss: 0.4744 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.4776 - accuracy: 0.7875 - f1_m: 0.787 - ETA: 0s - loss: 0.4773 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4807 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.4915 - accuracy: 0.7695 - f1_m: 0.769 - 2s 7ms/sample - loss: 0.4981 - accuracy: 0.7651 - f1_m: 0.7640\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4714 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5336 - accuracy: 0.7344 - f1_m: 0.734 - ETA: 1s - loss: 0.5184 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 1s - loss: 0.4972 - accuracy: 0.7734 - f1_m: 0.773 - ETA: 0s - loss: 0.4932 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.4978 - accuracy: 0.7604 - f1_m: 0.760 - ETA: 0s - loss: 0.5076 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5217 - accuracy: 0.7539 - f1_m: 0.753 - 2s 8ms/sample - loss: 0.5034 - accuracy: 0.7687 - f1_m: 0.7724\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.3647 - accuracy: 0.7681 - f1_m: 0.8333\n",
      "f1_m: 83.33%\n",
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_98 (Embedding)     (None, 100, 100)          246400    \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 481,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 246,400\n",
      "_________________________________________________________________\n",
      "Train on 281 samples\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - ETA: 22s - loss: 0.6945 - accuracy: 0.3438 - f1_m: 0.34 - ETA: 10s - loss: 0.6945 - accuracy: 0.4375 - f1_m: 0.43 - ETA: 6s - loss: 0.6914 - accuracy: 0.4896 - f1_m: 0.4896 - ETA: 4s - loss: 0.6880 - accuracy: 0.5234 - f1_m: 0.523 - ETA: 2s - loss: 0.6820 - accuracy: 0.5500 - f1_m: 0.550 - ETA: 1s - loss: 0.6769 - accuracy: 0.5677 - f1_m: 0.567 - ETA: 1s - loss: 0.6638 - accuracy: 0.5982 - f1_m: 0.598 - ETA: 0s - loss: 0.7182 - accuracy: 0.5859 - f1_m: 0.585 - 5s 17ms/sample - loss: 0.7124 - accuracy: 0.5801 - f1_m: 0.5786\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6112 - accuracy: 0.5938 - f1_m: 0.593 - ETA: 1s - loss: 0.6282 - accuracy: 0.6094 - f1_m: 0.609 - ETA: 1s - loss: 0.6185 - accuracy: 0.6250 - f1_m: 0.625 - ETA: 1s - loss: 0.5865 - accuracy: 0.6641 - f1_m: 0.664 - ETA: 0s - loss: 0.5776 - accuracy: 0.6750 - f1_m: 0.675 - ETA: 0s - loss: 0.5814 - accuracy: 0.6667 - f1_m: 0.666 - ETA: 0s - loss: 0.5635 - accuracy: 0.6830 - f1_m: 0.683 - ETA: 0s - loss: 0.5304 - accuracy: 0.7070 - f1_m: 0.707 - 2s 8ms/sample - loss: 0.5375 - accuracy: 0.7046 - f1_m: 0.7040\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6567 - accuracy: 0.6875 - f1_m: 0.687 - ETA: 1s - loss: 0.6026 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.6279 - accuracy: 0.6354 - f1_m: 0.635 - ETA: 1s - loss: 0.6053 - accuracy: 0.6328 - f1_m: 0.632 - ETA: 0s - loss: 0.6018 - accuracy: 0.6375 - f1_m: 0.637 - ETA: 0s - loss: 0.5998 - accuracy: 0.6458 - f1_m: 0.645 - ETA: 0s - loss: 0.5811 - accuracy: 0.6696 - f1_m: 0.669 - ETA: 0s - loss: 0.5625 - accuracy: 0.6836 - f1_m: 0.683 - 2s 8ms/sample - loss: 0.5697 - accuracy: 0.6904 - f1_m: 0.6921\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5184 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5782 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 1s - loss: 0.5761 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5458 - accuracy: 0.6719 - f1_m: 0.671 - ETA: 0s - loss: 0.5510 - accuracy: 0.6812 - f1_m: 0.681 - ETA: 0s - loss: 0.5419 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 0s - loss: 0.5363 - accuracy: 0.7098 - f1_m: 0.709 - ETA: 0s - loss: 0.5132 - accuracy: 0.7227 - f1_m: 0.722 - 2s 8ms/sample - loss: 0.5531 - accuracy: 0.7082 - f1_m: 0.7046\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.6076 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.5628 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5652 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5419 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5443 - accuracy: 0.7625 - f1_m: 0.762 - ETA: 0s - loss: 0.5576 - accuracy: 0.7448 - f1_m: 0.744 - ETA: 0s - loss: 0.5457 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5467 - accuracy: 0.7617 - f1_m: 0.761 - 2s 8ms/sample - loss: 0.5440 - accuracy: 0.7687 - f1_m: 0.7704\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.5461 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5477 - accuracy: 0.7031 - f1_m: 0.703 - ETA: 1s - loss: 0.5053 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5059 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5286 - accuracy: 0.7688 - f1_m: 0.768 - ETA: 0s - loss: 0.5259 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5192 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 0s - loss: 0.5117 - accuracy: 0.7734 - f1_m: 0.773 - 2s 8ms/sample - loss: 0.5223 - accuracy: 0.7651 - f1_m: 0.7631\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4010 - accuracy: 0.8750 - f1_m: 0.875 - ETA: 1s - loss: 0.5035 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 1s - loss: 0.5206 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5570 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5442 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5216 - accuracy: 0.7760 - f1_m: 0.776 - ETA: 0s - loss: 0.5352 - accuracy: 0.7679 - f1_m: 0.767 - ETA: 0s - loss: 0.5194 - accuracy: 0.7695 - f1_m: 0.769 - 2s 7ms/sample - loss: 0.5090 - accuracy: 0.7794 - f1_m: 0.7818\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4538 - accuracy: 0.8125 - f1_m: 0.812 - ETA: 1s - loss: 0.5129 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.4766 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5471 - accuracy: 0.7578 - f1_m: 0.757 - ETA: 0s - loss: 0.5644 - accuracy: 0.7437 - f1_m: 0.743 - ETA: 0s - loss: 0.5681 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 0s - loss: 0.5628 - accuracy: 0.7589 - f1_m: 0.758 - ETA: 0s - loss: 0.5692 - accuracy: 0.7656 - f1_m: 0.765 - 2s 8ms/sample - loss: 0.5504 - accuracy: 0.7758 - f1_m: 0.7783\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.4784 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5426 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 1s - loss: 0.5365 - accuracy: 0.7812 - f1_m: 0.781 - ETA: 1s - loss: 0.5509 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5249 - accuracy: 0.7937 - f1_m: 0.793 - ETA: 0s - loss: 0.5122 - accuracy: 0.7969 - f1_m: 0.796 - ETA: 0s - loss: 0.5099 - accuracy: 0.7902 - f1_m: 0.790 - ETA: 0s - loss: 0.5268 - accuracy: 0.7773 - f1_m: 0.777 - 2s 7ms/sample - loss: 0.5150 - accuracy: 0.7900 - f1_m: 0.7932\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - ETA: 1s - loss: 0.7053 - accuracy: 0.6562 - f1_m: 0.656 - ETA: 1s - loss: 0.6042 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.6098 - accuracy: 0.7500 - f1_m: 0.750 - ETA: 1s - loss: 0.5682 - accuracy: 0.7656 - f1_m: 0.765 - ETA: 0s - loss: 0.5719 - accuracy: 0.7563 - f1_m: 0.756 - ETA: 0s - loss: 0.5612 - accuracy: 0.7552 - f1_m: 0.755 - ETA: 0s - loss: 0.5258 - accuracy: 0.7768 - f1_m: 0.776 - ETA: 0s - loss: 0.5357 - accuracy: 0.7734 - f1_m: 0.773 - 2s 8ms/sample - loss: 0.5359 - accuracy: 0.7758 - f1_m: 0.7764\n",
      "69/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 9ms/sample - loss: 0.3572 - accuracy: 0.7971 - f1_m: 0.8542\n",
      "f1_m: 85.42%\n",
      "77.72% (+/- 8.37%)\n"
     ]
    }
   ],
   "source": [
    "# Skenario P\n",
    "do_experiment(X_train_raw, y_train_raw, 'bidirectional', 100, entity_masking=True, dropout_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          249300    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 484,310\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 249,300\n",
      "_________________________________________________________________\n",
      "Train on 350 samples\n",
      "Epoch 1/10\n",
      "350/350 [==============================] - 18s 51ms/sample - loss: 0.6571 - accuracy: 0.6543 - f1_m: 0.6540\n",
      "Epoch 2/10\n",
      "350/350 [==============================] - 7s 19ms/sample - loss: 0.5790 - accuracy: 0.7429 - f1_m: 0.7436\n",
      "Epoch 3/10\n",
      "350/350 [==============================] - 7s 19ms/sample - loss: 0.5280 - accuracy: 0.7629 - f1_m: 0.7634\n",
      "Epoch 4/10\n",
      "350/350 [==============================] - 7s 19ms/sample - loss: 0.5322 - accuracy: 0.7771 - f1_m: 0.7767\n",
      "Epoch 5/10\n",
      "350/350 [==============================] - 7s 19ms/sample - loss: 0.5304 - accuracy: 0.7343 - f1_m: 0.7341\n",
      "Epoch 6/10\n",
      "350/350 [==============================] - 7s 19ms/sample - loss: 0.5234 - accuracy: 0.7571 - f1_m: 0.7574\n",
      "Epoch 7/10\n",
      "350/350 [==============================] - 7s 19ms/sample - loss: 0.5290 - accuracy: 0.7800 - f1_m: 0.7805\n",
      "Epoch 8/10\n",
      "350/350 [==============================] - 7s 20ms/sample - loss: 0.4711 - accuracy: 0.7914 - f1_m: 0.7913\n",
      "Epoch 9/10\n",
      "350/350 [==============================] - 7s 20ms/sample - loss: 0.5277 - accuracy: 0.7571 - f1_m: 0.7568\n",
      "Epoch 10/10\n",
      "350/350 [==============================] - 7s 21ms/sample - loss: 0.4935 - accuracy: 0.7600 - f1_m: 0.7597\n"
     ]
    }
   ],
   "source": [
    "best_model = train_best_model(X_train_raw, y_train_raw, 'bidirectional', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_raw_2 = preprocess(X_test_raw, 100, False)\n",
    "y_test_raw_2 = to_categorical(y_test_raw)\n",
    "\n",
    "y_test_raw_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 0.5719 - accuracy: 0.7133 - f1_m: 0.7142\n",
      "f1_m: 71.42%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7142045"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = best_model.evaluate(X_test_raw_2, y_test_raw_2)\n",
    "print(\"%s: %.2f%%\" % (best_model.metrics_names[2], scores[2]*100))\n",
    "scores[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "save_best_model(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_best_model(X_test_raw, y_test_raw, 'bidirectional', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "best_model_predict = load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74133337, 0.2586667 ],\n",
       "       [0.5753387 , 0.42466122],\n",
       "       [0.24109656, 0.7589035 ],\n",
       "       [0.14595649, 0.8540435 ],\n",
       "       [0.43999812, 0.56000185],\n",
       "       [0.15845163, 0.8415484 ],\n",
       "       [0.78634447, 0.2136555 ],\n",
       "       [0.77449614, 0.22550388],\n",
       "       [0.80891067, 0.19108935]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw_demo = ['mid ty. tango please. miss mid. gg pugna. pugna noob. wtf pugna. gg end.', \n",
    "         'so ya mama likes dick ehh?. figures. ur not even a good hooker kid. passive shadow blade?. gg',\n",
    "         'reprot. SAD. fucking reported axe. WORST HOOK IN HISTORY. YOU THOUGHT. IM THE #1 ROAMER NA. STUPIDD PIUDGE. STUPID!. nice ult medusa. commended. DOWNYS  GET DUMKED. ez game. Ty ty . SOOO EZQ!!. COMMEND ME!. COMMAND ME!!',\n",
    "         'what. jeje fAM. free farming ls. not coming into play. let end. storm fat yet. zZZ. ok. U useless anyways. does it matter. 30mins in. cant seem to hit a singel call. ROFL. still didnt hirt. Aha. better share hero contorl. to someone else. might start hitting ur Q. ROFL. fuckING. Retard. yea. he had an. amazing blast laen. at mid. 20mins scythe on od. Yet my . LS . wants to farm. His orchid. Die btich. oh. my god. oh my god. oh. My. Fucking god',\n",
    "         'die idoot. divine for the win. so noob. fucker. Tk. ahahaha. so sad. triggerd. rep[ort jugg. ty. ez. just erport this jugg. ty. gg',\n",
    "         \"what's happening boyz ?. as you can see we are waiting . :D. I am just asking what happened. <3. how many more x3 mins do we have to wait. :D. BOI. you don't know how time works. look. you will have more gold if he leaves. If I roll a one. WE GO. izi. +. now you are lucky. :D. naah we ain't. :D. gg wp\",\n",
    "         'drow . remember me last game. haha. waot. haha',\n",
    "              'yes dog. yeah . fast and furious. too fas. haha. sad',\n",
    "             'no idiot. We too pro. Lol']\n",
    "X_demo = preprocess(X_raw_demo, 100, False)\n",
    "\n",
    "\n",
    "best_model_predict.predict(X_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(X_raw_demo[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(X_raw_demo[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(X_raw_demo[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(X_raw_demo[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_demo = load_ner_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes dog . yeah . fast and furious . too fas . haha . sad'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_mask(ner_model_demo, X_raw_demo[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding, tokenizer = load_word_embedding_and_tokenizer(50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[159, 10, 173, 118, 14, 1045, 135, 1046, 29, 66], [21, 10, 33, 135, 90, 7]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['yes XXBADXX. yeah . fast and furious. too fas. haha. sad', 'no XXBADXX. We too pro. Lol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04298469, -0.10425266, -0.00014928,  0.05328295,  0.06673229,\n",
       "       -0.03732836, -0.01063337, -0.01865371,  0.01436135,  0.01203198,\n",
       "        0.01007037,  0.03521364,  0.02111503, -0.00595104,  0.03267397,\n",
       "        0.00403544, -0.01873613,  0.05090657, -0.02693766, -0.00585618,\n",
       "       -0.00788706,  0.0331885 , -0.06891623, -0.02681163, -0.04487867,\n",
       "       -0.01028155,  0.00077757, -0.00344785, -0.06074489, -0.02487996,\n",
       "        0.05627621, -0.04253253, -0.04047497, -0.01041882,  0.00823402,\n",
       "        0.07487944, -0.00607836, -0.00507032, -0.02371054, -0.04531853,\n",
       "       -0.01809775,  0.01073977,  0.06511343,  0.01847629,  0.08275626,\n",
       "        0.01185936,  0.01731069, -0.01189964,  0.02377721, -0.01875727])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding[21]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}